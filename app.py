"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - Oracle Vector Storeç‰ˆæœ¬
"""

import streamlit as st
from utils.oracle_vector_store import OracleVectorStore
from utils.oracle_json_store import OracleJsonStore
from utils.medical_record_parser import MedicalRecordParser
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import shutil
from pathlib import Path
import logging
import openai
import pdfplumber
import json
import datetime
import hashlib
from typing import Dict, Any, List
from decimal import Decimal
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è·å–OpenAIé…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

# è®¾ç½®æ–‡æ¡£å­˜å‚¨ç›®å½•
UPLOAD_DIR = Path("uploaded_documents")
UPLOAD_DIR.mkdir(exist_ok=True)

# è®¾ç½®JSONç¼“å­˜ç›®å½•
CACHE_DIR = Path("json_cache")
CACHE_DIR.mkdir(exist_ok=True)

# ä½¿ç”¨ st.cache_resource ç¼“å­˜æ¨¡å‹ï¼Œå¹¶éšè—ï¿½ï¿½
@st.cache_resource(show_spinner=False)
def load_embeddings_model():
    """åŠ è½½å‘é‡åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰"""
    logger.debug("åŠ è½½å‘é‡åŒ–æ¨¡å‹")
    return SentenceTransformer('all-MiniLM-L6-v2')

# åˆå§‹åŒ–æ¨¡å‹
embeddings_model = load_embeddings_model()

def get_cache_path(file_path: str) -> Path:
    """æ ¹æ®æ–‡ä»¶è·¯å¾„ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # è·å–åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    original_name = Path(file_path).stem
    return CACHE_DIR / f"{original_name}.json"

def save_to_cache(file_path: str, data: Dict) -> None:
    """ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°ç¼“å­˜"""
    cache_path = get_cache_path(file_path)
    with open(cache_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"ç¼“å­˜ä¿å­˜åˆ°: {cache_path}")

def load_from_cache(file_path: str) -> Dict:
    """ä»ç¼“å­˜åŠ è½½ç»“æ„åŒ–æ•°æ®"""
    cache_path = get_cache_path(file_path)
    if cache_path.exists():
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"ä»ç¼“å­˜åŠ è½½: {cache_path}")
        return data
    return None

def save_uploaded_file(uploaded_file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•"""
    file_path = UPLOAD_DIR / uploaded_file.name
    with open(file_path, "wb") as f:
        shutil.copyfileobj(uploaded_file, f)
    return str(file_path)

def read_file_content(file_path):
    """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨ pdfplumber å¤„ç† PDF"""
    try:
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_extension = str(file_path).lower().split('.')[-1]
        
        # PDFæ–‡ä»¶å¤„ç†
        if file_extension == 'pdf':
            with pdfplumber.open(file_path) as pdf:
                text = ''
                for page in pdf.pages:
                    text += page.extract_text() + '\n'
                return text.strip()
        
        # æ–‡æœ¬æ–‡ä»¶å¤„ç†
        else:
            encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read().strip()
                except UnicodeDecodeError:
                    continue
        
        return None
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        return None

def get_uploaded_files():
    """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
    return list(UPLOAD_DIR.glob("*.*"))

def vectorize_document(file_path):
    """å‘é‡åŒ–æ–‡æ¡£"""
    try:
        content = read_file_content(file_path)
        # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹è¿›è¡Œå‘é‡åŒ–
        vector = embeddings_model.encode([content])[0]
        documents = [{"file_path": str(file_path), "content": content}]
        
        with OracleVectorStore() as vector_store:
            vector_store.add_vectors([vector], documents)
        return True
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤±è´¥: {str(e)}")
        return False

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    try:
        with OracleJsonStore() as json_store:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_table_sql = """
            SELECT COUNT(*) as count
            FROM user_tables 
            WHERE table_name = 'DOCUMENT_JSON'
            """
            result = json_store.execute_search(check_table_sql)
            table_exists = result[0]['count'] > 0 if result else False
            
            if not table_exists:
                # åˆ›å»ºç»“æ„åŒ–æ–‡æ¡£è¡¨
                create_json_table_sql = """
                CREATE TABLE DOCUMENT_JSON (
                    id NUMBER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                    doc_info VARCHAR2(500),
                    doc_json JSON
                )
                """
                json_store.execute_sql(create_json_table_sql)
                logger.info("æˆåŠŸåˆ›å»ºDOCUMENT_JSONè¡¨")
            else:
                logger.debug("DOCUMENT_JSONè¡¨å·²å­˜åœ¨")  # æ”¹ç”¨debugçº§åˆ«çš„æ—¥å¿—

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")
        st.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")

class MedicalRecordParser:
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )

    def parse_medical_record(self, text: str, doc_info: str) -> Dict[str, Any]:
        """è§£æåŒ»ç–—è®°å½•æ–‡æœ¬ï¼Œç”Ÿæˆç»“æ„åŒ–JSONæ•°æ®å’ŒSQLæ’å…¥è¯­å¥"""
        try:
            prompt = f"""è¯·å°†ä»¥ä¸‹ç—…å†å†…å®¹ç»“æ„åŒ–ä¸ºJSONæ ¼å¼ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„SQLæ’å…¥è¯­å¥ã€‚

ç—…å†å†…å®¹ï¼š
{text}

è¦æ±‚ï¼š
1. JSONç»“æ„ï¼š
{{
    "æ‚£è€…å§“å": "æŸæŸï¼ˆä¿æŠ¤éšç§ï¼‰",
    "æ€§åˆ«": "æ€§åˆ«",
    "å¹´é¾„": "æ•°å­—",
    "å…¥é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "å‡ºé™¢æ—¥æœŸ": "YYYY-MM-DD",
    "ä¸»è¯‰": "ä¸»è¦ç—‡çŠ¶",
    "ç°ç—…å²": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "å…¥é™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‡ºé™¢è¯Šæ–­": ["æ–­1", "è¯Šæ–­2"],
    "ç”Ÿå‘½ä½“å¾": {{
        "ä½“æ¸©": "åŒ…å«å•ä½",
        "è¡€å‹": "åŒ…å«å•ä½"
    }},
    "ç”ŸåŒ–æŒ‡æ ‡": {{
        "æŒ‡æ ‡å": "å€¼å’Œå•ä½ï¼Œå¼‚å¸¸æ ‡â†‘â†“"
    }},
    "è¯Šç–—ç»è¿‡": "æ²»ç–—è¿‡ç¨‹æè¿°",
    "å‡ºé™¢åŒ»å˜±": ["åŒ»å˜±1", "åŒ»å˜±2"]
}}

2. SQLè¦æ±‚ï¼š
- è¡¨åï¼šDOCUMENT_JSON
- å­—æ®µï¼š(id NUMBERè‡ªå¢, doc_info VARCHAR2(500), doc_json JSON)
- doc_infoå€¼ï¼š{doc_info}
- ä½¿ç”¨JSON_OBJECTæˆ–FORMAT JSONè¯­æ³•

è¯·è¿”å›ï¼š
{{
    "structured_data": JSONæ ¼å¼çš„ç—…å†æ•°æ®,
    "sql_statement": Oracleæ’å…¥è¯­å¥
}}"""

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯åŒ»ç–—æ•°æ®ç»“æ„åŒ–ä¸“å®¶ï¼Œæ“…é•¿è§£æç—…å†æ–‡æœ¬å¹¶ç”Ÿæˆè§„èŒƒçš„JSONå’ŒSQLã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )

            # è§£æè¿”å›çš„JSON
            result = json.loads(response.choices[0].message.content)
            
            # éªŒè¯ç»“æœæ ¼å¼
            if not isinstance(result, dict) or 'structured_data' not in result or 'sql_statement' not in result:
                raise ValueError("GPTè¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®")

            # æ·»åŠ å…ƒæ•°æ®
            if 'metadata' not in result['structured_data']:
                result['structured_data']['metadata'] = {}
            result['structured_data']['metadata'].update({
                'import_time': datetime.datetime.now().isoformat(),
                'source_type': 'text',
                'last_updated': datetime.datetime.now().isoformat()
            })

            return result

        except Exception as e:
            logger.error(f"è§£æåŒ»ç–—è®°å½•å¤±è´¥: {str(e)}")
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}

def parse_document_to_json(file_path):
    """è§£ææ–‡æ¡£ä¸ºç»“æ„åŒ–JSONå¹¶ç”ŸæˆSQLæ’å…¥è¯­å¥"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = read_file_content(file_path)
        if not content:
            logger.error(f"æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {file_path}")
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cached_data = load_from_cache(file_path)
        if cached_data:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„ç»“æ„åŒ–æ•°æ®")
            structured_data = cached_data
        else:
            # ä½¿ç”¨è§£æå™¨å¤„ç†
            parser = MedicalRecordParser()
            doc_info = str(file_path)
            result = parser.parse_medical_record(content, doc_info)
            
            # å¦‚æœè¿”å›çš„æ˜¯é”™è¯¯ä¿¡æ¯ï¼Œè¿”å› None
            if 'error' in result:
                logger.error(f"è§£æå¤±è´¥: {result['error']}")
                return None
            
            structured_data = result['structured_data']
            # ä¿å­˜åˆ°ç¼“å­˜
            save_to_cache(file_path, structured_data)
        
        # æ‰§è¡ŒSQLæ’å…¥
        with OracleJsonStore() as json_store:
            try:
                # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
                check_sql = """
                SELECT COUNT(*) as count 
                FROM DOCUMENT_JSON 
                WHERE doc_info = :1
                """
                result = json_store.execute_search(check_sql, [str(file_path)])
                doc_exists = result[0]['count'] > 0 if result else False
                
                # å°†æ•°æ®è½¬ä¸ºJSONå­—ç¬¦ä¸²
                doc_json_str = json.dumps(structured_data, ensure_ascii=False)
                
                if doc_exists:
                    # æ›´æ–°ç°æœ‰æ–‡æ¡£
                    update_sql = """
                    UPDATE DOCUMENT_JSON 
                    SET doc_json = JSON(:1)
                    WHERE doc_info = :2
                    """
                    # è®°å½•å®é™…æ‰§è¡Œçš„SQLå’Œå‚æ•°
                    logger.info("æ‰§è¡Œæ›´æ–°SQL: %s", update_sql)
                    logger.info("å‚æ•°1 (doc_json): %s", doc_json_str)
                    logger.info("å‚æ•°2 (doc_info): %s", str(file_path))
                    
                    json_store.execute_sql(update_sql, [doc_json_str, str(file_path)])
                    logger.info("æ•°æ®æ›´æ–°æˆåŠŸ")
                else:
                    # æ’å…¥æ–°æ–‡æ¡£
                    insert_sql = """
                    INSERT INTO DOCUMENT_JSON (doc_info, doc_json) 
                    VALUES (:1, JSON(:2))
                    """
                    # è®°å½•å®é™…æ‰§è¡Œçš„SQLå’Œå‚æ•°
                    logger.info("æ‰§è¡Œæ’å…¥SQL: %s", insert_sql)
                    logger.info("å‚æ•°1 (doc_info): %s", str(file_path))
                    logger.info("å‚æ•°2 (doc_json): %s", doc_json_str)
                    
                    json_store.execute_sql(insert_sql, [str(file_path), doc_json_str])
                    logger.info("æ•°æ®æ’å…¥æˆåŠŸ")
                
                return structured_data
                
            except Exception as e:
                logger.error(f"SQLè¡Œè´¥: {str(e)}")
                return None
        
    except Exception as e:
        logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
        return None

def search_similar_documents(query: str, top_k: int = 3):
    """å‘é‡æœç´¢å¹¶ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£"""
    try:
        # 1. å‘é‡æœç´¢
        vector = embeddings_model.encode([query])[0]
        with OracleVectorStore() as vector_store:
            results = vector_store.search_vectors([vector], top_k=top_k)
        
        if not results:
            return []

        # 2. ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£
        best_match = results[0]  # å–ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£
        
        prompt = f"""
        åŸºäºä»¥ä¸‹åŒ»ç–—æ–‡æ¡£å†…å®¹ï¼Œå›ç­”é—®é¢˜ï¼š{query}

        æ–‡æ¡£å†…å®¹ï¼š
        {best_match['content']}

        è¯·æä¾›è¯¦ç»†ä¸“ä¸šåˆ†æå’Œç­”æ¡ˆã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚
        """

        # ä½¿ç”¨æ–° OpenAI API
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æåŒ»ç–—æ–‡æ¡£å¹¶ä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )

        # 3. å°† GPT åˆ†æç»“æœæ·»åŠ åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ä¸­
        best_match['gpt_analysis'] = response.choices[0].message.content
        
        # è¿”å›æ‰€æœ‰æ£€ç´¢ç»“æœï¼Œä½†åªæœ‰ç›¸å…³çš„æ–‡ï¿½ï¿½åŒ…å« GPT åˆ†æ
        return results

    except Exception as e:
        logger.error(f"æœç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []

def analyze_query_with_gpt(query_text):
    """ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾å¹¶ç”ŸæˆOracle 23c JSONæŸ¥è¯¢æ¡ä»¶"""
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
        
        # æä¾›å®Œæ•´çš„JSONç»“æ„ä¿¡æ¯
        json_structure = """
{
    "æ‚£è€…å§“å": "å§“å",
    "æ€§åˆ«": "ç”·/å¥³",
    "å¹´é¾„": "æ•°å­—",
    "å…¥é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "ä¸»è¯‰": "ä¸»è¦çŠ¶",
    "ç°ç—…å²": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "å…¥é™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‡ºé™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "ç”Ÿå‘½ä½“å¾": {
        "ä½“æ¸©": "åŒ…å«å•ä½",
        "è¡€å‹": "åŒ…å«å•ä½"
    },
    "ç”ŸåŒ–æŒ‡æ ‡": {
        "å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶": "å€¼å’Œå•ä½",
        "ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶": "å€¼å’Œå•ä½",
        "ç™½ç»†èƒ": "å€¼å’Œå•ä½",
        "æ·‹å·´ç»†èƒç™¾åˆ†æ¯”": "å€¼å’Œå•ä½",
        "ä¸­æ€§ç²’ç»†èƒç™¾åˆ†æ¯”": "å€¼å’Œå•ä½",
        "è¡€çº¢è›‹ç™½": "å€¼å’Œå•ä½",
        "è¡€å°æ¿": "å€¼å’Œå•ä½"
    },
    "è¯Šç–—ç»è¿‡": "æ²»ç–—è¿‡ç¨‹æè¿°",
    "å‡ºé™¢åŒ»å˜±": ["åŒ»å˜±1", "åŒ»å˜±2"]
}"""
        
        messages = [
            {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä¸ªOracle 23c JSONæŸ¥è¯¢ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹çš„JSONæ–‡æ¡£ç»“æ„ï¼Œå¸®åŠ©ç”Ÿæˆå‡†ç¡®çš„Oracle JSONæŸ¥è¯¢æ¡ä»¶ã€‚

æ–‡æ¡£ç»“æ„ï¼š
{json_structure}

æ•°æ®åº“ä¿¡æ¯ï¼š
- ä½¿ç”¨Oracle Database 23c
- è¡¨åï¼šDOCUMENT_JSON
- å­—æ®µï¼šdoc_info (VARCHAR2), doc_json (JSON)
- JSONæ•°æ®å­˜å‚¨åœ¨doc_jsonå­—æ®µä¸­

Oracle 23c JSONæŸ¥è¯¢ç‰¹æ€§ï¼š
1. ä½¿ç”¨ JSON_EXISTS è¿›è¡Œæ¡ä»¶åŒ¹é…
2. ä½¿ç”¨ JSON_VALUE æå–å•ä¸ªå€¼
3. ä½¿ç”¨ JSON_QUERY æå–JSONæ•°ç»„æˆ–å¯¹è±¡
4. æ”¯æŒç‚¹å·è®¿é—®åµŒå¥—å±æ€§
5. æ”¯æŒï¿½ï¿½ç»„ç´¢å¼•è®¿é—®
6. æ”¯æŒæ¡ä»¶è¿‡æ»¤ï¼š?(@ == "value")
7. æ”¯æŒé”®ååŒ¹é…ï¼š
   - ä½¿ç”¨ $.ç”ŸåŒ–æŒ‡æ ‡.* éå†æ‰€æœ‰æŒ‡æ ‡
   - ä½¿ç”¨ EXISTS å’Œ OR ç»„åˆå¤šä¸ªæ¡ä»¶

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
2. æ ¹æ®æ–‡æ¡£ç»“æ„å’ŒOracle 23cç‰¹æ€§ï¼Œç”Ÿæˆæœ€ä¼˜çš„æŸ¥è¯¢æ¡ä»¶
3. å¯¹äºåŒ»å­¦æœ¯è¯­ï¼Œè€ƒè™‘åŒä¹‰è¯å’Œç®€å†™ï¼ˆå¦‚"è½¬æ°¨é…¶"å¯èƒ½æŒ‡"å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"æˆ–"ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"ï¼‰
4. è¿”å›æ ¼å¼ä¸ºJSONï¼š
{{
    "query_type": "æŸ¥è¯¢ç±»å‹",
    "conditions": ["Oracle JSONæŸ¥è¯¢æ¡ä»¶"],
    "fields": ["éœ€è¦è¿”å›çš„æ®µ"],
    "keywords": ["é”®"]
}}

ç¤ºä¾‹1ï¼š
è¾“å…¥ï¼š"é©¬æŸæŸçš„è½¬æ°¨é…¶æŒ‡æ ‡"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@ == \"é©¬æŸæŸ\")')",
        "(JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶') OR JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶'))"
    ],
    "fields": ["ç”ŸåŒ–æŒ‡æ ‡.å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶", "ç”ŸåŒ–æŒ‡æ ‡.ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"],
    "keywords": ["é©¬æŸæŸ", "è½¬æ°¨é…¶"]
}}

ç¤ºä¾‹2ï¼š
è¾“å…¥ï¼š"æŸ¥çœ‹é©¬æŸæŸçš„æ·‹å·´ç»†èƒæ¯”ä¾‹"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£ï¿½ï¿½ï¿½å§“å?(@ == \"é©¬æŸæŸ\")')",
        "JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.æ·‹å·´ç»†èƒç™¾åˆ†æ¯”')"
    ],
    "fields": ["ç”ŸåŒ–æŒ‡æ ‡.æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"],
    "keywords": ["é©¬æŸæŸ", "æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"]
}}

ç¤ºä¾‹3ï¼š
è¾“å…¥ï¼š"é©¬æŸæŸçš„è¡€æ¶²ç›¸å…³æŒ‡æ ‡"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@ == \"é©¬æŸæŸ\")')",
        "(JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.è¡€çº¢è›‹ç™½') OR JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.è¡€å°æ¿'))"
    ],
    "fields": [
        "ç”ŸåŒ–æŒ‡æ ‡.ç™½ç»†èƒ",
        "ç”ŸåŒ–æŒ‡æ ‡.è¡€çº¢è›‹ç™½",
        "ç”ŸåŒ–æŒ‡æ ‡.è¡€å°æ¿"
    ],
    "keywords": ["é©¬æŸæŸ"]
}}"""},
            {"role": "user", "content": query_text}
        ]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPTåˆ†æè¯¢å¤±è´¥: {str(e)}")
        return None

# é…ç½®å¸¸é‡
TOP_K = 5  # æœç´¢ç»“æœè¿”å›çš„æœ€å¤§æ•°é‡

def normalize_medical_term(query_text):
    """ä½¿ç”¨ GPT å°†ç”¨æˆ·æŸ¥è¯¢çš„æ ‡åç§°æ ‡å‡†åŒ–"""
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
        
        messages = [
            {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æŒ‡æ ‡åç§°æ ‡å‡†åŒ–ä¸“å®¶ã€‚
è¯·å°†ç”¨æˆ·æŸ¥è¯¢ä¸­çš„æŒ‡æ ‡åç§°è½¬æ¢ä¸ºæ ‡å‡†çš„åŒ»ç–—æŒ‡æ ‡åç§°ã€‚

è§„åˆ™ï¼š
1. å¦‚æœæŸ¥è¯¢ä¸­åŒ…å«æŸä¸ªæ£€éªŒæŒ‡æ ‡çš„åŒä¹‰è¯æˆ–è¿‘ä¹‰è¯ï¼Œè¿”å›æ ‡å‡†åç§°
2. å¦‚æœä¸ç¡®å®šï¼Œè¿”å›åŸå§‹è¯è¯­
3. è¿”å›æ ¼å¼ä¸º JSONï¼š{"standard_term": "æ ‡å‡†åç§°"}

ç¤ºï¼š
è¾“å…¥ï¼š"æ·‹å·´ç»†èƒæ¯”ä¾‹"
è¾“å‡ºï¼š{"standard_term": "æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"}

è¾“å…¥ï¼š"ç™½ç»†èƒè®¡æ•°"
è¾“å‡ºï¼š{"standard_term": "ç™½ç»†èƒ"}

è¾“å…¥ï¼š"è¡€çº¢è›‹ç™½å«é‡"
è¾“å‡ºï¼š{"standard_term": "è¡€çº¢è›‹ç™½"}"""},
            {"role": "user", "content": query_text}
        ]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        return result.get('standard_term', query_text)
        
    except Exception as e:
        logger.error(f"æŒ‡æ ‡åç§°æ ‡å‡†åŒ–å¤±è´¥: {str(e)}")
        return query_text

def search_documents(query_text):
    """åŸºäºGPTç”Ÿæˆçš„æŸ¥è¯¢æ¡ä»¶æœç´¢æ–‡æ¡£ï¼Œæ”¯æŒç»“æ„åŒ–æ•°æ®å’Œå…¨æ–‡æœç´¢"""
    try:
        # ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾å¹¶ç”ŸæˆæŸ¥è¯¢æ¡ä»¶
        analysis_result = analyze_query_with_gpt(query_text)
        logger.info(f"GPTåˆ†æç»“æœ: {json.dumps(analysis_result, ensure_ascii=False)}")
        
        if not analysis_result or 'conditions' not in analysis_result:
            logger.error("GPTåˆ†æç»“æœæ ¼å¼é”™è¯¯")
            return []
            
        # ä½¿ç”¨GPTç”Ÿæˆçš„æ¡ä»¶æ„å»ºæŸ¥è¯¢
        conditions = analysis_result.get('conditions', [])
        keywords = analysis_result.get('keywords', [])
        
        # ç»„åˆæ¡ä»¶ï¼ˆä½¿ç”¨ANDè¿æ¥å§“åå’Œå…¶ä»–æ¡ä»¶ï¼‰
        name_conditions = [c for c in conditions if 'æ‚£è€…å§“å' in c]
        other_conditions = [c for c in conditions if 'æ‚£è€…å§“å' not in c]
        
        # æ„å»ºJSONæŸ¥è¯¢æ¡ä»¶
        if name_conditions and other_conditions:
            json_where = f"({' OR '.join(name_conditions)}) AND ({' OR '.join(other_conditions)})"
        else:
            json_where = " OR ".join(conditions) if conditions else "1=1"
            
        # æ„å»ºå…¨æ–‡æœç´¢æ¡ä»¶ï¼ˆæ’é™¤å§“åå…³é”®è¯ï¼‰
        content_conditions = []
        for keyword in keywords:
            if keyword not in ["é©¬æŸæŸ", "å‘¨æŸæŸ"]:
                content_conditions.append(f"CONTAINS(content, '{keyword}') > 0")
        
        content_where = " OR ".join(content_conditions) if content_conditions else "1=1"
        
        # æ„å»ºå®Œæ•´çš„æŸ¥è¯¢è¯­å¥
        query = """
        SELECT d.doc_info,
               d.doc_json,
               d.content
        FROM DOCUMENT_JSON d
        WHERE 
            -- é¦–å…ˆåŒ¹é…æ‚£è€…å§“åï¼ˆJSONæˆ–å…¨æ–‡ï¼‰
            (
                JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@=="æ¨æŸæŸ")') 
                OR CONTAINS(content, :1) > 0
            )
        ORDER BY id DESC
        FETCH FIRST :2 ROWS ONLY
        """
        
        # è·å–å§“åå…³é”®è¯ï¼ˆå¦‚æœæœ‰ï¼‰
        name_keyword = next((k for k in keywords if k in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]), None)
        
        # æ‰§è¡ŒæŸ¥è¯¢
        with OracleJsonStore() as json_store:
            if name_keyword:
                # æ„å»ºå¸¦æœ‰å…·ä½“å§“åçš„æŸ¥è¯¢
                actual_query = query.replace('æ¨æŸæŸ', name_keyword)
                results = json_store.execute_search(actual_query, [name_keyword, TOP_K])
            else:
                results = json_store.execute_search(query, [TOP_K])
                
        if not results:
            logger.info("æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
            return []
                
        # å¤„ç†æŸ¥è¯¢ç»“æœ
        processed_results = []
        for row in results:
            try:
                doc_json = json.loads(json.dumps(row['doc_json'], cls=DecimalEncoder))
                processed_results.append({
                    'doc_info': row['doc_info'],
                    'doc_json': doc_json,
                    'content': row['content']
                })
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡æ¡£ç»“æœæ—¶å‡ºé”™: {str(e)}")
                continue
                    
        logger.info(f"æ•°æ®åº“çš„æ–‡æ¡£æ•°é‡: {len(processed_results)}")
        for result in processed_results:
            logger.info(f"æ–‡æ¡£è·¯å¾„: {result['doc_info']}")
                
        return processed_results
        
    except Exception as e:
        logger.error(f"JSONæ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
        return []

class DecimalEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Decimal):
            return str(obj)
        return super(DecimalEncoder, self).default(obj)

def generate_answer(query_text, doc_json, content=None):
    """æ ¹æ®æŸ¥è¯¢æ„å›¾ç”Ÿæˆç­”æ¡ˆï¼Œæ”¯æŒç»“æ„åŒ–æ•°æ®å’Œå…¨æ–‡å†…å®¹"""
    try:
        # ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾
        analysis_result = analyze_query_with_gpt(query_text)
        if not analysis_result:
            return None
            
        query_type = analysis_result.get('query_type', '')
        fields = analysis_result.get('fields', [])
        keywords = analysis_result.get('keywords', [])
        
        # è·å–æ‚£è€…å§“å
        name = next((k for k in keywords if k in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]), "æ‚£è€…")
        
        # å¦‚æœæŸ¥è¯¢åŒ…å«ç—‡çŠ¶æˆ–è¯Šæ–­ç›¸å…³çš„å…³é”®è¯ï¼Œä¼˜å…ˆä»å…¨æ–‡å†…å®¹ä¸­æå–
        symptom_keywords = [k for k in keywords if k not in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]]
        if symptom_keywords and content:
            try:
                # ä½¿ç”¨GPTä»å…¨æ–‡ä¸­æå–ç›¸å…³ä¿¡æ¯
                extract_prompt = f"""
ä»ä»¥ä¸‹åŒ»ç–—æ–‡æ¡£ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼š

é—®é¢˜ï¼š{query_text}
å…³æ³¨çš„ç—‡çŠ¶/è¯Šæ–­ï¼š{', '.join(symptom_keywords)}

æ–‡æ¡£å†…å®¹ï¼š
{content}

è¯·æä¾›ç®€æ´çš„ç­”æ¡ˆï¼Œé‡ç‚¹å…³æ³¨é—®é¢˜ä¸­æåˆ°çš„ç—‡çŠ¶æˆ–è¯Šæ–­ã€‚å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›å¤"æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚
"""
                
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—ä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»æ–‡æ¡£ä¸­å‡†ç¡®æå–ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ¨æµ‹çš„å†…å®¹ã€‚"},
                        {"role": "user", "content": extract_prompt}
                    ],
                    temperature=0.1
                )
                
                answer = response.choices[0].message.content.strip()
                if answer != "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯":
                    return f"{name}çš„æƒ…å†µï¼š{answer}"
            except Exception as e:
                logger.error(f"GPTåˆ†æå¤±è´¥: {str(e)}")
                # å¦‚æœGPTåˆ†æå¤±è´¥ï¼Œç»§ç»­å°è¯•ç»“æ„åŒ–æ•°æ®
                
        # å¦‚æœæ²¡æœ‰ä»å…¨æ–‡ä¸­æ‰¾åˆ°ä¿¡æ¯ï¼Œæˆ–GPTåˆ†æå¤±è´¥ï¼Œå°è¯•ä»ç»“æ„åŒ–æ•°æ®ä¸­è·å–
        info = []
        for field in fields:
            if '.' in field:  # å¤„ç†åµŒå¥—å­—æ®µ
                parent, child = field.split('.')
                if parent in doc_json and child in doc_json[parent]:
                    info.append(f"{child}æ˜¯{doc_json[parent][child]}")
            else:  # å¤„ç†é¡¶å±‚å­—æ®µ
                if field in doc_json:
                    info.append(f"{field}æ˜¯{doc_json[field]}")
        
        if info:
            if query_type:
                return f"{name}çš„{query_type}ï¼š" + "ï¼Œ".join(info)
            else:
                return f"{name}çš„ä¿¡æ¯ï¼š" + "ï¼Œ".join(info)
                
        return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}")
        return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯"

def display_search_results(query_text, results):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    if not results:
        st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return

    # å°è¯•ç”Ÿæˆç²¾ç¡®ç­”æ¡ˆ
    for result in results:
        answer = generate_answer(query_text, result['doc_json'], result['content'])
        if answer and not answer.startswith("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"):
            st.success(answer)
        else:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")

def main():
    st.title("åŒ»ç–—ç—…å†å¤„ç†ç³»ç»Ÿ")
    
    # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
    with st.spinner("æ­£åœ¨è¿æ¥æ•°æ®åº“..."):
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            init_database()
        except Exception as e:
            st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            st.info("è¯·æ£€æŸ¥ï¼š\n1. æ•°æ®åº“æœåŠ¡æ˜¯å¦å¯åŠ¨\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. æ•°æ®åº“é…ç½®æ˜¯å¦æ­£ç¡®")
            return
    
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œä¸ä¼šæ¯æ¬¡éƒ½ä¸‹è½½ï¼‰
    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œå¯èƒ½éœ€è¦ä¸‹è½½ï¼‰..."):
        try:
            embeddings_model = load_embeddings_model()
            st.success("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        except Exception as e:
            st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            st.info("å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œæ‚¨å¯ä»¥ï¼š\n1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n2. ç­‰å¾…ä¸€ä¼šå„¿å†è¯•\n3. æˆ–è€…æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ~/.cache/torch/sentence_transformers")
            return
    
    # ä¾§è¾¹æ ï¼šåŠŸèƒ½é€‰æ‹©
    with st.sidebar:
        st.header("åŠŸèƒ½é€‰æ‹©")
        mode = st.radio(
            "é€‰æ‹©åŠŸèƒ½",
            ["æ–‡æ¡£ç®¡ç†", "å‘é‡æ£€ç´¢", "ç»“æ„åŒ–æ£€ç´¢"]
        )
    
    if mode == "æ–‡æ¡£ç®¡ç†":
        st.header("æ–‡æ¡£ç®¡ç†")
        
        # æ‰¹é‡æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ ç—…å†æ–‡æ¡£ï¼ˆå¯å¤šé€‰ï¼‰", 
            type=["txt", "docx", "pdf"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            col1, col2 = st.columns([2, 1])
            with col1:
                st.write(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
            with col2:
                if st.button("ä¿å­˜æ‰€æœ‰æ–‡æ¡£"):
                    for uploaded_file in uploaded_files:
                        with st.spinner(f"æ­£åœ¨ä¿å­˜ {uploaded_file.name}..."):
                            file_path = save_uploaded_file(uploaded_file)
                            st.success(f"å·²ä¿å­˜: {uploaded_file.name}")
        
        # æ˜¾ç¤ºå·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        st.subheader("å·²ä¸Šä¼ çš„æ–‡ä»¶")
        files = get_uploaded_files()
        if not files:
            st.info("æš‚æ— ä¸Šä¼ çš„æ–‡ä»¶")
        else:
            # æ·»åŠ æ‰¹é‡æ“ä½œæŒ‰é’®
            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button("æ‰¹é‡å‘é‡åŒ–"):
                    for file in files:
                        with st.spinner(f"æ­£åœ¨å‘é‡åŒ– {file.name}..."):
                            if vectorize_document(file):
                                st.success(f"{file.name} å‘é‡åŒ–å®Œæˆ")
                            else:
                                st.error(f"{file.name} å‘é‡åŒ–å¤±è´¥")
            with col2:
                if st.button("æ‰¹é‡ç»“æ„åŒ–"):
                    for file in files:
                        with st.spinner(f"æ­£åœ¨ç»“æ„åŒ– {file.name}..."):
                            result = parse_document_to_json(file)
                            if result:
                                st.success(f"{file.name} ç»“æ„åŒ–å®Œæˆ")
                                with st.expander("æŸ¥çœ‹ç»“æ„åŒ–æ•°æ®"):
                                    st.json(result)
                            else:
                                st.error(f"{file.name} ç»“æ„åŒ–å¤±è´¥")
            
            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            for file in files:
                col1, col2, col3 = st.columns([3, 1, 1])
                with col1:
                    st.write(file.name)
                with col2:
                    if st.button("å‘é‡åŒ–", key=f"vec_{file.name}"):
                        with st.spinner("æ­£åœ¨å‘é‡åŒ–..."):
                            if vectorize_document(file):
                                st.success("å‘é‡åŒ–å®Œæˆ")
                            else:
                                st.error("å‘é‡åŒ–å¤±è´¥")
                with col3:
                    if st.button("ï¿½ï¿½æ„åŒ–", key=f"struct_{file.name}"):
                        with st.spinner("æ­£åœ¨ç»“æ„åŒ–..."):
                            result = parse_document_to_json(file)
                            if result:
                                st.success("ç»“æ„åŒ–å®Œæˆ")
                                with st.expander("æŸ¥çœ‹ç»“æ„åŒ–æ•°æ®"):
                                    st.json(result)
                            else:
                                st.error("ç»“æ„åŒ–å¤±è´¥")
    
    elif mode == "å‘é‡æ£€ç´¢":
        st.header("å‘é‡æ£€ç´¢")
        
        # æ˜¾ç¤ºå·²å‘é‡åŒ–çš„æ–‡æ¡£åˆ—è¡¨
        with st.expander("æŸ¥çœ‹å·²å‘é‡åŒ–çš„æ–‡æ¡£", expanded=True):
            with OracleVectorStore() as vector_store:
                documents = vector_store.list_documents()
                if documents:
                    st.write("å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼š")
                    for doc in documents:
                        st.write(f"- {doc['file_path']} (å‘é‡æ•°: {doc['chunk_count']})")
                else:
                    st.info("æš‚æ— å‘é‡åŒ–æ–‡æ¡£")
        
        # æœç´¢åŠŸèƒ½
        query = st.text_input("è¾“æœç´¢å†…å®¹")
        if query:
            with st.spinner("æ­£åœ¨æœç´¢å¹¶åˆ†æ..."):
                results = search_similar_documents(query, top_k=3)
                
                if results:
                    # æ˜¾ç¤ºæœ€ç›¸å…³æ–‡æ¡£çš„ GPT åˆ†æç»“æœ
                    st.subheader("GPT åˆ†æç»“æœ")
                    st.write(results[0]['gpt_analysis'])
                    
                    # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æ–‡æ¡£
                    st.subheader("ç›¸å…³æ–‡æ¡£")
                    for i, result in enumerate(results, 1):
                        similarity = 1 - result['similarity']
                        with st.expander(f"æ–‡æ¡£ {i}: {result['file_path']} (ç›¸ä¼¼åº¦: {similarity:.2%})"):
                            st.write(result['content'])
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
    
    else:  # ç»“æ„åŒ–æ£€ç´¢
        st.header("ç»“æ„åŒ–æ£€ç´¢")
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„ç»“æ„åŒ–æ–‡æ¡£
        with OracleJsonStore() as json_store:
            try:
                # é¦–å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                check_table_sql = """
                SELECT COUNT(*) as count
                FROM user_tables 
                WHERE table_name = 'DOCUMENT_JSON'
                """
                result = json_store.execute_search(check_table_sql)
                table_exists = result[0]['count'] > 0 if result else False
                
                if not table_exists:
                    st.warning("æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåœ¨æ–‡æ¡£ç®¡ç†ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
                    return
                
                # è·å–æ‰€æœ‰æ–‡æ¡£
                check_sql = """
                SELECT doc_info, doc_json, content 
                FROM DOCUMENT_JSON 
                ORDER BY id DESC
                """
                all_docs = json_store.execute_search(check_sql)
                
                # æ˜¾ç¤ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
                st.subheader("ğŸ“š æ•°æ®åº“ä¸­æ‰€æœ‰æ–‡æ¡£")
                if all_docs:
                    st.write(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£")
                    for doc in all_docs:
                        st.markdown(f"### ğŸ“„ {Path(doc['doc_info']).name}")
                        if isinstance(doc['doc_json'], dict):
                            data = doc['doc_json']
                            
                            # ä½¿ç”¨tabsæ¥ç»„ç»‡å†…å®¹
                            tabs = st.tabs([
                                "åŸºæœ¬ä¿¡æ¯", "ä¸»è¯‰ä¸è¯Šæ–­", "ç°ç—…å²", 
                                "ç”Ÿå‘½ä½“å¾", "ç”ŸåŒ–æŒ‡æ ‡", "è¯Šç–—ç»è¿‡", "å…¨æ–‡å†…å®¹"
                            ])
                            
                            with tabs[0]:
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.markdown("**æ‚£è€…ä¿¡æ¯**")
                                    info = {
                                        "å§“å": data.get("æ‚£è€…å§“å", "æœªçŸ¥"),
                                        "æ€§åˆ«": data.get("æ€§åˆ«", "æœªçŸ¥"),
                                        "å¹´é¾„": data.get("å¹´é¾„", "æœªçŸ¥"),
                                        "æ°‘æ—": data.get("æ°‘æ—", "æœªçŸ¥"),
                                        "èŒä¸š": data.get("èŒä¸š", "æœª"),
                                        "å©šå§»çŠ¶å†µ": data.get("å©šå§»çŠ¶å†µ", "æœªçŸ¥")
                                    }
                                    st.json(info)
                                with col2:
                                    st.markdown("**ä½é™¢ä¿¡æ¯**")
                                    info = {
                                        "å…¥é™¢æ—¥æœŸ": data.get("å…¥é™¢æ—¥æœŸ", "æœªçŸ¥"),
                                        "å‡ºé™¢æ—¥æœŸ": data.get("å‡ºé™¢æ—¥æœŸ", "æœªçŸ¥"),
                                        "ä½é™¢å¤©æ•°": data.get("ä½é™¢å¤©æ•°", "æœªçŸ¥"),
                                        "å‡ºé™¢æƒ…å†µ": data.get("å‡ºé™¢æƒ…å†µ", "æœªçŸ¥")
                                    }
                                    st.json(info)
                            
                            with tabs[1]:
                                st.markdown("**ä¸»è¯‰**")
                                st.write(data.get("ä¸»è¯‰", "æœªè®°å½•"))
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.markdown("**å…¥é™¢è¯Šæ–­**")
                                    for diag in data.get("å…¥é™¢è¯Šæ–­", []):
                                        st.write(f"- {diag}")
                                with col2:
                                    st.markdown("**å‡ºé™¢è¯Šæ–­**")
                                    for diag in data.get("å‡ºé™¢è¯Šæ–­", []):
                                        st.write(f"- {diag}")
                            
                            with tabs[2]:
                                st.markdown("**ç°ç—…å²**")
                                for item in data.get("ç°ç—…å²", []):
                                    st.write(f"- {item}")
                            
                            with tabs[3]:
                                if "ç”Ÿå‘½ä½“å¾" in data:
                                    st.json(data["ç”Ÿå‘½ä½“å¾"])
                                else:
                                    st.info("æœªè®°å½•ç”Ÿå‘½ä½“å¾")
                            
                            with tabs[4]:
                                if "ç”ŸåŒ–æŒ‡æ ‡" in data:
                                    st.json(data["ç”ŸåŒ–æŒ‡æ ‡"])
                                else:
                                    st.info("æœªè®°å½•ç”ŸåŒ–æŒ‡æ ‡")
                            
                            with tabs[5]:
                                st.markdown("**è¯Šç–—ç»è¿‡**")
                                st.write(data.get("è¯Šç–—ç»è¿‡", "æœªè®°å½•"))
                                if "å‡ºé™¢åŒ»å˜±" in data:
                                    st.markdown("**å‡ºé™¢åŒ»å˜±**")
                                    for advice in data["å‡ºé™¢åŒ»å˜±"]:
                                        st.write(f"- {advice}")
                                        
                            with tabs[6]:
                                st.markdown("**æ–‡æ¡£å…¨æ–‡**")
                                if "content" in doc:
                                    st.text_area("", doc["content"], height=400)
                                else:
                                    st.info("æœªæ‰¾åˆ°æ–‡æ¡£å…¨æ–‡å†…å®¹ï¼Œè¯·è¿è¡Œ update_content.py æ›´æ–°å…¨æ–‡å†…å®¹")
                        
                        st.markdown("---")
                else:
                    st.info("ğŸ“­ æ•°æ®åº“ä¸­æš‚æ— ç»“æ„åŒ–æ–‡æ¡£ï¼Œè¯·å…ˆåœ¨æ–‡æ¡£ç®¡ï¿½ï¿½ï¿½ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
                
                # æœç´¢åŠŸèƒ½
                st.divider()
                st.subheader("ğŸ” æ™ºèƒ½æœç´¢")
                query = st.text_input("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼ˆæ”¯æŒç»“æ„åŒ–æ•°æ®å’Œå…¨æ–‡æœç´¢ï¼‰")
                
                if query:
                    with st.spinner("æ­£åœ¨åˆ†ææŸ¥è¯¢å¹¶æœç´¢..."):
                        results = search_documents(query)
                        if results:
                            for result in results:
                                answer = generate_answer(query, result['doc_json'], result['content'])
                                if answer:
                                    st.success(answer)
                                    with st.expander("æŸ¥çœ‹å®Œæ•´æ–‡æ¡£"):
                                        display_search_results(query, [result])
                        else:
                            st.warning("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                        
            except Exception as e:
                logger.error(f"æ£€ç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.error(f"æ£€ç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()

