"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - Oracle Vector Storeç‰ˆæœ¬
"""

import streamlit as st
from utils.oracle_vector_store import OracleVectorStore
from utils.oracle_json_store import OracleJsonStore
from utils.medical_record_parser import MedicalRecordParser
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import shutil
from pathlib import Path
import logging
import openai
import pdfplumber
import json
import datetime
import hashlib
from typing import Dict, Any, List

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è·å–OpenAIé…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

# è®¾ç½®æ–‡æ¡£å­˜å‚¨ç›®å½•
UPLOAD_DIR = Path("uploaded_documents")
UPLOAD_DIR.mkdir(exist_ok=True)

# è®¾ç½®JSONç¼“å­˜ç›®å½•
CACHE_DIR = Path("json_cache")
CACHE_DIR.mkdir(exist_ok=True)

# ä½¿ç”¨ st.cache_resource ç¼“å­˜æ¨¡å‹ï¼Œå¹¶éšè—åŠ è½½çŠ¶æ€
@st.cache_resource(show_spinner=False)
def load_embeddings_model():
    """åŠ è½½å‘é‡åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰"""
    logger.debug("åŠ è½½å‘é‡åŒ–æ¨¡å‹")
    return SentenceTransformer('all-MiniLM-L6-v2')

# åˆå§‹åŒ–æ¨¡å‹
embeddings_model = load_embeddings_model()

def get_cache_path(file_path: str) -> Path:
    """æ ¹æ®æ–‡ä»¶è·¯å¾„ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # è·å–åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    original_name = Path(file_path).stem
    return CACHE_DIR / f"{original_name}.json"

def save_to_cache(file_path: str, data: Dict) -> None:
    """ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°ç¼“å­˜"""
    cache_path = get_cache_path(file_path)
    with open(cache_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"ç¼“å­˜ä¿å­˜åˆ°: {cache_path}")

def load_from_cache(file_path: str) -> Dict:
    """ä»ç¼“å­˜åŠ è½½ç»“æ„åŒ–æ•°æ®"""
    cache_path = get_cache_path(file_path)
    if cache_path.exists():
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"ä»ç¼“å­˜åŠ è½½: {cache_path}")
        return data
    return None

def save_uploaded_file(uploaded_file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•"""
    file_path = UPLOAD_DIR / uploaded_file.name
    with open(file_path, "wb") as f:
        shutil.copyfileobj(uploaded_file, f)
    return str(file_path)

def read_file_content(file_path):
    """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨ pdfplumber å¤„ç† PDF"""
    try:
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_extension = str(file_path).lower().split('.')[-1]
        
        # PDFæ–‡ä»¶å¤„ç†
        if file_extension == 'pdf':
            with pdfplumber.open(file_path) as pdf:
                text = ''
                for page in pdf.pages:
                    text += page.extract_text() + '\n'
                return text.strip()
        
        # æ–‡æœ¬æ–‡ä»¶å¤„ç†
        else:
            encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read().strip()
                except UnicodeDecodeError:
                    continue
        
        return None
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        return None

def get_uploaded_files():
    """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
    return list(UPLOAD_DIR.glob("*.*"))

def vectorize_document(file_path):
    """å‘é‡åŒ–æ–‡æ¡£"""
    try:
        content = read_file_content(file_path)
        # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹è¿›è¡Œå‘é‡åŒ–
        vector = embeddings_model.encode([content])[0]
        documents = [{"file_path": str(file_path), "content": content}]
        
        with OracleVectorStore() as vector_store:
            vector_store.add_vectors([vector], documents)
        return True
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤±è´¥: {str(e)}")
        return False

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    try:
        with OracleJsonStore() as json_store:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_table_sql = """
            SELECT COUNT(*) as count
            FROM user_tables 
            WHERE table_name = 'DOCUMENT_JSON'
            """
            result = json_store.execute_search(check_table_sql)
            table_exists = result[0]['count'] > 0 if result else False
            
            if not table_exists:
                # åˆ›å»ºç»“æ„åŒ–æ–‡æ¡£è¡¨
                create_json_table_sql = """
                CREATE TABLE DOCUMENT_JSON (
                    id NUMBER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                    doc_info VARCHAR2(500),
                    doc_json JSON
                )
                """
                json_store.execute_sql(create_json_table_sql)
                logger.info("æˆåŠŸåˆ›å»ºDOCUMENT_JSONè¡¨")
            else:
                logger.debug("DOCUMENT_JSONè¡¨å·²å­˜åœ¨")  # æ”¹ç”¨debugçº§åˆ«çš„æ—¥å¿—

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")
        st.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")

class MedicalRecordParser:
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )

    def parse_medical_record(self, text: str, doc_info: str) -> Dict[str, Any]:
        """è§£æåŒ»ç–—è®°å½•æ–‡æœ¬ï¼Œç”Ÿæˆç»“æ„åŒ–JSONæ•°æ®å’ŒSQLæ’å…¥è¯­å¥"""
        try:
            prompt = f"""è¯·å°†ä»¥ä¸‹ç—…å†å†…å®¹ç»“æ„åŒ–ä¸ºJSONæ ¼å¼ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„SQLæ’å…¥è¯­å¥ã€‚

ç—…å†å†…å®¹ï¼š
{text}

è¦æ±‚ï¼š
1. JSONç»“æ„ï¼š
{{
    "æ‚£è€…å§“å": "æŸæŸï¼ˆä¿æŠ¤éšç§ï¼‰",
    "æ€§åˆ«": "æ€§åˆ«",
    "å¹´é¾„": "æ•°å­—",
    "å…¥é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "å‡ºé™¢æ—¥æœŸ": "YYYY-MM-DD",
    "ä¸»è¯‰": "ä¸»è¦ç—‡çŠ¶",
    "ç°ç—…å²": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "å…¥é™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‡ºé™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "ç”Ÿå‘½ä½“å¾": {{
        "ä½“æ¸©": "åŒ…å«å•ä½",
        "è¡€å‹": "åŒ…å«å•ä½"
    }},
    "ç”ŸåŒ–æŒ‡æ ‡": {{
        "æŒ‡æ ‡å": "å€¼å’Œå•ä½ï¼Œå¼‚å¸¸æ ‡â†‘â†“"
    }},
    "è¯Šç–—ç»è¿‡": "æ²»ç–—è¿‡ç¨‹æè¿°",
    "å‡ºé™¢åŒ»å˜±": ["åŒ»å˜±1", "åŒ»å˜±2"]
}}

2. SQLè¦æ±‚ï¼š
- è¡¨åï¼šDOCUMENT_JSON
- å­—æ®µï¼š(id NUMBERè‡ªå¢, doc_info VARCHAR2(500), doc_json JSON)
- doc_infoå€¼ï¼š{doc_info}
- ä½¿ç”¨JSON_OBJECTæˆ–FORMAT JSONè¯­æ³•

è¯·è¿”å›ï¼š
{{
    "structured_data": JSONæ ¼å¼çš„ç—…å†æ•°æ®,
    "sql_statement": Oracleæ’å…¥è¯­å¥
}}"""

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯åŒ»ç–—æ•°æ®ç»“æ„åŒ–ä¸“å®¶ï¼Œæ“…é•¿è§£æç—…å†æ–‡æœ¬å¹¶ç”Ÿæˆè§„èŒƒçš„JSONå’ŒSQLã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )

            # è§£æè¿”å›çš„JSON
            result = json.loads(response.choices[0].message.content)
            
            # éªŒè¯ç»“æœæ ¼å¼
            if not isinstance(result, dict) or 'structured_data' not in result or 'sql_statement' not in result:
                raise ValueError("GPTè¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®")

            # æ·»åŠ å…ƒæ•°æ®
            if 'metadata' not in result['structured_data']:
                result['structured_data']['metadata'] = {}
            result['structured_data']['metadata'].update({
                'import_time': datetime.datetime.now().isoformat(),
                'source_type': 'text',
                'last_updated': datetime.datetime.now().isoformat()
            })

            return result

        except Exception as e:
            logger.error(f"è§£æåŒ»ç–—è®°å½•å¤±è´¥: {str(e)}")
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}

def parse_document_to_json(file_path):
    """è§£ææ–‡æ¡£ä¸ºç»“æ„åŒ–JSONå¹¶ç”ŸæˆSQLæ’å…¥è¯­å¥"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = read_file_content(file_path)
        if not content:
            logger.error(f"æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {file_path}")
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cached_data = load_from_cache(file_path)
        if cached_data:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„ç»“æ„åŒ–æ•°æ®")
            structured_data = cached_data
        else:
            # ä½¿ç”¨è§£æå™¨å¤„ç†
            parser = MedicalRecordParser()
            doc_info = str(file_path)
            result = parser.parse_medical_record(content, doc_info)
            
            # å¦‚æœè¿”å›çš„æ˜¯é”™è¯¯ä¿¡æ¯ï¼Œè¿”å› None
            if 'error' in result:
                logger.error(f"è§£æå¤±è´¥: {result['error']}")
                return None
            
            structured_data = result['structured_data']
            # ä¿å­˜åˆ°ç¼“å­˜
            save_to_cache(file_path, structured_data)
        
        # æ‰§è¡ŒSQLæ’å…¥
        with OracleJsonStore() as json_store:
            try:
                # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
                check_sql = """
                SELECT COUNT(*) as count 
                FROM DOCUMENT_JSON 
                WHERE doc_info = :1
                """
                result = json_store.execute_search(check_sql, [str(file_path)])
                doc_exists = result[0]['count'] > 0 if result else False
                
                # å°†æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                doc_json_str = json.dumps(structured_data, ensure_ascii=False)
                
                if doc_exists:
                    # æ›´æ–°ç°æœ‰æ–‡æ¡£
                    update_sql = """
                    UPDATE DOCUMENT_JSON 
                    SET doc_json = JSON(:1)
                    WHERE doc_info = :2
                    """
                    # è®°å½•å®é™…æ‰§è¡Œçš„SQLå’Œå‚æ•°
                    logger.info("æ‰§è¡Œæ›´æ–°SQL: %s", update_sql)
                    logger.info("å‚æ•°1 (doc_json): %s", doc_json_str)
                    logger.info("å‚æ•°2 (doc_info): %s", str(file_path))
                    
                    json_store.execute_sql(update_sql, [doc_json_str, str(file_path)])
                    logger.info("æ•°æ®æ›´æ–°æˆåŠŸ")
                else:
                    # æ’å…¥æ–°æ–‡æ¡£
                    insert_sql = """
                    INSERT INTO DOCUMENT_JSON (doc_info, doc_json) 
                    VALUES (:1, JSON(:2))
                    """
                    # è®°å½•å®é™…æ‰§è¡Œçš„SQLå’Œå‚æ•°
                    logger.info("æ‰§è¡Œæ’å…¥SQL: %s", insert_sql)
                    logger.info("å‚æ•°1 (doc_info): %s", str(file_path))
                    logger.info("å‚æ•°2 (doc_json): %s", doc_json_str)
                    
                    json_store.execute_sql(insert_sql, [str(file_path), doc_json_str])
                    logger.info("æ•°æ®æ’å…¥æˆåŠŸ")
                
                return structured_data
                
            except Exception as e:
                logger.error(f"SQLæ‰§è¡Œå¤±è´¥: {str(e)}")
                return None
        
    except Exception as e:
        logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
        return None

def search_similar_documents(query: str, top_k: int = 3):
    """å‘é‡æœç´¢å¹¶ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£"""
    try:
        # 1. å‘é‡æœç´¢
        vector = embeddings_model.encode([query])[0]
        with OracleVectorStore() as vector_store:
            results = vector_store.search_vectors([vector], top_k=top_k)
        
        if not results:
            return []

        # 2. ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£
        best_match = results[0]  # å–ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£
        
        prompt = f"""
        åŸºäºä»¥ä¸‹åŒ»ç–—æ–‡æ¡£å†…å®¹ï¼Œå›ç­”é—®é¢˜ï¼š{query}

        æ–‡æ¡£å†…å®¹ï¼š
        {best_match['content']}

        è¯·æä¾›è¯¦ç»†çš„ä¸“ä¸šåˆ†æå’Œç­”æ¡ˆã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚
        """

        # ä½¿ç”¨æ–°ç‰ˆ OpenAI API
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æåŒ»ç–—æ–‡æ¡£å¹¶æä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )

        # 3. å°† GPT åˆ†æç»“æœæ·»åŠ åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ä¸­
        best_match['gpt_analysis'] = response.choices[0].message.content
        
        # è¿”å›æ‰€æœ‰æ£€ç´¢ç»“æœï¼Œä½†åªæœ‰æœ€ç›¸å…³çš„æ–‡æ¡£åŒ…å« GPT åˆ†æ
        return results

    except Exception as e:
        logger.error(f"æœç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []

def analyze_query_with_gpt(query: str) -> Dict[str, Any]:
    """ä½¿ç”¨ GPT åˆ†æé—®é¢˜å¹¶æå–æœç´¢å…³é”®è¯"""
    try:
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )
        
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹åŒ»ç–—ç›¸å…³çš„é—®é¢˜ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶æ„é€ æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶ã€‚
        
        é—®é¢˜ï¼š{query}
        
        è¯·è¿”å›ä¸€ä¸ªJSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        1. keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
        2. fields: åº”è¯¥åœ¨å“ªäº›å­—æ®µä¸­æœç´¢ï¼ˆä¾‹å¦‚ï¼šbasic_information, chief_complaint, diagnosisç­‰ï¼‰
        3. search_type: æœç´¢ç±»å‹ï¼ˆexactç²¾ç¡®åŒ¹é…è¿˜æ˜¯fuzzyæ¨¡ç³ŠåŒ¹é…ï¼‰
        
        ç¤ºä¾‹è¿”å›æ ¼å¼ï¼š
        {{
            "keywords": ["å‘çƒ§", "å’³å—½"],
            "fields": ["chief_complaint", "present_illness_history"],
            "search_type": "fuzzy"
        }}
        """

        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯æ£€ç´¢ä¸“å®¶ï¼Œå¸®åŠ©æ„é€ ç²¾ç¡®çš„æœç´¢æ¡ä»¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPTåˆ†ææŸ¥è¯¢å¤±è´¥: {str(e)}")
        return None

def search_json_documents(query: str, top_k: int = 3) -> List[Dict[str, Any]]:
    """åŸºäºGPTåˆ†æç»“æœæœç´¢æ–‡æ¡£"""
    try:
        # 1. ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢
        analysis = analyze_query_with_gpt(query)
        if not analysis:
            return []
            
        logger.info(f"GPTåˆ†æç»“æœ: {json.dumps(analysis, ensure_ascii=False)}")
        
        # 2. æ„é€ Oracle JSONæŸ¥è¯¢
        with OracleJsonStore() as json_store:
            search_conditions = []
            
            # ä¸ºæ¯ä¸ªå­—æ®µæ„é€ æŸ¥è¯¢æ¡ä»¶
            for field in analysis['fields']:
                for keyword in analysis['keywords']:
                    if analysis['search_type'] == 'exact':
                        condition = f"JSON_EXISTS(doc_json, '$.structured_data.{field}?(@ == \"{keyword}\")')"
                    else:
                        condition = f"JSON_EXISTS(doc_json, '$.structured_data.{field}?(@ like_regex \"{keyword}\" flag \"i\")')"
                    search_conditions.append(condition)
            
            # ç»„åˆæŸ¥è¯¢æ¡ä»¶
            where_clause = " OR ".join(search_conditions)
            
            search_sql = f"""
                SELECT d.doc_info,
                       d.doc_json,
                       1 as relevance
                FROM DOCUMENT_JSON d
                WHERE {where_clause}
                FETCH FIRST :1 ROWS ONLY
            """
            
            logger.info(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {search_sql}")
            logger.info(f"æŸ¥è¯¢å‚æ•°: top_k={top_k}")
            
            # å…ˆè·å–æ‰€æœ‰æ–‡æ¡£ï¼Œçœ‹çœ‹æ•°æ®åº“ä¸­æœ‰ä»€ä¹ˆ
            check_sql = "SELECT doc_info, doc_json FROM DOCUMENT_JSON"
            all_docs = json_store.execute_search(check_sql, [])
            logger.info(f"æ•°æ®åº“çš„æ–‡æ¡£æ•°é‡: {len(all_docs)}")
            for doc in all_docs:
                logger.info(f"æ–‡æ¡£è·¯å¾„: {doc['doc_info']}")
                logger.info(f"æ–‡æ¡£å†…å®¹: {json.dumps(doc['doc_json'], ensure_ascii=False)}")
            
            # æ‰§è¡Œå®é™…æŸ¥è¯¢
            results = json_store.execute_search(search_sql, [top_k])
            logger.info(f"æŸ¥è¯¢ç»“æœæ•°é‡: {len(results)}")
            return results
            
    except Exception as e:
        logger.error(f"JSONæ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
        return []

def main():
    st.title("åŒ»ç–—ç—…å†å¤„ç†ç³»ç»Ÿ")
    
    # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
    with st.spinner("æ­£åœ¨è¿æ¥æ•°æ®åº“..."):
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            init_database()
        except Exception as e:
            st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            st.info("è¯·æ£€æŸ¥ï¼š\n1. æ•°æ®åº“æœåŠ¡æ˜¯å¦å¯åŠ¨\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. æ•°æ®åº“é…ç½®æ˜¯å¦æ­£ç¡®")
            return
    
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œä¸ä¼šæ¯æ¬¡éƒ½ä¸‹è½½ï¼‰
    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œå¯èƒ½éœ€è¦ä¸‹è½½ï¼‰..."):
        try:
            embeddings_model = load_embeddings_model()
            st.success("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        except Exception as e:
            st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            st.info("å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œæ‚¨å¯ä»¥ï¼š\n1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n2. ç­‰å¾…ä¸€ä¼šå„¿å†è¯•\n3. æˆ–è€…æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ~/.cache/torch/sentence_transformers")
            return
    
    # ä¾§è¾¹æ ï¼šåŠŸèƒ½é€‰æ‹©
    with st.sidebar:
        st.header("åŠŸèƒ½é€‰æ‹©")
        mode = st.radio(
            "é€‰æ‹©åŠŸèƒ½",
            ["æ–‡æ¡£ç®¡ç†", "å‘é‡æ£€ç´¢", "ç»“æ„åŒ–æ£€ç´¢"]
        )
    
    if mode == "æ–‡æ¡£ç®¡ç†":
        st.header("æ–‡æ¡£ç®¡ç†")
        
        # æ‰¹é‡æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ ç—…å†æ–‡æ¡£ï¼ˆå¯å¤šé€‰ï¼‰", 
            type=["txt", "docx", "pdf"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            col1, col2 = st.columns([2, 1])
            with col1:
                st.write(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
            with col2:
                if st.button("ä¿å­˜æ‰€æœ‰æ–‡æ¡£"):
                    for uploaded_file in uploaded_files:
                        with st.spinner(f"æ­£åœ¨ä¿å­˜ {uploaded_file.name}..."):
                            file_path = save_uploaded_file(uploaded_file)
                            st.success(f"å·²ä¿å­˜: {uploaded_file.name}")
        
        # æ˜¾ç¤ºå·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        st.subheader("å·²ä¸Šä¼ çš„æ–‡ä»¶")
        files = get_uploaded_files()
        if not files:
            st.info("æš‚æ— ä¸Šä¼ çš„æ–‡ä»¶")
        else:
            # æ·»åŠ æ‰¹é‡æ“ä½œæŒ‰é’®
            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button("æ‰¹é‡å‘é‡åŒ–"):
                    for file in files:
                        with st.spinner(f"æ­£åœ¨å‘é‡åŒ– {file.name}..."):
                            if vectorize_document(file):
                                st.success(f"{file.name} å‘é‡åŒ–å®Œæˆ")
                            else:
                                st.error(f"{file.name} å‘é‡åŒ–å¤±è´¥")
            with col2:
                if st.button("æ‰¹é‡ç»“æ„åŒ–"):
                    for file in files:
                        with st.spinner(f"æ­£åœ¨ç»“æ„åŒ– {file.name}..."):
                            result = parse_document_to_json(file)
                            if result:
                                st.success(f"{file.name} ç»“æ„åŒ–å®Œæˆ")
                                with st.expander("æŸ¥çœ‹ç»“æ„åŒ–æ•°æ®"):
                                    st.json(result)
                            else:
                                st.error(f"{file.name} ç»“æ„åŒ–å¤±è´¥")
            
            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            for file in files:
                col1, col2, col3 = st.columns([3, 1, 1])
                with col1:
                    st.write(file.name)
                with col2:
                    if st.button("å‘é‡åŒ–", key=f"vec_{file.name}"):
                        with st.spinner("æ­£åœ¨å‘é‡åŒ–..."):
                            if vectorize_document(file):
                                st.success("å‘é‡åŒ–å®Œæˆ")
                            else:
                                st.error("å‘é‡åŒ–å¤±è´¥")
                with col3:
                    if st.button("ç»“æ„åŒ–", key=f"struct_{file.name}"):
                        with st.spinner("æ­£åœ¨ç»“æ„åŒ–..."):
                            result = parse_document_to_json(file)
                            if result:
                                st.success("ç»“æ„åŒ–å®Œæˆ")
                                with st.expander("æŸ¥çœ‹ç»“æ„åŒ–æ•°æ®"):
                                    st.json(result)
                            else:
                                st.error("ç»“æ„åŒ–å¤±è´¥")
    
    elif mode == "å‘é‡æ£€ç´¢":
        st.header("å‘é‡æ£€ç´¢")
        
        # æ˜¾ç¤ºå·²å‘é‡åŒ–çš„æ–‡æ¡£åˆ—è¡¨
        with st.expander("æŸ¥çœ‹å·²å‘é‡åŒ–çš„æ–‡æ¡£", expanded=True):
            with OracleVectorStore() as vector_store:
                documents = vector_store.list_documents()
                if documents:
                    st.write("å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼š")
                    for doc in documents:
                        st.write(f"- {doc['file_path']} (å‘é‡æ•°: {doc['chunk_count']})")
                else:
                    st.info("æš‚æ— å‘é‡åŒ–æ–‡æ¡£")
        
        # æœç´¢åŠŸèƒ½
        query = st.text_input("è¯·è¾“å…¥æœç´¢å†…å®¹")
        if query:
            with st.spinner("æ­£åœ¨æœç´¢å¹¶åˆ†æ..."):
                results = search_similar_documents(query, top_k=3)
                
                if results:
                    # æ˜¾ç¤ºæœ€ç›¸å…³æ–‡æ¡£çš„ GPT åˆ†æç»“æœ
                    st.subheader("GPT åˆ†æç»“æœ")
                    st.write(results[0]['gpt_analysis'])
                    
                    # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æ–‡æ¡£
                    st.subheader("ç›¸å…³æ–‡æ¡£")
                    for i, result in enumerate(results, 1):
                        similarity = 1 - result['similarity']
                        with st.expander(f"æ–‡æ¡£ {i}: {result['file_path']} (ç›¸ä¼¼åº¦: {similarity:.2%})"):
                            st.write(result['content'])
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
    
    else:  # ç»“æ„åŒ–æ£€ç´¢
        st.header("ç»“æ„åŒ–æ£€ç´¢")
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„ç»“æ„åŒ–æ–‡æ¡£
        with OracleJsonStore() as json_store:
            try:
                # é¦–å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                check_table_sql = """
                SELECT COUNT(*) as count
                FROM user_tables 
                WHERE table_name = 'DOCUMENT_JSON'
                """
                result = json_store.execute_search(check_table_sql)
                table_exists = result[0]['count'] > 0 if result else False
                
                if not table_exists:
                    st.warning("æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåœ¨æ–‡æ¡£ç®¡ç†ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
                    return
                
                # è·å–æ‰€æœ‰æ–‡æ¡£
                check_sql = """
                SELECT doc_info, doc_json 
                FROM DOCUMENT_JSON 
                ORDER BY id DESC
                """
                all_docs = json_store.execute_search(check_sql)
                
                # æ˜¾ç¤ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
                st.subheader("ğŸ“š æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£")
                if all_docs:
                    st.write(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£")
                    for doc in all_docs:
                        st.markdown(f"### ğŸ“„ {Path(doc['doc_info']).name}")
                        if isinstance(doc['doc_json'], dict):
                            data = doc['doc_json']
                            
                            # ä½¿ç”¨tabsæ¥ç»„ç»‡å†…å®¹
                            tabs = st.tabs([
                                "åŸºæœ¬ä¿¡æ¯", "ä¸»è¯‰ä¸è¯Šæ–­", "ç°ç—…å²", 
                                "ç”Ÿå‘½ä½“å¾", "ç”ŸåŒ–æŒ‡æ ‡", "è¯Šç–—ç»è¿‡"
                            ])
                            
                            with tabs[0]:
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.markdown("**æ‚£è€…ä¿¡æ¯**")
                                    info = {
                                        "å§“å": data.get("æ‚£è€…å§“å", "æœªçŸ¥"),
                                        "æ€§åˆ«": data.get("æ€§åˆ«", "æœªçŸ¥"),
                                        "å¹´é¾„": data.get("å¹´é¾„", "æœªçŸ¥"),
                                        "æ°‘æ—": data.get("æ°‘æ—", "æœªçŸ¥"),
                                        "èŒä¸š": data.get("èŒä¸š", "æœªçŸ¥"),
                                        "å©šå§»çŠ¶å†µ": data.get("å©šå§»çŠ¶å†µ", "æœªçŸ¥")
                                    }
                                    st.json(info)
                                with col2:
                                    st.markdown("**ä½é™¢ä¿¡æ¯**")
                                    info = {
                                        "å…¥é™¢æ—¥æœŸ": data.get("å…¥é™¢æ—¥æœŸ", "æœªçŸ¥"),
                                        "å‡ºé™¢æ—¥æœŸ": data.get("å‡ºé™¢æ—¥æœŸ", "æœªçŸ¥"),
                                        "ä½é™¢å¤©æ•°": data.get("ä½é™¢å¤©æ•°", "æœªçŸ¥"),
                                        "å‡ºé™¢æƒ…å†µ": data.get("å‡ºé™¢æƒ…å†µ", "æœªçŸ¥")
                                    }
                                    st.json(info)
                            
                            with tabs[1]:
                                st.markdown("**ä¸»è¯‰**")
                                st.write(data.get("ä¸»è¯‰", "æœªè®°å½•"))
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.markdown("**å…¥é™¢è¯Šæ–­**")
                                    for diag in data.get("å…¥é™¢è¯Šæ–­", []):
                                        st.write(f"- {diag}")
                                with col2:
                                    st.markdown("**å‡ºé™¢è¯Šæ–­**")
                                    for diag in data.get("å‡ºé™¢è¯Šæ–­", []):
                                        st.write(f"- {diag}")
                            
                            with tabs[2]:
                                st.markdown("**ç°ç—…å²**")
                                for item in data.get("ç°ç—…å²", []):
                                    st.write(f"- {item}")
                            
                            with tabs[3]:
                                if "ç”Ÿå‘½ä½“å¾" in data:
                                    st.json(data["ç”Ÿå‘½ä½“å¾"])
                                else:
                                    st.info("æœªè®°å½•ç”Ÿå‘½ä½“å¾")
                            
                            with tabs[4]:
                                if "ç”ŸåŒ–æŒ‡æ ‡" in data:
                                    st.json(data["ç”ŸåŒ–æŒ‡æ ‡"])
                                else:
                                    st.info("æœªè®°å½•ç”ŸåŒ–æŒ‡æ ‡")
                            
                            with tabs[5]:
                                st.markdown("**è¯Šç–—ç»è¿‡**")
                                st.write(data.get("è¯Šç–—ç»è¿‡", "æœªè®°å½•"))
                                if "å‡ºé™¢åŒ»å˜±" in data:
                                    st.markdown("**å‡ºé™¢åŒ»å˜±**")
                                    for advice in data["å‡ºé™¢åŒ»å˜±"]:
                                        st.write(f"- {advice}")
                        
                        st.markdown("---")
                else:
                    st.info("ğŸ“­ æ•°æ®åº“ä¸­æš‚æ— ç»“æ„åŒ–æ–‡æ¡£ï¼Œè¯·å…ˆåœ¨æ–‡æ¡£ç®¡ç†ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
                
                # æœç´¢åŠŸèƒ½
                st.divider()
                st.subheader("ğŸ” ç»“æ„åŒ–æœç´¢")
                query = st.text_input("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼ˆä¾‹å¦‚ï¼š'æŸ¥æ‰¾ä¸»è¯‰åŒ…å«å‘çƒ­çš„æ‚£è€…' æˆ– 'æŸ¥æ‰¾å¹´é¾„å¤§äº60å²çš„é«˜è¡€å‹æ‚£è€…'ï¼‰")
                
                if query:
                    with st.spinner("æ­£åœ¨åˆ†ææŸ¥è¯¢å¹¶æœç´¢..."):
                        results = search_json_documents(query, top_k=5)
                        
                        if results:
                            st.subheader("ğŸ¯ æœç´¢ç»“æœ")
                            for i, result in enumerate(results, 1):
                                with st.expander(f"åŒ¹é…æ–‡æ¡£ {i}: {Path(result['doc_info']).name}"):
                                    if result.get('highlights'):
                                        st.markdown("**ğŸ” åŒ¹é…ä¿¡æ¯**")
                                        for highlight in result['highlights']:
                                            st.markdown(f"*{highlight['field']}*:")
                                            for match in highlight['matches']:
                                                st.write(f"- {match}")
                                    
                                    if isinstance(result['doc_json'], dict):
                                        data = result['doc_json']
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.markdown("**ğŸ“‹ åŸºæœ¬ä¿¡æ¯**")
                                            info = {
                                                "æ‚£è€…": f"{data.get('æ‚£è€…å§“å', 'æœªçŸ¥')} ({data.get('æ€§åˆ«', 'æœªçŸ¥')}/{data.get('å¹´é¾„', 'æœªçŸ¥')}å²)",
                                                "å…¥é™¢æ—¥æœŸ": data.get("å…¥é™¢æ—¥æœŸ", "æœªçŸ¥"),
                                                "ä½é™¢å¤©æ•°": data.get("ä½é™¢å¤©æ•°", "æœªçŸ¥")
                                            }
                                            st.json(info)
                                        
                                        with col2:
                                            st.markdown("**ğŸ¥ ä¸»è¦è¯Šæ–­**")
                                            if "å…¥é™¢è¯Šæ–­" in data:
                                                for diag in data["å…¥é™¢è¯Šæ–­"][:3]:
                                                    st.write(f"- {diag}")
                                
                                st.markdown("---")
                        else:
                            st.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            except Exception as e:
                logger.error(f"æ£€ç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.error(f"æ£€ç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()

