"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - Oracle Vector Storeç‰ˆæœ¬
"""

import streamlit as st
from utils.oracle_vector_store import OracleVectorStore
from utils.oracle_json_store import OracleJsonStore
from utils.oracle_graph_store import OracleGraphStore
from utils.oracle_property_graph import OraclePropertyGraph
from utils.medical_record_parser import MedicalRecordParser
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import shutil
from pathlib import Path
import logging
from openai import OpenAI
import pdfplumber
import json
import datetime
import hashlib
from typing import Dict, Any, List
from decimal import Decimal
from utils.medical_graph_parser import MedicalGraphParser
from pyvis.network import Network
import streamlit.components.v1 as components
import tempfile
import networkx as nx
from collections import defaultdict
import pandas as pd
import plotly.express as px

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è·å–OpenAIé…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

# è®¾ç½®æ–‡æ¡£å­˜å½•
UPLOAD_DIR = Path("uploaded_documents")
UPLOAD_DIR.mkdir(exist_ok=True)

# è®¾ç½®JSONç¼“å­˜ç›®å½•
CACHE_DIR = Path("json_cache")
CACHE_DIR.mkdir(exist_ok=True)

# ä½¿ç”¨ st.cache_resource ç¼“å­˜æ¨¡å‹ï¼Œå¹¶éšè—
@st.cache_resource(show_spinner=False)
def load_embeddings_model():
    """åŠ è½½å‘é‡åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰"""
    logger.debug("åŠ è½½å‘é‡åŒ–æ¨¡å‹")
    return SentenceTransformer('all-MiniLM-L6-v2')

# åˆå§‹åŒ–æ¨¡å‹
embeddings_model = load_embeddings_model()

def get_cache_path(file_path: str) -> Path:
    """æ ¹æ®æ–‡ä»¶è·¯å¾„ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # è·å–åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    original_name = Path(file_path).stem
    return CACHE_DIR / f"{original_name}.json"

def save_to_cache(file_path: str, data: Dict) -> None:
    """ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°ç¼“å­˜"""
    cache_path = get_cache_path(file_path)
    with open(cache_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"ç¼“å­˜ä¿å­˜åˆ°: {cache_path}")

def load_from_cache(file_path: str) -> Dict:
    """ä»ç¼“å­˜åŠ è½½ç»“æ„åŒ–æ•°æ®"""
    cache_path = get_cache_path(file_path)
    if cache_path.exists():
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"ä»ç¼“å­˜åŠ è½½: {cache_path}")
        return data
    return None

def save_uploaded_file(uploaded_file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•"""
    file_path = UPLOAD_DIR / uploaded_file.name
    with open(file_path, "wb") as f:
        shutil.copyfileobj(uploaded_file, f)
    return str(file_path)

def read_file_content(file_path):
    """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨ pdfplumber å¤„ç† PDF"""
    try:
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_extension = str(file_path).lower().split('.')[-1]
        
        # PDFæ–‡ä»¶å¤„ç†
        if file_extension == 'pdf':
            with pdfplumber.open(file_path) as pdf:
                text = ''
                for page in pdf.pages:
                    text += page.extract_text() + '\n'
                return text.strip()
        
        # æ–‡æœ¬æ–‡ä»¶å¤„ç†
        else:
            encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read().strip()
                except UnicodeDecodeError:
                    continue
        
        return None
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        return None

def get_uploaded_files():
    """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
    return list(UPLOAD_DIR.glob("*.*"))

def vectorize_document(file_path):
    """å‘é‡åŒ–æ–‡æ¡£"""
    try:
        content = read_file_content(file_path)
        # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹è¿›è¡Œå‘é‡åŒ–
        vector = embeddings_model.encode([content])[0]
        documents = [{"file_path": str(file_path), "content": content}]
        
        with OracleVectorStore() as vector_store:
            vector_store.add_vectors([vector], documents)
        return True
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤±è´¥: {str(e)}")
        return False

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    try:
        with OracleJsonStore() as json_store:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_table_sql = """
            SELECT COUNT(*) as count
            FROM user_tables 
            WHERE table_name = 'DOCUMENT_JSON'
            """
            result = json_store.execute_search(check_table_sql)
            table_exists = result[0]['count'] > 0 if result else False
            
            if not table_exists:
                # åˆ›å»ºç»“æ„åŒ–æ–‡æ¡£è¡¨
                create_json_table_sql = """
                CREATE TABLE DOCUMENT_JSON (
                    id NUMBER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                    doc_info VARCHAR2(500),
                    doc_json JSON
                )
                """
                json_store.execute_sql(create_json_table_sql)
                logger.info("æˆåŠŸåˆ›å»ºDOCUMENT_JSONè¡¨")
            else:
                logger.debug("DOCUMENT_JSONè¡¨å·²å­˜åœ¨")  # æ”¹ç”¨debugçº§åˆ«çš„æ—¥å¿—

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")
        st.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")

class MedicalRecordParser:
    def __init__(self):
        self.client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )

    def parse_medical_record(self, text: str, doc_info: str) -> Dict[str, Any]:
        """è§£æåŒ»ç–—è®°å½•æ–‡æœ¬ï¼Œç”Ÿæˆç»“æ„åŒ–JSONæ•°æ®å’ŒSQLæ’å…¥è¯­å¥"""
        try:
            prompt = f"""è¯·å°†ä»¥ä¸‹ç—…å†å†…å®¹ç»“æ„åŒ–ä¸ºJSONæ ¼å¼ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„SQLæ’å…¥è¯­å¥ã€‚

ç—…å†å†…å®¹ï¼š
{text}

è¦æ±‚ï¼š
1. JSONç»“æ„ï¼š
{{
    "æ‚£è€…å§“å": "æŸæŸï¼ˆä¿æŠ¤éšç§ï¼‰",
    "æ€§åˆ«": "æ€§åˆ«",
    "å¹´é¾„": "æ•°å­—",
    "å…¥é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "å‡ºé™¢æ—¥æœŸ": "YYYY-MM-DD",
    "ä¸»è¯‰": "ä¸»è¦ç—‡çŠ¶",
    "ç°ç—…å²": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "å…¥è¯Š": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‡ºé™¢è¯Š": ["æ–­1", "è¯Šæ–­2"],
    "ç”Ÿå‘½ä½“å¾": {{
        "ä½“æ¸©": "åŒ…å«å•ä½",
        "è¡€å‹": "åŒ…å«å•ä½"
    }},
    "ç”ŸåŒ–æŒ‡æ ‡": {{
        "æŒ‡æ ‡å": "å€¼å’Œå•ä½ï¼Œå¼‚å¸¸æ ‡â†‘â†“"
    }},
    "è¯Šç–—ç»è¿‡": "æ²»ç–—è¿‡ç¨‹æè¿°",
    "å‡ºé™¢åŒ»å˜±": ["åŒ»å˜±1", "åŒ»å˜±2"]
}}

2. SQLè¦æ±‚ï¼š
- è¡¨åï¼šDOCUMENT_JSON
- å­—æ®µï¼š(id NUMBERè‡ªå¢, doc_info VARCHAR2(500), doc_json JSON)
- doc_infoå€¼ï¼š{doc_info}
- ä½¿ç”¨JSON_OBJECTæˆ–FORMAT JSONè¯­æ³•

è¯·è¿”å›ï¼š
{{
    "structured_data": JSONæ ¼å¼çš„ç—…å†æ•°æ®,
    "sql_statement": Oracleæ’å…¥è¯­å¥
}}"""

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ ï¿½ï¿½ï¿½ï¿½ï¿½ç–—æ•°æ®ç»“æ„åŒ–ä¸“å®¶ï¼Œæ“…é•¿è§£æç—…å†æ–‡æœ¬å¹¶ç”Ÿæˆè§„èŒƒçš„JSONå’ŒSQL"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )

            # è§£æè¿”å›çš„JSON
            result = json.loads(response.choices[0].message.content)
            
            # éªŒè¯ç»“æœæ ¼å¼
            if not isinstance(result, dict) or 'structured_data' not in result or 'sql_statement' not in result:
                raise ValueError("GPTè¿”å›æ•°æ®æ ¼å¼ä¸æ­£ç¡®")

            # æ·»åŠ å…ƒæ•°æ®
            if 'metadata' not in result['structured_data']:
                result['structured_data']['metadata'] = {}
            result['structured_data']['metadata'].update({
                'import_time': datetime.datetime.now().isoformat(),
                'source_type': 'text',
                'last_updated': datetime.datetime.now().isoformat()
            })

            return result

        except Exception as e:
            logger.error(f"è§£æåŒ»ç–—è®°å½•å¤±è´¥: {str(e)}")
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}

def parse_document_to_json(file):
    """è§£ææ–‡æ¡£ä¸ºJSONæ ¼å¼"""
    try:
        # æ£€æŸ¥ json_cache ä¸­æ˜¯å¦å­˜åœ¨å¯¹åº”çš„ JSON æ–‡ä»¶
        cache_path = os.path.join("json_cache", f"{Path(file.name).stem}.json")
        if os.path.exists(cache_path):
            logger.info(f"æ‰¾åˆ°ç¼“å­˜çš„JSONæ–‡ä»¶: {cache_path}")
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                    
                # æ„é€ æ•°æ®åº“éœ€è¦çš„æ ¼å¼
                json_result = {
                    "structured_data": cached_data,
                    "sql_statement": None  # ä¸éœ€è¦ SQL è¯­å¥ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ add_document
                }
                    
                # ä¿å­˜JSONç»“æœåˆ°æ•°æ®åº“
                with OracleJsonStore() as json_store:
                    json_store.add_document(file.name, json_result['structured_data'])
                    
                logger.info(f"æˆåŠŸä»ç¼“å­˜å¯¼å…¥æ–‡æ¡£: {file.name}")
                return True
            except Exception as e:
                logger.error(f"è¯»å–ç¼“å­˜JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
                # å¦‚æœè¯»å–ç¼“å­˜å¤±è´¥ï¼Œç»§ç»­å°è¯•è§£ææ–‡æ¡£
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–è¯»å–ç¼“å­˜å¤±è´¥ï¼Œåˆ™è§£ææ–‡æ¡£
        content = read_file_content(file)
        
        # è§£æä¸ºJSON
        parser = MedicalRecordParser()
        json_result = parser.parse_medical_record(content, file.name)
        
        # æ£€æŸ¥è§£æç»“æœ
        if "error" in json_result:
            logger.error(f"è§£ææ–‡æ¡£å¤±è´¥: {json_result['error']}")
            return False
            
        # è§£æä¸ºå›¾æ•°æ®
        graph_parser = MedicalGraphParser()
        graph_result = graph_parser.parse_to_graph(content, file.name)
        
        if "error" in graph_result:
            logger.error(f"è§£æå›¾æ•°æ®å¤±è´¥: {graph_result['error']}")
            return False
            
        # ä¿å­˜JSONç»“æœåˆ°æ•°æ®åº“
        with OracleJsonStore() as json_store:
            json_store.add_document(file.name, json_result['structured_data'])
            
        return True
    except Exception as e:
        logger.error(f"è§£ææ–‡æ¡£å¤±è´¥: {str(e)}")
        return False

def search_similar_documents(query: str, top_k: int = 3):
    """å‘é‡æœç´¢å¹¶ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£"""
    try:
        # 1. å‘é‡æœç´¢
        vector = embeddings_model.encode([query])[0]
        with OracleVectorStore() as vector_store:
            results = vector_store.search_vectors([vector], top_k=top_k)
        
        if not results:
            return []

        # 2. ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£
        best_match = results[0]  # å–ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£
        
        prompt = f"""
        åŸºäºä»¥ä¸‹åŒ»ç–—æ–‡æ¡£å†…å®¹ï¼Œå›ç­”é—®é¢˜ï¼š{query}

        æ–‡æ¡£å†…å®¹ï¼š
        {best_match['content']}

        è¯·æä¾›è¯¦ç»†ä¸“ä¸šåˆ†æå’Œç­”æ¡ˆã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚
        """

        # ä½¿ç”¨æ–° OpenAI API
        client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æåŒ»ç–—æ–‡æ¡£å¹¶ä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )

        # 3. å°† GPT åˆ†æç»“æœæ·»åŠ åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ä¸­
        best_match['gpt_analysis'] = response.choices[0].message.content
        
        # è¿”å›æ‰€æœ‰æ£€ç´¢ç»“æœï¼Œä½†åªæœ‰ç›¸å…³çš„æ–‡åŒ…å« GPT åˆ†æ
        return results

    except Exception as e:
        logger.error(f"æœç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []

def analyze_query_with_gpt(query_text):
    """ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾å¹¶ç”ŸæˆOracle 23c JSONæŸ¥è¯¢æ¡ä»¶"""
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
        
        # æä¾›å®Œæ•´çš„JSONç»“æ„ä¿¡æ¯
        json_structure = """
{
    "æ‚£è€…å§“å": "å§“å",
    "æ€§åˆ«": "ç”·/å¥³",
    "å¹´é¾„": "æ•°å­—",
    "å…¥é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "ä¸»è¯‰": "ä¸»è¦çŠ¶",
    "ç°ç—…å²": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "å…¥é™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‡ºé™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‘½ä½“å¾": {
        "ä½“æ¸©": "åŒ…å«å•ä½",
        "è¡€å‹": "åŒ…å«å•ä½"
    },
    "ç”ŸåŒ–æŒ‡æ ‡": {
        "å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶": "å€¼å’Œå•ä½",
        "ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶": "å€¼å’Œå•ä½",
        "ç™½ç»†èƒ": "å€¼å’Œå•ä½",
        "æ·‹å·´ç»†ï¿½ï¿½åˆ†æ¯”": "å€¼å•ä½",
        "ä¸­æ€§ç²’ç»†èƒç™¾åˆ†æ¯”": "å€¼å’Œå•ä½",
        "è¡€çº¢è›‹ç™½": "å€¼å’Œå•ä½",
        "è¡€å°æ¿": "å€¼å’Œå•ä½"
    },
    "è¯Šç–—ç»è¿‡": "æ²»ç–—è¿‡ç¨‹æè¿°",
    "å‡ºé™¢åŒ»å˜±": ["åŒ»å˜±1", "åŒ»2"]
}"""
        
        messages = [
            {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä¸ªOracle 23c JSONæŸ¥è¯¢ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹çš„JSONæ–‡æ¡£ç»“æ„ï¼Œå¸®åŠ©ç”Ÿæˆå‡†ç¡®çš„Oracle JSONæŸ¥è¯¢æ¡ä»¶ã€‚

æ–‡æ¡£ç»“æ„ï¼š
{json_structure}

æ•°æ®åº“ä¿¡æ¯ï¼š
- ä½¿ç”¨Oracle Database 23c
- è¡¨åï¼šDOCUMENT_JSON
- å­—æ®µï¼šdoc_info (VARCHAR2), doc_json (JSON)
- JSONæ•°æ®å­˜å‚¨åœ¨doc_jsonå­—æ®µä¸­

Oracle 23c JSONæŸ¥è¯¢ç‰¹ï¼š
1. ä½¿ç”¨ JSON_EXISTS è¿›è¡Œæ¡ä»¶åŒ¹é…
2. ä½¿ç”¨ JSON_VALUE æå–å•ä¸ªå€¼
3. ä½¿ç”¨ JSON_QUERY æå–JSONæ•°ç»„æˆ–å¯¹è±¡
4. æ”¯æŒç‚¹å·è®¿é—®åµŒå¥—å±æ€§
5. æ”¯æŒç»„ç´¢å¼•è®¿é—®
6. æ”¯æŒæ¡ä»¶è¿‡æ»¤ï¼š?(@ == "value")
7. æ”¯æŒé”®ååŒ¹é…ï¼š
   - ä½¿ç”¨ $.ç”ŸåŒ–æŒ‡æ ‡.* éå†æ‰€æœ‰æŒ‡æ ‡
   - ä½¿ç”¨ EXISTS å’Œ OR ç»„åˆå¤šä¸ªæ¡ä»¶

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
2. æ ¹æ®æ¡£ç»“æ„å’ŒOracle 23cç‰¹æ€§ï¼Œç”Ÿæˆæœ€ä¼˜çš„æŸ¥è¯¢æ¡ä»¶
3. å¯¹äºåŒ»å­¦æœ¯è¯­ï¼Œè€ƒè™‘åŒä¹‰è¯å’Œç®€å†™ï¼ˆå¦‚"è½¬æ°¨é…¶"å¯èƒ½æŒ‡"å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"æˆ–"ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"ï¼‰
4. è¿”å›æ ¼å¼ä¸ºJSONï¼š
{{
    "query_type": "æŸ¥è¯¢ç±»å‹",
    "conditions": ["Oracle JSONæŸ¥è¯¢æ¡ä»¶"],
    "fields": ["éœ€è¦è¿”å›çš„æ®µ"],
    "keywords": ["é”®"]
}}

ç¤ºä¾‹1ï¼š
è¾“å…¥ï¼š"é©¬æŸæŸçš„è½¬æ°¨é…¶æŒ‡æ ‡"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@ == \"é©¬æŸæŸ\")')",
        "(JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶') OR JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶'))"
    ],
    "fields": ["ç”ŸåŒ–æ ‡.å¤©æ°¨é…¸æ°¨åŸºè½¬é…¶", "ç”ŸåŒ–æŒ‡æ ‡.ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"],
    "keywords": ["é©¬æŸæŸ", "è½¬æ°¨é…¶"]
}}

ç¤ºä¾‹2ï¼š
è¾“å…¥ï¼š"æŸ¥çœ‹é©¬æŸæŸçš„æ·‹å·´ç»†èƒæ¯”ä¾‹"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£å§“å?(@ == \"é©¬æŸæŸ\")')",
        "JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.æ·‹å·´ç»†èƒç™¾åˆ†æ¯”')"
    ],
    "fields": ["ç”ŸåŒ–æŒ‡æ ‡.æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"],
    "keywords": ["é©¬æŸæŸ", "æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"]
}}

ç¤ºä¾‹3ï¼š
è¾“å…¥ï¼š"é©¬æŸæŸçš„è¡€æ¶²ç›¸å…³æŒ‡æ ‡"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@ == \"é©¬æŸæŸ\")')",
        "(JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.è¡€çº¢è›‹ç™½') OR JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.è¡€å°æ¿'))"
    ],
    "fields": [
        "ç”ŸæŒ‡æ ‡.ç™½ç»†èƒ",
        "ç”ŸåŒ–æŒ‡æ ‡.è¡€çº¢è›‹ç™½",
        "ç”ŸåŒ–æŒ‡æ ‡.è¡€å°æ¿"
    ],
    "keywords": ["é©¬æŸæŸ"]
}}"""},
            {"role": "user", "content": query_text}
        ]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPTåˆ†æè¯¢å¤±è´¥: {str(e)}")
        return None

# é…ï¿½ï¿½ï¿½ï¿½ï¿½å¸¸é‡
TOP_K = 5  # æœç´¢ç»“æœè¿”å›çš„æœ€å¤§æ•°é‡

def normalize_medical_term(query_text):
    """ä½¿ç”¨ GPT å°†ç”¨ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½è¯¢çš„æ ‡åç§°æ ‡å‡†åŒ–"""
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
        
        messages = [
            {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æŒ‡æ ‡åç§°æ ‡å‡†åŒ–ä¸“å®¶ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
è¯·ï¿½ï¿½ï¿½ï¿½ç”¨ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ä¸­çš„æŒ‡æ ‡åç§°ä¸ºæ ‡çš„ç–—æ ‡ç§°

è§„ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
1. æŸ¥è¯¢ä¸­åŒ…å«æŸä¸ªæ£€éªŒæŒ‡æ ‡çš„åŒä¹‰è¯æˆ–è¿‘ä¹‰è¯ï¼Œè¿”å›æ ‡å‡†åç§°
2. å¦‚æœä¸ç¡®å®šï¼Œè¿”å›åŸå§‹è¯è¯­
3. è¿”å›æ ¼å¼ä¸º JSONï¼š{"standard_term": "æ ‡å‡†åç§°"}

ç¤ºï¼š
è¾“å…¥ï¼š"æ·‹å·´ç»†èƒæ¯”ä¾‹"
è¾“å‡ºï¼š{"standard_term": "æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"}

è¾“å…¥ï¼š"ç™½ç»†èƒè®¡æ•°"
è¾“å‡ºï¼š{"standard_term": "ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½èƒ"}

è¾“å…¥ï¼š"è¡€çº¢è›‹ç™½å«é‡"
è¾“å‡ºï¼š{"standard_term": "è¡€çº¢è›‹ç™½"}"""},
            {"role": "user", "content": query_text}
        ]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        return result.get('standard_term', query_text)
        
    except Exception as e:
        logger.error(f"æŒ‡æ ‡åç§°æ ‡å‡†åŒ–: {str(e)}")
        return query_text

def search_documents(query: str, top_k: int = 3) -> List[Dict[str, Any]]:
    """æœç´¢ç»“æ„åŒ–æ–‡æ¡£"""
    try:
        with OracleJsonStore() as json_store:
            results = json_store.search_documents(query, top_k)
            return results
    except Exception as e:
        logger.error(f"æœç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
        return []

class DecimalEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Decimal):
            return str(obj)
        return super(DecimalEncoder, self).default(obj)

def generate_answer(query: str, doc_json: Dict[str, Any]) -> str:
    """ç”ŸæˆåŸºäºæ–‡æ¡£å†…å®¹çš„å›ç­”"""
    try:
        # å°†æ–‡æ¡£å†…å®¹è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ JSON ç¼–ç å™¨å¤„ç† Decimal ç±»å‹
        doc_str = json.dumps(doc_json, ensure_ascii=False, indent=2, cls=DecimalEncoder)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹åŒ»ç–—è®°å½•å›ç­”é—®é¢˜ã€‚å¦‚æœè®°å½•ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”"æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

é—®é¢˜ï¼š{query}

åŒ»ç–—è®°å½•ï¼š
{doc_str}
"""
        
        # è°ƒç”¨ OpenAI API ç”Ÿæˆç­”æ¡ˆ
        client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )
        
        response = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=[
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—åŠ©æ‰‹ï¼Œè´Ÿè´£è§£è¯»åŒ»ç–—è®°å½•å¹¶å›ç­”é—®é¢˜ã€‚è¯·åŸºäºç»™å®šçš„åŒ»ç–—è®°å½•ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œå¦‚æœè®°å½•ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”\"æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯\"ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0,
            max_tokens=500
        )
        
        answer = response.choices[0].message.content.strip()
        return answer if answer else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}")
        return "å¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯"

def display_search_results(query: str, results: List[Dict[str, Any]]):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    try:
        if not results:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return
            
        # ç”Ÿæˆå›ç­”
        doc = results[0]  # åªå¤„ç†ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡æ¡£
        answer = generate_answer(doc['query'], doc['doc_json'])
        if answer:
            st.success(answer)
        
        # æ˜¾ç¤ºæ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        with st.expander("ğŸ“‹ æŸ¥çœ‹åŸå§‹ç—…å†", expanded=False):
            data = doc['doc_json']
            # åˆ›å»ºæ ‡ç­¾é¡µ
            tabs = st.tabs([
                "åŸºæœ¬ä¿¡æ¯", "ä¸»è¯‰ä¸è¯Šæ–­", "ç°ç—…å²", 
                "ç”Ÿå‘½ä½“å¾", "ç”ŸåŒ–æŒ‡æ ‡", "è¯Šç–—ç»è¿‡"
            ])
            
            with tabs[0]:
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**æ‚£è€…ä¿¡æ¯**")
                    info = {
                        "å§“å": data.get("æ‚£è€…å§“å", "æœªçŸ¥"),
                        "æ€§åˆ«": data.get("æ€§åˆ«", "æœªçŸ¥"),
                        "å¹´é¾„": data.get("å¹´é¾„", "æœªçŸ¥"),
                        "æ°‘æ—": data.get("æ°‘æ—", "æœªçŸ¥"),
                        "èŒä¸š": data.get("èŒä¸š", "æœªçŸ¥"),
                        "å©šå§»çŠ¶å†µ": data.get("å©šå§»çŠ¶å†µ", "æœªçŸ¥")
                    }
                    st.json(info)
                with col2:
                    st.markdown("**ä½é™¢ä¿¡æ¯**")
                    info = {
                        "å…¥é™¢æ—¥æœŸ": data.get("å…¥é™¢æ—¥æœŸ", "æœªçŸ¥"),
                        "å‡ºé™¢æ—¥æœŸ": data.get("å‡ºé™¢æ—¥æœŸ", "æœªçŸ¥"),
                        "ä½é™¢å¤©æ•°": data.get("ä½é™¢å¤©æ•°", "æœªçŸ¥"),
                        "å‡ºé™¢æƒ…å†µ": data.get("å‡ºé™¢æƒ…å†µ", "æœªçŸ¥")
                    }
                    st.json(info)
            
            with tabs[1]:
                st.markdown("**ä¸»è¯‰**")
                st.write(data.get("ä¸»è¯‰", "æœªè®°å½•"))
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**å…¥é™¢è¯Šæ–­**")
                    for diag in data.get("å…¥é™¢è¯Šæ–­", []):
                        st.write(f"- {diag}")
                with col2:
                    st.markdown("**å‡ºé™¢è¯Šæ–­**")
                    for diag in data.get("å‡ºé™¢è¯Šæ–­", []):
                        st.write(f"- {diag}")
            
            with tabs[2]:
                st.markdown("**ç°ç—…å²**")
                for item in data.get("ç°ç—…å²", []):
                    st.write(f"- {item}")
            
            with tabs[3]:
                if "ç”Ÿå‘½ä½“å¾" in data:
                    st.json(data["ç”Ÿå‘½ä½“å¾"])
                else:
                    st.info("æœªè®°å½•ç”Ÿå‘½ä½“å¾")
            
            with tabs[4]:
                if "ç”ŸåŒ–æŒ‡æ ‡" in data:
                    st.json(data["ç”ŸåŒ–æŒ‡æ ‡"])
                else:
                    st.info("æœªè®°å½•ç”ŸåŒ–æŒ‡æ ‡")
            
            with tabs[5]:
                st.markdown("**è¯Šç–—ç»è¿‡**")
                st.write(data.get("è¯Šç–—ç»è¿‡", "æœªè®°å½•"))
                if "å‡ºé™¢åŒ»å˜±" in data:
                    st.markdown("**å‡ºé™¢åŒ»å˜±**")
                    for advice in data["å‡ºé™¢åŒ»å˜±"]:
                        st.write(f"- {advice}")
                    
    except Exception as e:
        st.error(f"æ˜¾ç¤ºæœç´¢ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error("æ˜¾ç¤ºæœç´¢ç»“æœå¤±è´¥", exc_info=True)

def get_patient_metadata(patient_name: str) -> Dict[str, Any]:
    """è·å–æ‚£è€…çš„å®é™…æ•°æ®ç»“æ„"""
    try:
        with OracleGraphStore() as graph_store:
            patient_info = graph_store.get_patient_info(patient_name)
            if not patient_info:
                return {}
            return patient_info
    except Exception as e:
        logger.error(f"è·å–æ‚£è€…å…ƒæ•°æ®å¤±è´¥: {str(e)}")
        return {}

def analyze_graph_query(query_text: str) -> Dict[str, Any]:
    """ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå›¾æ•°æ®æŸ¥è¯¢æ„å›¾"""
    try:
        # ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–æ‚£è€…å§“å
        patient_name = None
        for name in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]:
            if name in query_text:
                patient_name = name
                break
        
        if not patient_name:
            logger.warning("æœªèƒ½ä»æŸ¥è¯¢ä¸­è¯†åˆ«å‡ºæ‚£è€…å§“å")
            return {
                "query_type": "åŸºæœ¬ä¿¡æ¯",
                "field": "all",
                "patient_name": None,
                "explanation": "æœªèƒ½è¯†åˆ«æ‚£è€…å§“å"
            }
            
        # è·å–è¯¥æ‚£è€…çš„å®é™…æ•°æ®ç»“æ„
        patient_data = get_patient_metadata(patient_name)
        if not patient_data:
            logger.warning(f"æœªæ‰¾åˆ°æ‚£è€… {patient_name} çš„æ•°æ®")
            return {
                "query_type": "åŸºæœ¬ä¿¡æ¯",
                "field": "all",
                "patient_name": patient_name,
                "explanation": f"æœªæ‰¾åˆ°æ‚£è€… {patient_name} çš„æ•°æ®"
            }
            
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹åŒ»ç–—æŸ¥è¯¢ï¼Œæå–æŸ¥è¯¢æ„å›¾å’Œå…³é”®ä¿¡æ¯ã€‚ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ¼å¼æˆ–ä»£ç å—æ ‡è®°ã€‚

        æŸ¥è¯¢æ–‡æœ¬ï¼š{query_text}

        æ‚£è€… {patient_name} çš„å®é™…æ•°æ®ç»“æ„ä¸‹ï¼š
        {json.dumps(patient_data, ensure_ascii=False, indent=2)}

        ä½ éœ€è¦åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ˆä¸è¦æ·»åŠ ä»»ä½•markdownæ ¼å¼æˆ–ä»£ç å—æ ‡è®°ï¼‰ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - query_type: æŸ¥è¯¢ç±»å‹ï¼Œå¿…ä»¥ä¸‹ä¹‹ä¸€ï¼šåŸºæœ¬ä¿¡æ¯/ä¸»è¯‰ä¸è¯Šæ–­/ç°ç—…å²/ç”Ÿå‘½ä½“å¾/ç”ŸåŒ–æŒ‡æ ‡/è¯Šç–—ç»è¿‡
        - field: å…·ä½“æŸ¥è¯¢çš„å­—æ®µåï¼Œå¦‚æœæ˜¯æŸ¥è¯¢æ•´ä¸ªç±»åˆ«ï¼Œè¯·è¿”å›"all"
        - patient_name: æ‚£è€…å§“å
        - explanation: æŸ¥è¯¢æ„å›¾çš„è§£é‡Š

        å¦‚æœæ˜¯æŸ¥è¯¢ä¸ªäººä¿¡æ¯ï¼Œåº”è¿”å›ï¼š
        {{
            "query_type": "åŸºæœ¬ä¿¡æ¯",
            "field": "all",
            "patient_name": "{patient_name}",
            "explanation": "æŸ¥è¯¢æ‚£è€…çš„æ‰€æœ‰åŸºæœ¬ä¿¡æ¯"
        }}

        å¦‚æœæ˜¯æŸ¥è¯¢ä¸ªç±»åˆ«çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆå¦‚"ç”ŸåŒ–æŒ‡æ ‡"ã€"ä¸»è¯‰ä¸è¯Šæ–­"ç­‰ï¼‰ï¼Œè¯·å°†fieldè®¾ç½®ä¸º"all"ã€‚
        å¦‚æœæ˜¯æŸ¥è¯¢å…·ä½“çš„æŒ‡æ ‡æˆ–ç—‡çŠ¶ï¼ˆå¦‚"ç™½ç»†èƒ"ã€"è¡€å‹"ç­‰ï¼‰ï¼Œè¯·å°†fieldè®¾ç½®ä¸ºå…·ä½“çš„æŒ‡æ ‡åç§°ã€‚

        è¯·åˆ†æè¿™ä¸ªæŸ¥è¯¢å¹¶è¿”å›JSONï¼ˆä¸è¦æ·»åŠ markdownæ ¼å¼ï¼‰
        """

        client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )
        
        response = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ¼å¼æˆ–ä»£ç å—æ ‡è®°ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content.strip()
        logger.info(f"OpenAI APIè¿”å›å†…å®¹: {content}")
        
        # å°è¯•è§£æè¿”å›å†…å®¹
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            try:
                result = json.loads(content)
            except json.JSONDecodeError:
                logger.warning("ç›´æ¥è§£æJSONå¤±è´¥ï¼Œå°è¯•æ¸…ç†å†…å®¹åé‡æ–°è§£æ")
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ¸…ç†å†…å®¹ï¼ˆå»é™¤å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦ç­‰ï¼‰
                cleaned_content = content.replace('\n', '').replace('\r', '').strip()
                result = json.loads(cleaned_content)
            
            # å¦‚æœç»“æœè¢«åŒ…è£…åœ¨responseå­—æ®µä¸­ï¼Œæå–å†…å±‚JSON
            if isinstance(result, dict) and "response" in result:
                try:
                    inner_content = result["response"]
                    if isinstance(inner_content, str):
                        # æ¸…ç†å†…å±‚JSONå­—ç¬¦ä¸²
                        inner_content = inner_content.replace('\n', '').replace('\r', '').strip()
                        result = json.loads(inner_content)
                    elif isinstance(inner_content, dict):
                        result = inner_content
                except json.JSONDecodeError as e:
                    logger.error(f"è§£æresponseå­—æ®µå¤±è´¥: {str(e)}")
                    result = {
                        "query_type": "åŸºæœ¬ä¿¡æ¯",
                        "field": "all",
                        "patient_name": patient_name,
                        "explanation": "è§£ææŸ¥è¯¢æ„å›¾æ—¶å‡ºç°é”™è¯¯"
                    }
            
            # éªŒè¯ç»“æœæ ¼å¼
            if not isinstance(result, dict):
                raise ValueError("è¿”å›ç»“æœä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ["query_type", "field", "patient_name", "explanation"]
            missing_fields = [field for field in required_fields if field not in result]
            if missing_fields:
                logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
                for field in missing_fields:
                    if field == "query_type":
                        result[field] = "åŸºæœ¬ä¿¡æ¯"
                    elif field == "field":
                        result[field] = "all"
                    elif field == "patient_name":
                        result[field] = patient_name
                    elif field == "explanation":
                        result[field] = "æŸ¥è¯¢æ‚£è€…ä¿¡æ¯"
            
            # éªŒè¯query_typeæ˜¯å¦ä¸ºæœ‰æ•ˆå€¼
            valid_query_types = ["åŸºæœ¬ä¿¡æ¯", "ä¸»è¯‰ä¸è¯Šæ–­", "ç°ç—…å²", "ç”Ÿå‘½ä½“å¾", "ç”ŸåŒ–æŒ‡æ ‡", "è¯Šç–—ç»è¿‡"]
            if result["query_type"] not in valid_query_types:
                logger.warning(f"æ— æ•ˆquery_type: {result['query_type']}, ä½¿ç”¨é»˜è®¤å€¼")
                result["query_type"] = "åŸºæœ¬ä¿¡æ¯"
            
            # ç¡®ä¿patient_nameä¸æŸ¥ä¸­è¯†åˆ«çš„ä¸€è‡´
            if result["patient_name"] != patient_name:
                logger.warning(f"patient_nameä¸åŒ¹é…: {result['patient_name']} != {patient_name}")
                result["patient_name"] = patient_name
            
            # å¤„ç†fieldå€¼
            if result["field"] == result["query_type"] or result["field"] in valid_query_types:
                logger.info(f"å°†fieldæ”¹ä¸º {result['field']}")
                result["field"] = "all"
            
            logger.info(f"æŸ¥è¯¢æ„å›¾åˆ†æç»“æœ: {json.dumps(result, ensure_ascii=False)}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {str(e)}, åŸå§‹å†…å®¹: {content}")
            return {
                "query_type": "åŸºæœ¬ä¿¡æ¯",
                "field": "all",
                "patient_name": patient_name,
                "explanation": "è§£ææŸ¥è¯¢æ„å›¾æ—¶å‡ºç°é”™è¯¯"
            }
            
    except Exception as e:
        logger.error(f"åˆ†ææŸ¥è¯¢æ„å›¾å¤±è´¥: {str(e)}")
        return {
            "query_type": "åŸºæœ¬ä¿¡æ¯",
            "field": "all",
            "patient_name": patient_name if 'patient_name' in locals() else None,
            "explanation": "åˆ†ææŸ¥è¯¢æ„å›¾æ—¶å‡ºç°é”™è¯¯"
        }

def search_graph_data(query_text: str) -> List[Dict[str, Any]]:
    """åŸºäºå›¾æ•°æ®çš„æœç´¢"""
    try:
        # ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾
        analysis = analyze_graph_query(query_text)
        query_type = analysis.get("query_type")
        field = analysis.get("field")
        patient_name = analysis.get("patient_name")
        
        if not all([query_type, patient_name]):
            logger.error("æŸ¥è¯¢æ„å›¾åˆ†æç»“æœä¸å®Œæ•´")
            return []
        
        # ä½¿ç”¨å›¾æ•°æ®åº“æœç´¢
        with OracleGraphStore() as graph_store:
            # è·å–æ‚£è€…ä¿¡æ¯
            patient_info = graph_store.get_patient_info(patient_name)
            if not patient_info:
                return []
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹è¿”å›ç»“æœ
            if query_type == "åŸºæœ¬ä¿¡æ¯":
                if field == "all":
                    # è¿”å›æ‰€æœ‰ä¿¡æ¯
                    info = patient_info.get("æ‚£è€…", {}).get("åŸºæœ¬ä¿¡æ¯", {})
                    if not info:
                        info = patient_info.get("åŸºæœ¬ä¿¡æ¯", {})
                    if info:
                        result = []
                        for k, v in info.items():
                            result.append(f"{k}ï¼š{v}")
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„åŸºæœ¬ä¿¡æ¯ï¼š\n" + "\n".join(result),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    # æŸ¥è¯¢ç‰¹å®šå­—æ®µ
                    value = None
                    # å…ˆå°è¯•ä»æ‚£è€….åŸºæœ¬ä¿¡æ¯ä¸­è·å–
                    info = patient_info.get("æ‚£è€…", {}).get("åŸºæœ¬ä¿¡æ¯", {})
                    if not info:
                        # å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»åŸºæœ¬ä¿¡æ¯ä¸­è·å–
                        info = patient_info.get("åŸºæœ¬ä¿¡æ¯", {})
                    value = info.get(field)
                    if value:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}æ˜¯{value}",
                            "explanation": analysis.get("explanation")
                        }}]
            
            elif query_type == "ä¸»è¯‰ä¸è¯Šæ–­":
                items = patient_info.get("ä¸»è¯‰ä¸è¯Šæ–­", [])
                if field == "all":
                    results = []
                    # åˆ†ç±»å¤„ç†ä¸»è¯‰å’Œè¯Šæ–­
                    chief_complaints = []
                    admission_diagnoses = []
                    discharge_diagnoses = []
                    for item in items:
                        if item.get("ç±»å‹") == "ä¸»è¯‰":
                            chief_complaints.append(item.get("å†…å®¹"))
                        elif item.get("ç±»å‹") == "å…¥é™¢è¯Šæ–­":
                            admission_diagnoses.append(item.get("å†…å®¹"))
                        elif item.get("ç±»å‹") == "å‡ºé™¢è¯Šæ–­":
                            discharge_diagnoses.append(item.get("å†…å®¹"))
                    
                    # ç»„ç»‡è¿”å›å†…å®¹
                    if chief_complaints:
                        results.append("ä¸»è¯‰ï¼š")
                        results.extend([f"- {complaint}" for complaint in chief_complaints])
                    if admission_diagnoses:
                        results.append("\nå…¥é™¢è¯Šæ–­ï¼š")
                        results.extend([f"- {diagnosis}" for diagnosis in admission_diagnoses])
                    if discharge_diagnoses:
                        results.append("\nå‡ºé™¢è¯Šæ–­ï¼š")
                        results.extend([f"- {diagnosis}" for diagnosis in discharge_diagnoses])
                        
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ä¸»è¯‰ä¸è¯Šæ–­ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    results = []
                    for item in items:
                        if field.lower() in item.get("ç±»å‹", "").lower():
                            results.append(item.get("å†…å®¹"))
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}ï¼š\n" + "\n".join([f"- {r}" for r in results]),
                            "explanation": analysis.get("explanation")
                        }}]
            
            elif query_type == "ç°ç—…å²":
                items = patient_info.get("ç°ç—…å²", [])
                if field == "all":
                    results = []
                    for item in items:
                        symptom = item.get("ç—‡çŠ¶", "")
                        description = item.get("æè¿°", "")
                        if description:
                            results.append(f"- {symptom}ï¼š{description}")
                        else:
                            results.append(f"- {symptom}")
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ç°ç—…å²ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    results = []
                    for item in items:
                        if field.lower() in item.get("ç—‡çŠ¶", "").lower():
                            description = item.get("æè¿°", "")
                            if description:
                                results.append(f"{item.get('ç—‡çŠ¶')}ï¼š{description}")
                            else:
                                results.append(item.get('ç—‡çŠ¶'))
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
            
            elif query_type == "ç”Ÿå‘½ä½“å¾":
                items = patient_info.get("ç”Ÿå‘½ä½“å¾", [])
                if field == "all":
                    results = []
                    for item in items:
                        results.append(f"- {item.get('æŒ‡æ ‡')}ï¼š{item.get('æ•°å€¼')}{item.get('å•ä½', '')}")
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ç”Ÿå‘½ä½“å¾ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    for item in items:
                        if field.lower() in item.get("æŒ‡æ ‡", "").lower():
                            return [{"type": "answer", "data": {
                                "question": query_text,
                                "answer": f"{patient_name}çš„{field}æ˜¯ï¼š{item.get('æ•°å€¼')}{item.get('å•ä½', '')}",
                                "explanation": analysis.get("explanation")
                            }}]
            
            elif query_type == "ç”ŸåŒ–æŒ‡æ ‡":
                items = patient_info.get("ç”ŸåŒ–æŒ‡æ ‡", [])
                if field == "all":
                    results = []
                    # æŒ‰ç…§å‚è€ƒèŒƒå›´åˆ†ç±»
                    abnormal_items = []
                    normal_items = []
                    for item in items:
                        result_str = f"- {item.get('é¡¹ç›®')}ï¼š{item.get('ç»“æœ')}{item.get('å•ä½', '')}"
                        if item.get('å‚è€ƒèŒƒå›´') == 'å¼‚å¸¸':
                            abnormal_items.append(result_str + " (å¼‚å¸¸)")
                        else:
                            normal_items.append(result_str + " (æ­£å¸¸)")
                    
                    # ç»„ç»‡è¿”å›å†…å®¹
                    if abnormal_items:
                        results.append("å¼‚å¸¸æŒ‡æ ‡ï¼š")
                        results.extend(abnormal_items)
                    if normal_items:
                        if results:  # å¦‚æœå·²ç»æœ‰å¼‚å¸¸æŒ‡æ ‡ï¼Œæ·»åŠ ç©ºè¡Œ
                            results.append("")
                        results.append("æ­£å¸¸æŒ‡æ ‡ï¼š")
                        results.extend(normal_items)
                        
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ç”ŸåŒ–æŒ‡æ ‡ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    for item in items:
                        if field.lower() in item.get("é¡¹ç›®", "").lower():
                            return [{"type": "answer", "data": {
                                "question": query_text,
                                "answer": f"{patient_name}çš„{field}æ˜¯ï¼š{item.get('ç»“æœ')}{item.get('å•ä½', '')} ({item.get('å‚è€ƒèŒƒå›´', '')})",
                                "explanation": analysis.get("explanation")
                            }}]
            
            elif query_type == "è¯Šç–—ç»è¿‡":
                items = patient_info.get("è¯Šç–—ç»è¿‡", [])
                if field == "all":
                    results = []
                    # åˆ†ç±»å¤„ç†è¯Šç–—ç»è¿‡å’Œå‡ºé™¢åŒ»å˜±
                    diagnoses = []
                    advices = []
                    for item in items:
                        if item.get("ç±»å‹") == "è¯Šç–—ç»è¿‡":
                            diagnoses.append(item.get("å†…å®¹"))
                        elif item.get("ç±»å‹") == "å‡ºé™¢åŒ»å˜±":
                            advices.append(item.get("å†…å®¹"))
                    
                    # ç»„ç»‡è¿”å›å†…å®¹
                    if diagnoses:
                        results.append("è¯Šç–—ç»è¿‡ï¼š")
                        results.extend([f"- {diagnosis}" for diagnosis in diagnoses])
                    if advices:
                        if results:  # å¦‚æœå·²ç»æœ‰è¯Šç–—ç»è¿‡ï¼Œæ·»åŠ ç©ºè¡Œ
                            results.append("")
                        results.append("å‡ºé™¢åŒ»å˜±ï¼š")
                        results.extend([f"- {advice}" for advice in advices])
                        
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„è¯Šç–—ç»è¿‡ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    results = []
                    for item in items:
                        if field.lower() in item.get("ç±»å‹", "").lower():
                            results.append(item.get("å†…å®¹"))
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}ï¼š\n" + "\n".join([f"- {r}" for r in results]),
                            "explanation": analysis.get("explanation")
                        }}]
            
            return []
            
    except Exception as e:
        logger.error(f"å›¾æ•°æ®æœç´¢å¤±è´¥: {str(e)}")
        return []

def display_graph_results(results: List[Dict[str, Any]], query_text: str):
    """æ˜¾ç¤ºå›¾æ•°æ®æœç´¢ç»“æœ"""
    if not results:
        st.warning("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
        return
        
    for result in results:
        if result["type"] == "answer":
            data = result["data"]
            st.write(data["answer"])

def process_graph_data(file_path: str, content: str) -> bool:
    """å¤„ç†æ–‡æ¡£çš„å›¾æ•°æ®"""
    try:
        # åªå¤„ç†å›¾æ•°æ®
        graph_parser = MedicalGraphParser()
        graph_result = graph_parser.parse_to_graph(content, file_path)
        
        if "error" in graph_result:
            return False
            
        return True
    except Exception as e:
        logger.error(f"å¤„ç†å›¾æ•°æ®å¤±è´¥: {str(e)}")
        return False

def visualize_patient_graph(patient_info: Dict[str, Any]) -> str:
    """ä½¿ç”¨pyviså¯è§†åŒ–æ‚£è€…çš„å±æ€§å›¾"""
    try:
        # åˆ›å»ºç½‘ç»œå›¾
        net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="black", directed=False)
        
        # è®¾ç½®ç‰©ç†å¸ƒå±€é€‰é¡¹
        net.set_options("""
        {
          "nodes": {
            "font": {
              "size": 14,
              "face": "Microsoft YaHei"
            }
          },
          "edges": {
            "color": {
              "color": "#666666",
              "highlight": "#000000"
            }
          },
          "physics": {
            "enabled": true,
            "solver": "forceAtlas2Based",
            "forceAtlas2Based": {
              "gravitationalConstant": -50,
              "centralGravity": 0.01,
              "springLength": 100,
              "springConstant": 0.08,
              "damping": 0.4,
              "avoidOverlap": 0.5
            }
          }
        }
        """)
        
        # æ·»åŠ æ‚£è€…èŠ‚ç‚¹ï¼ˆä¸­å¿ƒèŠ‚ç‚¹ï¼‰
        patient_name = patient_info.get('å§“å', 'æœªçŸ¥æ‚£è€…')
        
        net.add_node(patient_name, 
                    label=patient_name,
                    color='#add8e6',  # lightblue
                    size=30,
                    shape='circle')
        
        # æ·»åŠ åŸºæœ¬ä¿¡æ¯èŠ‚ç‚¹
        basic_info = patient_info.get('åŸºæœ¬ä¿¡æ¯', {})
        if basic_info:
            for key, value in basic_info.items():
                node_id = f'basic_{key}'
                net.add_node(node_id,
                            label=f'{key}ï¼š{value}',
                            color='#90EE90',  # lightgreen
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='åŸºæœ¬ä¿¡æ¯')
        
        # æ·»åŠ ä¸»è¯‰ä¸è¯Šæ–­èŠ‚ç‚¹
        if 'ä¸»è¯‰ä¸è¯Šæ–­' in patient_info:
            for i, item in enumerate(patient_info['ä¸»è¯‰ä¸è¯Šæ–­']):
                node_id = f'diag_{i}'
                net.add_node(node_id,
                            label=f"{item.get('ç±»å‹')}ï¼š{item.get('å†…å®¹')}",
                            color='#FFB6C1',  # lightpink
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ä¸»è¯‰ä¸è¯Šæ–­')
        
        # æ·»åŠ ç°ç—…å²èŠ‚ç‚¹
        if 'ç°ç—…å²' in patient_info:
            for i, item in enumerate(patient_info['ç°ç—…å²']):
                node_id = f'hist_{i}'
                net.add_node(node_id,
                            label=f"{item.get('ç—‡çŠ¶')}ï¼š{item.get('æè¿°')}",
                            color='#FFFFE0',  # lightyellow
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ç°ç—…å²')

        # æ·»åŠ ç”Ÿå‘½ä½“å¾èŠ‚ç‚¹
        if 'ç”Ÿå‘½ä½“å¾' in patient_info:
            for i, item in enumerate(patient_info['ç”Ÿå‘½ä½“å¾']):
                node_id = f'vital_{i}'
                net.add_node(node_id,
                            label=f"{item.get('æŒ‡æ ‡')}ï¼š{item.get('æ•°å€¼')}",
                            color='#F08080',  # lightcoral
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ç”Ÿå‘½ä½“å¾')
        
        # æ·»åŠ ç”ŸåŒ–æŒ‡æ ‡ï¿½ï¿½ï¿½ç‚¹
        if 'ç”ŸåŒ–ï¿½ï¿½ï¿½ï¿½æ ‡' in patient_info:
            for i, item in enumerate(patient_info['ç”ŸåŒ–æŒ‡æ ‡']):
                node_id = f'biochem_{i}'
                net.add_node(node_id,
                            label=f"{item.get('é¡¹ç›®')}ï¼š{item.get('ç»“æœ')}",
                            color='#DDA0DD',  # plum
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ç”ŸåŒ–æŒ‡æ ‡')
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜HTML
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.html', encoding='utf-8') as f:
            try:
                net.save_graph(f.name)
                return f.name
            except Exception as e:
                raise
                
    except Exception as e:
        raise

def display_parsed_documents():
    """æ˜¾ç¤ºå·²è§£æçš„æ–‡æ¡£"""
    st.subheader("æŸ¥çœ‹å·²è§£æçš„æ–‡æ¡£")
    
    try:
        with OracleGraphStore() as graph_store:
            patients = graph_store.get_all_patients()
            
            if not patients:
                st.info("ğŸ“­ æ•°æ®åº“ä¸­æš‚æ— ç»“æ„åŒ–æ–‡æ¡£ï¼Œè¯·å…ˆåœ¨æ–‡æ¡£ç®¡ç†ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
                return
                
            st.write("å·²è§£æçš„æ–‡æ¡£ä¸­åŒ…å«ä»¥ä¸‹æ‚£è€…ï¼š")
            
            # æ‚£ï¿½ï¿½ï¿½åˆ—è¡¨
            for patient in patients:
                patient_name = patient.get('å§“å', 'æœªçŸ¥æ‚£è€…')
                # ä½¿ç”¨expanderä½¿æ¯ä¸ªæ‚£è€…çš„ä¿¡æ¯é»˜è®¤æŠ˜å 
                with st.expander(f"ğŸ“‹ {patient_name}", expanded=False):
                    # è·å–æ‚£è€…å®Œæ•´ä¿¡æ¯
                    patient_info = graph_store.get_patient_info(patient_name)
                    if patient_info:
                        # åˆ›å»ºä¸¤é¡µæ ‡ç­¾
                        tab1, tab2 = st.tabs(["çŸ¥è¯†å›¾è°±", "å®Œæ•´æ•°æ®"])
                        
                        with tab1:
                            try:
                                # åˆ›å»ºå¹¶æ˜¾ç¤ºäº¤äº’å¼ç½‘ç»œå›¾
                                html_path = visualize_patient_graph(patient_info)
                                with open(html_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                components.html(html_content, height=600)
                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                os.unlink(html_path)
                            except Exception as e:
                                st.error(f"æ˜¾ç¤ºå›¾å½¢æ—¶å‡ºé”™: {str(e)}")
                        
                        with tab2:
                            st.json(patient_info)
                    else:
                        st.error("æ— è·å–æ‚£è€…è¯¦ç»†ä¿¡æ¯")
            
    except Exception as e:
        logger.error(f"æ˜¾ç¤ºå·²è§£ææ–‡æ¡£å¤±è´¥: {str(e)}")
        st.error("æ˜¾ç¤ºå·²è§£ææ–‡æ¡£æ—¶å‡ºç°é”™è¯¯")

def display_document_management():
    """æ˜¾ç¤ºæ–‡æ¡£ç®¡ç†ç•Œé¢"""
    st.header("æ–‡æ¡£ç®¡ç†")
    
    # æ˜¾ç¤ºå·²ä¸Šä¼ çš„æ–‡æ¡£
    st.subheader("å·²ä¸Šä¼ çš„æ–‡æ¡£")
    files = list(UPLOAD_DIR.glob("*.*"))
    if files:
        for file in files:
            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
            with col1:
                st.write(file.name)
            with col2:
                if st.button("å‘é‡åŒ–", key=f"vec_{file.name}"):
                    with st.spinner("æ­£åœ¨å‘é‡åŒ–..."):
                        if vectorize_document(file):
                            st.success("å‘é‡åŒ–æˆåŠŸ")
                        else:
                            st.error("å‘é‡åŒ–å¤±è´¥")
            with col3:
                if st.button("ç»“æ„åŒ–", key=f"struct_{file.name}"):
                    with st.spinner("æ­£åœ¨ç»“æ„åŒ–..."):
                        # è¯»å–æ–‡ä»¶å†…å®¹
                        content = read_file_content(file)
                        file_obj = type('FileObject', (), {
                            'name': file.name,
                            'type': 'application/pdf' if file.suffix == '.pdf' else 'text/plain',
                            'read': lambda: open(file, 'rb').read()
                        })()
                        if parse_document_to_json(file_obj):
                            st.success("ç»“æ„åŒ–æˆåŠŸ")
                        else:
                            st.error("ç»“æ„åŒ–å¤±è´¥")
            with col4:
                if st.button("å›¾æ•°æ®", key=f"graph_{file.name}"):
                    with st.spinner("æ­£åœ¨å¤„ç†å›¾æ•°æ®..."):
                        # è¯»å–æ–‡ä»¶å†…å®¹
                        content = read_file_content(file)
                        if process_graph_data(file.name, content):
                            st.success("å›¾æ•°æ®å¤„ç†æˆåŠŸ")
                        else:
                            st.error("å›¾æ•°æ®å¤„ç†å¤±è´¥")
            st.markdown("---")
    else:
        st.info("æš‚æ— ä¸Šä¼ çš„æ–‡æ¡£")
        
    # æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
    uploaded_file = st.file_uploader("ä¸Šä¼ åŒ»ç–—æ–‡æ¡£", type=["pdf", "docx", "txt"])
    
    if uploaded_file:
        col1, col2 = st.columns([2, 1])
        with col1:
            st.write(f"å·²é€‰æ‹© {uploaded_file.name}")
        with col2:
            if st.button("ä¿å­˜æ–‡æ¡£"):
                with st.spinner(f"æ­£åœ¨ä¿å­˜ {uploaded_file.name}..."):
                    file_path = save_uploaded_file(uploaded_file)
                    if file_path:
                        st.success(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
                    else:
                        st.error("æ–‡ä»¶ä¿å­˜å¤±è´¥")

def display_vector_search():
    """æ˜¾ç¤ºå‘é‡æ£€ç´¢ç•Œé¢"""
    st.header("å‘é‡æ£€ç´¢")
    
    # æ˜¾ç¤ºå·²å‘é‡åŒ–çš„æ–‡æ¡£åˆ—è¡¨
    with st.expander("æŸ¥çœ‹å·²å‘é‡åŒ–çš„æ–‡æ¡£", expanded=True):
        with OracleVectorStore() as vector_store:
            documents = vector_store.list_documents()
            if documents:
                st.write("å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼š")
                for doc in documents:
                    st.write(f"- {doc['file_path']} (å‘é‡æ•°: {doc['chunk_count']})")
            else:
                st.info("æš‚æ— å‘é‡åŒ–æ–‡æ¡£")
    
    # æœç´¢åŠŸèƒ½
    query = st.text_input("è¯·è¾“å…¥æœç´¢å†…å®¹")
    if query:
        with st.spinner("æ­£åœ¨æœç´¢å¹¶åˆ†æ..."):
            results = search_similar_documents(query, top_k=3)
            
            if results:
                # æ˜¾ç¤ºæœ€ç›¸å…³æ–‡æ¡£çš„ GPT åˆ†æç»“æœ
                st.subheader("GPT åˆ†æç»“æœ")
                st.write(results[0]['gpt_analysis'])
                
                # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æ–‡æ¡£
                st.subheader("ç›¸å…³æ–‡æ¡£")
                for i, result in enumerate(results, 1):
                    similarity = 1 - result['similarity']
                    with st.expander(f"æ–‡æ¡£ {i}: {result['file_path']} (ç›¸ä¼¼åº¦: {similarity:.2%})"):
                        st.write(result['content'])
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

def display_structured_search():
    """æ˜¾ç¤ºç»“æ„åŒ–æ£€ç´¢ç•Œé¢"""
    st.title("ç»“æ„åŒ–æ£€ç´¢")
    
    try:
        # åˆ›å»º OracleJsonStore å®ä¾‹
        logger.info("å¼€å§‹åˆå§‹åŒ– OracleJsonStore...")
        with OracleJsonStore() as json_store:
            # è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯
            docs_sql = """
            SELECT doc_info, created_at 
            FROM DOCUMENT_JSON 
            ORDER BY created_at DESC
            """
            logger.info(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {docs_sql}")
            docs = json_store.execute_search(docs_sql)
            logger.info(f"æŸ¥è¯¢ç»“æœ: {docs}")
            
            # æ˜¾ç¤ºæ–‡æ¡£åˆ—è¡¨
            st.write("è°ƒè¯•ä¿¡æ¯ - æ–‡æ¡£åˆ—è¡¨:", docs)  # ä¸´æ—¶è°ƒè¯•è¾“å‡º
            st.subheader("ğŸ“š å·²å¯¼å…¥çš„ç»“æ„åŒ–æ–‡æ¡£")
            
            if docs:
                logger.info(f"æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
                for doc in docs:
                    logger.info(f"å¤„ç†æ–‡æ¡£: {doc}")
                    created_time = doc.get('created_at', 'æœªçŸ¥æ—¶é—´')
                    doc_info = doc.get('doc_info', 'æœªçŸ¥æ–‡æ¡£')
                    st.write(f"- {doc_info} (å¯¼å…¥æ—¶é—´: {created_time})")
            else:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
                st.warning("æ•°æ®åº“ä¸­è¿˜æ²¡æœ‰ç»“æ„åŒ–æ–‡æ¡£")
                return
                
            # è·å–ç”¨æˆ·è¾“å…¥
            query = st.text_input("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼ˆæ”¯æŒç»“æ„åŒ–æ•°æ®æœç´¢ï¼‰")
            
            if not query:
                return
                
            # æœç´¢æ–‡æ¡£
            results = search_documents(query)
            
            if not results:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                return
                
            # æ˜¾ç¤ºæœç´¢ç»“æœ
            display_search_results(query, results)
                
    except Exception as e:
        logger.error(f"æ£€ç´¢æ–‡æ¡£å¤±è´¥", exc_info=True)
        st.error(f"æ£€ç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
        # æ˜¾ç¤ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        import traceback
        st.code(traceback.format_exc())

def display_property_graph_search():
    """æ˜¾ç¤ºå±æ€§å›¾æ£€ç´¢ç•Œé¢"""
    st.header("å±æ€§å›¾æ£€ç´¢")
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2 = st.tabs(["æŸ¥è¯¢æ¨¡æ¿", "è‡ªå®šä¹‰æŸ¥è¯¢"])
    
    with tab1:
        st.subheader("å¸¸ç”¨æŸ¥è¯¢æ¨¡æ¿")
        query_type = st.selectbox(
            "é€‰æ‹©æŸ¥è¯¢ç±»å‹",
            [
                "æ‚£è€…ç›¸ä¼¼ç—‡çŠ¶åˆ†æ",
                "æ‚£è€…ç”ŸåŒ–æŒ‡æ ‡å¼‚å¸¸å…³è”",
                "æ‚£è€…è¯Šæ–­å…³ç³»ç½‘ç»œ",
                "æ‚£è€…ç”¨è¯å…³è”åˆ†æ",
                "æ‚£è€…æ²»ç–—æ–¹æ¡ˆå¯¹æ¯”"
            ]
        )
        
        if query_type == "æ‚£è€…ç›¸ä¼¼ç—‡çŠ¶åˆ†æ":
            patient_name = st.selectbox("é€‰æ‹©æ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æç›¸ä¼¼ç—‡çŠ¶..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä¸»æŸ¥è¯¢
                            query = """
                            SELECT *
                            FROM GRAPH_TABLE ( MEDICAL_KG
                                MATCH (v1) -[e1]-> (s1), (v2) -[e2]-> (s2)
                                WHERE v1.ENTITY_TYPE = 'æ‚£è€…'
                                AND v2.ENTITY_TYPE = 'æ‚£è€…'
                                AND v1.ENTITY_NAME = :patient_name
                                AND v1.ENTITY_NAME != v2.ENTITY_NAME
                                AND e1.RELATION_TYPE = 'ç°ç—…å²'
                                AND e2.RELATION_TYPE = 'ç°ç—…å²'
                                COLUMNS (
                                    v1.ENTITY_NAME AS patient1,
                                    v2.ENTITY_NAME AS patient2,
                                    JSON_VALUE(s1.ENTITY_VALUE, '$.ç—‡çŠ¶') AS symptom1,
                                    JSON_VALUE(s2.ENTITY_VALUE, '$.ç—‡çŠ¶') AS symptom2
                                )
                            )
                            """
                            results = graph_store.execute_pgql(query, {"patient_name": patient_name})
                            if results:
                                # æ„å»ºç”¨äºåˆ†æçš„æ–‡æœ¬
                                analysis_text = []
                                target_symptoms = []
                                other_patients = {}
                                
                                # æ•´ç†ç—‡çŠ¶æ•°æ®
                                for result in results:
                                    if result['symptom1']:
                                        target_symptoms.append(result['symptom1'])
                                    if result['patient2'] not in other_patients:
                                        other_patients[result['patient2']] = set()
                                    if result['symptom2']:
                                        other_patients[result['patient2']].add(result['symptom2'])
                                
                                if target_symptoms:
                                    analysis_text.append(f"ç›®æ ‡æ‚£è€… {patient_name} çš„ç—‡çŠ¶ï¼š")
                                    for symptom in sorted(set(target_symptoms)):
                                        analysis_text.append(f"- {symptom}")
                                    
                                    analysis_text.append("\nå…¶ä»–æ‚£è€…çš„ç—‡çŠ¶ï¼š")
                                    for p_name, symptoms in other_patients.items():
                                        if symptoms:
                                            analysis_text.append(f"\n{p_name} çš„ç—‡çŠ¶ï¼š")
                                            for symptom in sorted(symptoms):
                                                analysis_text.append(f"- {symptom}")
                                    
                                    # è°ƒç”¨OpenAIè¿›è¡Œåˆ†æ
                                    with st.spinner("æ­£åœ¨åˆ†æç—‡çŠ¶ç›¸ä¼¼åº¦..."):
                                        try:
                                            client = OpenAI(
                                                api_key=os.getenv('OPENAI_API_KEY'),
                                                base_url=os.getenv('OPENAI_API_BASE')
                                            )
                                            
                                            # æ„å»ºæç¤ºè¯
                                            prompt = f"""è¯·åˆ†æä»¥ä¸‹æ‚£è€…çš„ç—‡çŠ¶ä¿¡æ¯ï¼Œæ‰¾å‡ºç—‡çŠ¶ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œå¯èƒ½çš„å…³è”ï¼š

æ‚£è€…ç—‡çŠ¶ä¿¡æ¯ï¼š
{chr(10).join(analysis_text)}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æï¼š
1. å„ä¸ªæ‚£è€…ä¸ç›®æ ‡æ‚£è€…ï¼ˆ{patient_name}ï¼‰ç—‡çŠ¶çš„ç›¸ä¼¼åº¦ï¼ˆç”¨ç™¾åˆ†æ¯”è¡¨ç¤ºï¼‰
2. ç—‡çŠ¶çš„ç›¸ä¼¼æ€§å’Œå…³è”æ€§åˆ†æ
3. å¯èƒ½çš„å…±åŒç—…å› 
4. éœ€è¦æ³¨æ„çš„åŒ»å­¦é—®é¢˜

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶å°½å¯èƒ½ä¸“ä¸šå’Œè¯¦ç»†ã€‚å¯¹äºç—‡çŠ¶ç›¸ä¼¼åº¦çš„åˆ†æï¼Œè¯·ç»™å‡ºå…·ä½“çš„ç™¾åˆ†æ¯”æ•°å€¼ã€‚"""

                                            response = client.chat.completions.create(
                                                model=os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo'),
                                                messages=[
                                                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦é¡¾é—®ï¼Œæ“…é•¿åˆ†ææ‚£è€…ç—‡çŠ¶çš„ç›¸ä¼¼åº¦ã€‚è¯·ä»åŒ»å­¦ä¸“ä¸šçš„è§’åº¦åˆ†æç—‡çŠ¶çš„ç›¸å…³æ€§å’Œå¯èƒ½çš„ç—…å› ã€‚"},
                                                    {"role": "user", "content": prompt}
                                                ],
                                                temperature=0.7
                                            )
                                            
                                            analysis_result = response.choices[0].message.content
                                            st.write("### ç—‡çŠ¶åˆ†æç»“æœ")
                                            st.write(analysis_result)
                                        except Exception as e:
                                            st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                                            logger.error(f"åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
                                else:
                                    st.warning(f"æœªæ‰¾åˆ° {patient_name} çš„ç—‡çŠ¶è®°å½•")
                            else:
                                st.info("æœªæ‰¾åˆ°ä»»ä½•ç—‡çŠ¶è®°å½•")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        logger.error(f"åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
                        
        elif query_type == "æ‚£è€…ç”ŸåŒ–æŒ‡æ ‡å¼‚å¸¸å…³è”":
            patient_name = st.selectbox("é€‰æ‹©æ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æç”ŸåŒ–æŒ‡æ ‡å¼‚å¸¸å…³è”..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨JSON_TABLEæŸ¥è¯¢å¼‚å¸¸ç”ŸåŒ–æŒ‡æ ‡
                            query = """
                            SELECT v.ENTITY_NAME as patient,
                                   i.é¡¹ç›® as indicator,
                                   i.ç»“æœ as value,
                                   i.å•ä½ as unit,
                                   i.å‚è€ƒèŒƒå›´ as reference
                            FROM MEDICAL_ENTITIES v,
                                 JSON_TABLE(v.ENTITY_VALUE, '$.ç”ŸåŒ–æŒ‡æ ‡[*]'
                                     COLUMNS (
                                         é¡¹ç›® VARCHAR2(100) PATH '$.é¡¹ç›®',
                                         ç»“æœ VARCHAR2(100) PATH '$.ç»“æœ',
                                         å•ä½ VARCHAR2(100) PATH '$.å•ä½',
                                         å‚è€ƒèŒƒå›´ VARCHAR2(100) PATH '$.å‚è€ƒèŒƒå›´'
                                     )
                                 ) i
                            WHERE v.ENTITY_TYPE = 'æ‚£è€…'
                            AND v.ENTITY_NAME = :patient_name
                            AND i.å‚è€ƒèŒƒå›´ = 'å¼‚å¸¸'
                            """
                            results = graph_store.execute_sql(query, {"patient_name": patient_name})
                            if results:
                                # æ„å»ºç”¨äºåˆ†æçš„æ–‡æœ¬
                                analysis_text = []
                                analysis_text.append(f"æ‚£è€… {patient_name} çš„å¼‚å¸¸ç”ŸåŒ–æŒ‡æ ‡ï¼š")
                                for result in results:
                                    analysis_text.append(f"- {result['indicator']}: {result['value']} {result['unit']}")
                                
                                # æ„å»ºæç¤ºè¯
                                prompt = f"""
                                è¯·åˆ†æä»¥ä¸‹æ‚£è€…çš„å¼‚å¸¸ï¿½ï¿½åŒ–æŒ‡ï¿½ï¿½ï¼Œç»™å‡ºä¸“ä¸šçš„åŒ»å­¦åˆ†ææ„è§ã€‚
                                è¯·åŒ…å«ä»¥ä¸‹æ–¹é¢ï¼š
                                1. å¼‚å¸¸æŒ‡æ ‡çš„ä¸´åºŠæ„ä¹‰
                                2. å¯èƒ½çš„ç—…ç†ç”Ÿç†æœºåˆ¶
                                3. éœ€è¦å…³æ³¨çš„å¥åº·é£é™©
                                4. å»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥çš„é¡¹ç›®
                                5. ç”Ÿæ´»æ–¹å¼å»ºè®®

                                {chr(10).join(analysis_text)}

                                è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ã€‚
                                """

                                try:
                                    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
                                    client = OpenAI(
                                        api_key=os.getenv("OPENAI_API_KEY"),
                                        base_url=os.getenv("OPENAI_API_BASE")
                                    )
                                    
                                    # è°ƒç”¨OpenAI APIè¿›è¡Œåˆ†æ
                                    response = client.chat.completions.create(
                                        model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
                                        messages=[
                                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸´åºŠåŒ»ç”Ÿï¼Œæ“…é•¿è§£è¯»å„ç§ç”ŸåŒ–æŒ‡æ ‡ã€‚"},
                                            {"role": "user", "content": prompt}
                                        ],
                                        temperature=0.7
                                    )
                                    
                                    # æ˜¾ç¤ºåˆ†æç»“æœ
                                    analysis = response.choices[0].message.content
                                    st.success(f"æ‰¾åˆ° {len(results)} ä¸ªå¼‚å¸¸æŒ‡æ ‡")
                                    st.markdown(analysis)
                                except Exception as e:
                                    st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                            else:
                                st.info("æœªæ‰¾åˆ°å¼‚å¸¸ç”ŸåŒ–æŒ‡æ ‡è®°å½•")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        
        elif query_type == "æ‚£è€…è¯Šæ–­å…³ç³»ç½‘ç»œ":
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æè¯Šæ–­å…³ç³»ç½‘ç»œ..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨JSON_TABLEä»å®ä½“è¡¨ä¸­æå–è¯Šæ–­ä¿¡æ¯
                            query = """
                            WITH DIAGNOSES AS (
                                SELECT 
                                    e.ENTITY_NAME as patient_name,
                                    d.ç±»å‹ as diagnosis_type,
                                    d.å†…å®¹ as diagnosis
                                FROM MEDICAL_ENTITIES e,
                                     JSON_TABLE(e.ENTITY_VALUE, '$.ä¸»è¯‰ä¸è¯Šæ–­[*]'
                                         COLUMNS (
                                             ç±»å‹ VARCHAR2(100) PATH '$.ç±»å‹',
                                             å†…å®¹ VARCHAR2(1000) PATH '$.å†…å®¹'
                                         )
                                     ) d
                                WHERE e.ENTITY_TYPE = 'æ‚£è€…'
                                AND d.ç±»å‹ IN ('å…¥é™¢è¯Šæ–­', 'å‡ºé™¢è¯Šæ–­')
                            )
                            SELECT DISTINCT 
                                d1.patient_name AS patient1,
                                d2.patient_name AS patient2,
                                d1.diagnosis_type AS diagnosis_type,
                                d1.diagnosis AS diagnosis_value
                            FROM DIAGNOSES d1
                            JOIN DIAGNOSES d2 ON d1.diagnosis = d2.diagnosis 
                                AND d1.diagnosis_type = d2.diagnosis_type
                                AND d1.patient_name < d2.patient_name
                            ORDER BY d1.patient_name, d2.patient_name, d1.diagnosis_type
                            """
                            logger.info("æ‰§è¡ŒSQLæŸ¥è¯¢: %s", query)
                            results = graph_store.execute_sql(query)
                            logger.info("æŸ¥è¯¢ç»“æœ: %r", results)
                            
                            if results:
                                # åˆ›å»ºç½‘ç»œå›¾
                                net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="black")
                                
                                # è®¾ç½®ç‰©ç†å¸ƒå±€é€‰é¡¹
                                net.set_options("""
                                {
                                    "nodes": {
                                        "font": {
                                            "size": 16,
                                            "face": "Microsoft YaHei"
                                        }
                                    },
                                    "edges": {
                                        "color": {
                                            "color": "#666666",
                                            "highlight": "#000000"
                                        },
                                        "font": {
                                            "size": 12,
                                            "face": "Microsoft YaHei"
                                        }
                                    },
                                    "physics": {
                                        "enabled": true,
                                        "solver": "forceAtlas2Based",
                                        "forceAtlas2Based": {
                                            "gravitationalConstant": -50,
                                            "centralGravity": 0.01,
                                            "springLength": 200,
                                            "springConstant": 0.08,
                                            "damping": 0.4,
                                            "avoidOverlap": 0.5
                                        }
                                    }
                                }
                                """)
                                
                                # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
                                nodes = set()
                                diagnosis_data = []
                                
                                for result in results:
                                    patient1 = result['patient1']
                                    patient2 = result['patient2']
                                    diagnosis_type = result['diagnosis_type']
                                    diagnosis_value = result['diagnosis_value']
                                    
                                    # æ”¶é›†è¯Šæ–­æ•°æ®ç”¨äºåˆ†æ
                                    diagnosis_data.append({
                                        'patient1': patient1,
                                        'patient2': patient2,
                                        'diagnosis_type': diagnosis_type,
                                        'diagnosis': diagnosis_value
                                    })
                                    
                                    if patient1 not in nodes:
                                        net.add_node(patient1, label=patient1, color='#add8e6', size=30)
                                        nodes.add(patient1)
                                    if patient2 not in nodes:
                                        net.add_node(patient2, label=patient2, color='#add8e6', size=30)
                                        nodes.add(patient2)
                                        
                                    net.add_edge(patient1, patient2, 
                                               title=f"{diagnosis_type}: {diagnosis_value}",
                                               label=diagnosis_value)
                                
                                # ä¿å­˜å¹¶æ˜¾ç¤ºç½‘ç»œå›¾
                                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.html', encoding='utf-8') as f:
                                    net.save_graph(f.name)
                                    with open(f.name, 'r', encoding='utf-8') as f:
                                        html_content = f.read()
                                    components.html(html_content, height=600)
                                    os.unlink(f.name)
                                
                                # ä½¿ç”¨å¤§æ¨¡å‹åˆ†æè¯Šæ–­å…³ç³»
                                if diagnosis_data:
                                    with st.spinner("æ­£åœ¨åˆ†æè¯Šæ–­å…³ç³»..."):
                                        # è·å–æ¯ä¸ªæ‚£è€…çš„å®Œæ•´è¯Šæ–­ä¿¡æ¯
                                        patient_diagnoses_query = """
                                        SELECT 
                                            e.ENTITY_NAME as patient_name,
                                            JSON_QUERY(e.ENTITY_VALUE, '$.ä¸»è¯‰ä¸è¯Šæ–­') as diagnoses
                                        FROM MEDICAL_ENTITIES e
                                        WHERE e.ENTITY_TYPE = 'æ‚£è€…'
                                        """
                                        patient_diagnoses_results = graph_store.execute_sql(patient_diagnoses_query)
                                        
                                        # æ„å»ºåˆ†ææ–‡æœ¬
                                        analysis_text = []
                                        for result in patient_diagnoses_results:
                                            patient_name = result['patient_name']
                                            diagnoses = json.loads(result['diagnoses'])
                                            
                                            analysis_text.append(f"\næ‚£è€… {patient_name}:")
                                            admission_diagnoses = []
                                            discharge_diagnoses = []
                                            
                                            for diag in diagnoses:
                                                if diag['ç±»å‹'] == 'å…¥é™¢è¯Šæ–­':
                                                    admission_diagnoses.append(diag['å†…å®¹'])
                                                elif diag['ç±»å‹'] == 'å‡ºé™¢è¯Šæ–­':
                                                    discharge_diagnoses.append(diag['å†…å®¹'])
                                            
                                            if admission_diagnoses:
                                                analysis_text.append("å…¥é™¢è¯Šæ–­ï¼š")
                                                for diag in admission_diagnoses:
                                                    analysis_text.append(f"- {diag}")
                                            
                                            if discharge_diagnoses:
                                                analysis_text.append("å‡ºé™¢è¯Šæ–­ï¼š")
                                                for diag in discharge_diagnoses:
                                                    analysis_text.append(f"- {diag}")
                                        
                                        # æ„å»ºæç¤ºè¯
                                        prompt = f"""
                                        è¯·åˆ†æä»¥ä¸‹æ‚£è€…ç¾¤ä½“çš„è¯Šæ–­å…³ç³»ç½‘ç»œï¼Œç»™å‡ºä¸“ä¸šçš„åŒ»å­¦åˆ†ææ„è§ã€‚
                                        è¯·åŒ…å«ä»¥ä¸‹æ–¹é¢ï¼š
                                        1. æ‚£è€…ç¾¤ä½“çš„ä¸»è¦è¯Šæ–­ç±»å‹åˆ†å¸ƒ
                                        2. å…¥é™¢è¯Šæ–­å’Œå‡ºé™¢è¯Šæ–­çš„å˜åŒ–åˆ†æ
                                        3. è¯Šæ–­ä¹‹é—´çš„å…³è”æ€§åˆ†æ
                                        4. å¯èƒ½çš„æ²»ç–—è·¯å¾„å’Œæ•ˆæœåˆ†æ
                                        5. å¯¹ä¸´åºŠè¯Šç–—çš„å»ºè®®

                                        æ‚£è€…è¯Šæ–­æ•°æ®ï¼š
                                        {chr(10).join(analysis_text)}

                                        è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ã€‚
                                        """

                                        try:
                                            # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
                                            client = OpenAI(
                                                api_key=os.getenv("OPENAI_API_KEY"),
                                                base_url=os.getenv("OPENAI_API_BASE")
                                            )
                                            
                                            # è°ƒç”¨OpenAI APIè¿›è¡Œåˆ†æ
                                            response = client.chat.completions.create(
                                                model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
                                                messages=[
                                                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸´åºŠåŒ»ç”Ÿï¼Œæ“…é•¿åˆ†ææ‚£è€…è¯Šæ–­å…³ç³»å’Œæ²»ç–—è·¯å¾„ã€‚"},
                                                    {"role": "user", "content": prompt}
                                                ],
                                                temperature=0.7
                                            )
                                            
                                            # æ˜¾ç¤ºåˆ†æç»“æœ
                                            st.write("### è¯Šæ–­å…³ç³»ç½‘ç»œåˆ†æ")
                                            st.markdown(response.choices[0].message.content)
                                        except Exception as e:
                                            logger.error("å¤§æ¨¡å‹åˆ†æå¤±è´¥: %s", str(e))
                                            st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                                else:
                                    st.info("æœªæ‰¾åˆ°è¶³å¤Ÿçš„è¯Šæ–­æ•°æ®è¿›è¡Œåˆ†æ")
                            else:
                                st.info("æœªæ‰¾åˆ°è¯Šæ–­å…³ç³»")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        
        elif query_type == "æ‚£è€…ç”¨è¯å…³è”åˆ†æ":
            patient_name = st.selectbox("é€‰æ‹©æ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æç”¨è¯å…³è”..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨JSON_TABLEä»å®ä½“è¡¨ä¸­æå–ç”¨è¯ä¿¡æ¯
                            query = """
                            WITH MEDICATIONS AS (
                                SELECT 
                                    e.ENTITY_NAME as patient_name,
                                    m.è¯å“ as medication,
                                    m.ç”¨æ³• as usage,
                                    m.å‰‚é‡ as dosage
                                FROM MEDICAL_ENTITIES e,
                                     JSON_TABLE(e.ENTITY_VALUE, '$.ç”¨è¯è®°å½•[*]'
                                         COLUMNS (
                                             è¯å“ VARCHAR2(100) PATH '$.è¯å“',
                                             ç”¨æ³• VARCHAR2(100) PATH '$.ç”¨æ³•',
                                             å‰‚é‡ VARCHAR2(100) PATH '$.å‰‚é‡'
                                         )
                                     ) m
                                WHERE e.ENTITY_TYPE = 'æ‚£è€…'
                            )
                            SELECT DISTINCT 
                                m1.patient_name AS patient1,
                                m2.patient_name AS patient2,
                                m1.medication AS medication_name,
                                m1.usage AS medication_usage,
                                m1.dosage AS medication_dosage
                            FROM MEDICATIONS m1
                            JOIN MEDICATIONS m2 ON m1.medication = m2.medication
                                AND m1.patient_name = :patient_name
                                AND m1.patient_name != m2.patient_name
                            ORDER BY m1.patient_name, m2.patient_name, m1.medication
                            """
                            results = graph_store.execute_sql(query, {"patient_name": patient_name})
                            
                            if results:
                                st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç”¨è¯å…³è”")
                                
                                # åˆ›å»ºç½‘ç»œå›¾
                                net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="black")
                                
                                # è®¾ç½®ç‰©ç†å¸ƒå±€é€‰é¡¹
                                net.set_options("""
                                {
                                    "nodes": {
                                        "font": {
                                            "size": 16,
                                            "face": "Microsoft YaHei"
                                        }
                                    },
                                    "edges": {
                                        "color": {
                                            "color": "#666666",
                                            "highlight": "#000000"
                                        },
                                        "font": {
                                            "size": 12,
                                            "face": "Microsoft YaHei"
                                        }
                                    },
                                    "physics": {
                                        "enabled": true,
                                        "solver": "forceAtlas2Based",
                                        "forceAtlas2Based": {
                                            "gravitationalConstant": -50,
                                            "centralGravity": 0.01,
                                            "springLength": 200,
                                            "springConstant": 0.08,
                                            "damping": 0.4,
                                            "avoidOverlap": 0.5
                                        }
                                    }
                                }
                                """)
                                
                                # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
                                nodes = set()
                                medication_data = []
                                
                                for result in results:
                                    patient1 = result['patient1']
                                    patient2 = result['patient2']
                                    medication = result['medication_name']
                                    usage = result.get('medication_usage', '')
                                    dosage = result.get('medication_dosage', '')
                                    
                                    # æ”¶é›†ç”¨è¯æ•°æ®ç”¨äºåˆ†æ
                                    medication_data.append({
                                        'patient1': patient1,
                                        'patient2': patient2,
                                        'medication': medication,
                                        'usage': usage,
                                        'dosage': dosage
                                    })
                                    
                                    if patient1 not in nodes:
                                        net.add_node(patient1, label=patient1, color='#add8e6', size=30)
                                        nodes.add(patient1)
                                    if patient2 not in nodes:
                                        net.add_node(patient2, label=patient2, color='#add8e6', size=30)
                                        nodes.add(patient2)
                                        
                                    edge_label = f"{medication}"
                                    if usage and dosage:
                                        edge_title = f"{medication}\nç”¨æ³•ï¼š{usage}\nå‰‚é‡ï¼š{dosage}"
                                    else:
                                        edge_title = medication
                                        
                                    net.add_edge(patient1, patient2, 
                                               title=edge_title,
                                               label=edge_label)
                                
                                # ä¿å­˜å¹¶æ˜¾ç¤ºç½‘ç»œå›¾
                                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.html', encoding='utf-8') as f:
                                    net.save_graph(f.name)
                                    with open(f.name, 'r', encoding='utf-8') as f:
                                        html_content = f.read()
                                    components.html(html_content, height=600)
                                    os.unlink(f.name)
                                
                                # ä½¿ç”¨å¤§æ¨¡å‹åˆ†æç”¨è¯å…³è”
                                if medication_data:
                                    with st.spinner("æ­£åœ¨åˆ†æç”¨è¯å…³è”..."):
                                        # è·å–æ¯ä¸ªæ‚£è€…çš„å®Œæ•´ç”¨è¯ä¿¡æ¯
                                        patient_medications_query = """
                                        SELECT 
                                            e.ENTITY_NAME as patient_name,
                                            JSON_QUERY(e.ENTITY_VALUE, '$.ç”¨è¯è®°å½•') as medications
                                        FROM MEDICAL_ENTITIES e
                                        WHERE e.ENTITY_TYPE = 'æ‚£è€…'
                                        """
                                        patient_medications_results = graph_store.execute_sql(patient_medications_query)
                                        
                                        # æ„å»ºåˆ†ææ–‡æœ¬
                                        analysis_text = []
                                        for result in patient_medications_results:
                                            patient_name = result['patient_name']
                                            medications = json.loads(result['medications'] or '[]')
                                            
                                            if medications:
                                                analysis_text.append(f"\næ‚£è€… {patient_name} çš„ç”¨è¯è®°å½•ï¼š")
                                                for med in medications:
                                                    med_text = f"- {med['è¯å“']}"
                                                    if 'ç”¨æ³•' in med:
                                                        med_text += f"ï¼ˆ{med['ç”¨æ³•']}"
                                                        if 'å‰‚é‡' in med:
                                                            med_text += f"ï¼Œ{med['å‰‚é‡']}"
                                                        med_text += "ï¼‰"
                                                    analysis_text.append(med_text)
                                        
                                        # æ„å»ºæç¤ºè¯
                                        prompt = f"""
                                        è¯·åˆ†æä»¥ä¸‹æ‚£è€…ç¾¤ä½“çš„ç”¨è¯å…³è”æƒ…å†µï¼Œç»™å‡ºä¸“ä¸šçš„åŒ»å­¦åˆ†ææ„è§ã€‚
                                        è¯·åŒ…å«ä»¥ä¸‹æ–¹é¢ï¼š
                                        1. æ‚£è€…ç¾¤ä½“çš„ä¸»è¦ç”¨è¯ç±»å‹åˆ†å¸ƒ
                                        2. è¯ç‰©ä¹‹é—´çš„ç›¸äº’ä½œç”¨åˆ†æ
                                        3. ç”¨è¯æ–¹æ¡ˆçš„åˆç†æ€§åˆ†æ
                                        4. å¯èƒ½çš„ç”¨è¯é£é™©æç¤º
                                        5. å¯¹ä¸´åºŠç”¨è¯çš„å»ºè®®

                                        æ‚£è€…ç”¨è¯æ•°æ®ï¼š
                                        {chr(10).join(analysis_text)}

                                        è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ã€‚
                                        """

                                        try:
                                            # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
                                            client = OpenAI(
                                                api_key=os.getenv("OPENAI_API_KEY"),
                                                base_url=os.getenv("OPENAI_API_BASE")
                                            )
                                            
                                            # è°ƒç”¨OpenAI APIè¿›è¡Œåˆ†æ
                                            response = client.chat.completions.create(
                                                model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
                                                messages=[
                                                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸´åºŠè¯å¸ˆï¼Œæ“…é•¿åˆ†ææ‚£è€…ç”¨è¯æ–¹æ¡ˆå’Œè¯ç‰©ç›¸äº’ä½œç”¨ã€‚"},
                                                    {"role": "user", "content": prompt}
                                                ],
                                                temperature=0.7
                                            )
                                            
                                            # æ˜¾ç¤ºåˆ†æç»“æœ
                                            st.write("### ç”¨è¯å…³è”åˆ†æ")
                                            st.markdown(response.choices[0].message.content)
                                        except Exception as e:
                                            logger.error("å¤§æ¨¡å‹åˆ†æå¤±è´¥: %s", str(e))
                                            st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                                else:
                                    st.info("æœªæ‰¾åˆ°è¶³å¤Ÿçš„ç”¨è¯æ•°æ®è¿›è¡Œåˆ†æ")
                            else:
                                st.info("æœªæ‰¾åˆ°ç”¨è¯å…³è”")
                    except Exception as e:
                        logger.error("åˆ†æå¤±è´¥: %s", str(e))
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        
        elif query_type == "æ‚£è€…æ²»ç–—æ–¹æ¡ˆå¯¹æ¯”":
            patient1 = st.selectbox("é€‰æ‹©ç¬¬ä¸€ä¸ªæ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            patient2 = st.selectbox("é€‰æ‹©ç¬¬äºŒä¸ªæ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if patient1 != patient2 and st.button("å¯¹æ¯”"):
                with st.spinner("æ­£åœ¨å¯¹æ¯”æ²»ç–—æ–¹æ¡ˆ..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨PGQLæŸ¥è¯¢æ²»ç–—æ–¹æ¡ˆ
                            query = """
                            SELECT 
                                v.entity_name AS patient,
                                e.relation_type AS treatment_type,
                                t.entity_value AS treatment_detail
                            MATCH (v) -[e]-> (t)
                            WHERE v.entity_type = 'PATIENT'
                              AND v.entity_name IN (:patient1, :patient2)
                              AND e.relation_type = 'è¯Šç–—ç»è¿‡'
                            """
                            results = graph_store.execute_pgql(query, {
                                "patient1": patient1,
                                "patient2": patient2
                            })
                            
                            if results:
                                # æŒ‰æ‚£è€…åˆ†ç»„æ²»ç–—æ–¹æ¡ˆ
                                treatments = {}
                                for result in results:
                                    patient = result['patient']
                                    if patient not in treatments:
                                        treatments[patient] = []
                                    treatments[patient].append({
                                        'treatment': result['treatment_detail'],
                                        'effect': result['effect']
                                    })
                                
                                # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.subheader(patient1)
                                    if patient1 in treatments:
                                        for t in treatments[patient1]:
                                            st.write(f"- {t['treatment']} (æ•ˆæœ: {t['effect']})")
                                    else:
                                        st.info("æ— æ²»ç–—æ–¹æ¡ˆè®°å½•")
                                        
                                with col2:
                                    st.subheader(patient2)
                                    if patient2 in treatments:
                                        for t in treatments[patient2]:
                                            st.write(f"- {t['treatment']} (æ•ˆæœ: {t['effect']})")
                                    else:
                                        st.info("æ— æ²»ç–—æ–¹æ¡ˆè®°å½•")
                            else:
                                st.info("æœªæ‰¾åˆ°æ²»ç–—æ–¹æ¡ˆè®°å½•")
                    except Exception as e:
                        st.error(f"å¯¹æ¯”å¤±è´¥: {str(e)}")
    
    with tab2:
        st.subheader("è‡ªå®šä¹‰PGQLæŸ¥è¯¢")
        st.markdown("""
        æ‚¨å¯ä»¥è¾“å…¥è‡ªå®šä¹‰çš„PGQLæŸ¥è¯¢è¯­å¥ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š
        
        1. æŸ¥è¯¢æ‚£è€…çš„æ‰€æœ‰ç—‡çŠ¶ï¼š
        ```sql
        SELECT *
        FROM GRAPH_TABLE ( MEDICAL_KG
            MATCH (v) -[e]-> (s)
            WHERE v.ENTITY_TYPE = 'æ‚£è€…'
            AND e.RELATION_TYPE = 'ç°ç—…å²'
            AND s.ENTITY_TYPE = 'ç°ç—…å²'
            COLUMNS (
                v.ENTITY_NAME AS patient_name,
                s.ENTITY_VALUE AS symptom
            )
        )
        ```
        
        2. æŸ¥è¯¢ç‰¹å®šç—‡çŠ¶çš„æ‰€æœ‰æ‚£è€…ï¼š
        ```sql
        SELECT *
        FROM GRAPH_TABLE ( MEDICAL_KG
            MATCH (v) -[e]-> (s)
            WHERE v.ENTITY_TYPE = 'æ‚£è€…'
            AND e.RELATION_TYPE = 'ç°ç—…å²'
            AND s.ENTITY_TYPE = 'ç°ç—…å²'
            AND JSON_EXISTS(s.ENTITY_VALUE, '$.ç—‡çŠ¶[*]?(@=="å‘çƒ­")')
            COLUMNS (
                v.ENTITY_NAME AS patient_name
            )
        )
        ```
        
        3. æŸ¥è¯¢æ‚£è€…çš„å¼‚å¸¸ç”ŸåŒ–æŒ‡æ ‡ï¼š
        ```sql
        SELECT *
        FROM GRAPH_TABLE ( MEDICAL_KG
            MATCH (v) -[e]-> (s)
            WHERE v.ENTITY_TYPE = 'æ‚£è€…'
            AND JSON_EXISTS(v.ENTITY_VALUE, '$.ç”ŸåŒ–æŒ‡æ ‡[*]?(@.å‚è€ƒèŒƒå›´=="å¼‚å¸¸")')
            COLUMNS (
                v.ENTITY_NAME AS patient_name,
                JSON_QUERY(v.ENTITY_VALUE, '$.ç”ŸåŒ–æŒ‡æ ‡[*].é¡¹ç›®' WITH ARRAY WRAPPER) AS indicator_name,
                JSON_QUERY(v.ENTITY_VALUE, '$.ç”ŸåŒ–æŒ‡æ ‡[*].ç»“æœ' WITH ARRAY WRAPPER) AS value,
                JSON_QUERY(v.ENTITY_VALUE, '$.ç”ŸåŒ–æŒ‡æ ‡[*].å•ä½' WITH ARRAY WRAPPER) AS unit,
                JSON_QUERY(v.ENTITY_VALUE, '$.ç”ŸåŒ–æŒ‡æ ‡[*].å‚è€ƒèŒƒå›´' WITH ARRAY WRAPPER) AS reference_range
            )
        )
        ```
        """)
        
        query = st.text_area("è¾“å…¥PGQLæŸ¥è¯¢è¯­å¥", height=150)
        params = st.text_input("è¾“å…¥å‚æ•°ï¼ˆJSONæ ¼å¼ï¼Œå¯é€‰ï¼‰", "{}")
        
        if st.button("æ‰§è¡ŒæŸ¥è¯¢"):
            if query:
                with st.spinner("æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢..."):
                    try:
                        # è§£æå‚æ•°
                        try:
                            params = json.loads(params) if params.strip() else {}
                        except json.JSONDecodeError:
                            st.error("å‚æ•°æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æœ‰æ•ˆçš„JSONæ ¼å¼")
                            return
                        
                        # æ‰§è¡ŒæŸ¥è¯¢
                        with OracleGraphStore() as graph_store:
                            results = graph_store.execute_pgql(query, params)
                            if results:
                                st.success(f"æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
                                st.json(results)
                            else:
                                st.info("æŸ¥è¯¢æœªè¿”å›ä»»ä½•ç»“æœ")
                    except Exception as e:
                        st.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
            else:
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢è¯­å¥")

def analyze_medication():
    query = """
    WITH PATIENT_MEDS AS (
        SELECT 
            e.entity_name as patient_name,
            j.content as treatment_content
        FROM MEDICAL_ENTITIES e,
        JSON_TABLE(e.entity_value, '$.è¯Šç–—ç»è¿‡[*]' 
            COLUMNS (
                content PATH '$.å†…å®¹'
            )
        ) j
        WHERE e.entity_type = 'æ‚£è€…'
        AND REGEXP_LIKE(j.content, 'ç»™äºˆ|ä½¿ç”¨|æœç”¨|æ²»ç–—|ç”¨è¯', 'i')
    )
    SELECT 
        patient_name,
        treatment_content,
        COUNT(*) OVER (PARTITION BY treatment_content) as usage_count
    FROM PATIENT_MEDS
    ORDER BY usage_count DESC, patient_name
    """
    
    results = execute_sql(query)
    
    if not results:
        st.warning("æœªæ‰¾åˆ°ç”¨è¯è®°å½•æ•°æ®ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š\n1. æ•°æ®ä¸­æ²¡æœ‰è®°å½•ç”¨è¯ä¿¡æ¯\n2. ç”¨è¯ä¿¡æ¯çš„è®°å½•æ ¼å¼éœ€è¦æ ‡å‡†åŒ–")
        return
        
    # å±•ç¤ºç”¨è¯åˆ†æç»“æœ
    st.subheader("æ‚£è€…ç”¨è¯åˆ†æ")
    
    # åˆ›å»ºæ•°æ®è¡¨æ ¼
    df = pd.DataFrame(results, columns=['æ‚£è€…å§“å', 'ç”¨è¯è®°å½•', 'ä½¿ç”¨é¢‘æ¬¡'])
    
    # æ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨æ ¼
    st.dataframe(df)
    
    # ä½¿ç”¨plotlyåˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    fig = px.bar(df, 
                 x='ç”¨è¯è®°å½•', 
                 y='ä½¿ç”¨é¢‘æ¬¡',
                 title='ç”¨è¯é¢‘æ¬¡åˆ†å¸ƒ',
                 color='ä½¿ç”¨é¢‘æ¬¡',
                 hover_data=['æ‚£è€…å§“å'])
    
    fig.update_layout(
        xaxis_title="ç”¨è¯è®°å½•",
        yaxis_title="ä½¿ç”¨é¢‘æ¬¡",
        showlegend=True
    )
    
    st.plotly_chart(fig)
    
    # æ·»åŠ AIåˆ†æè§è§£
    st.subheader("AIåˆ†æè§è§£")
    
    # è®¡ç®—ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    total_patients = len(df['æ‚£è€…å§“å'].unique())
    total_medications = len(df['ç”¨è¯è®°å½•'].unique())
    most_common_med = df.loc[df['ä½¿ç”¨é¢‘æ¬¡'].idxmax(), 'ç”¨è¯è®°å½•']
    
    analysis_text = f"""
    æ ¹æ®æ•°æ®åˆ†æï¼Œå‘ç°ä»¥ä¸‹ä¸»è¦ç‰¹ç‚¹ï¼š
    
    1. å…±æœ‰{total_patients}ä½æ‚£è€…çš„ç”¨è¯è®°å½•
    2. è®°å½•ä¸­åŒ…å«{total_medications}ç§ä¸åŒçš„æ²»ç–—æ–¹æ¡ˆ
    3. æœ€å¸¸è§çš„æ²»ç–—æ–¹æ¡ˆæ˜¯"{most_common_med}"
    
    å»ºè®®ï¼š
    - è¿›ä¸€æ­¥ç»†åŒ–ç”¨è¯è®°å½•çš„åˆ†ç±»
    - æ ‡å‡†åŒ–ç”¨è¯è®°å½•çš„æ ¼å¼
    - æ·»åŠ è¯ç‰©å‰‚é‡ã€ç”¨è¯æ—¶é—´ç­‰è¯¦ç»†ä¿¡æ¯
    """
    
    st.markdown(analysis_text)

def main():
    st.title("åŸºäºOracle 23AIç”µå­ç—…å†æ£€ç´¢ç³»ç»Ÿ", anchor=False)
    st.markdown("<div style='text-align: right'>Developed by Huaiyuan Tan</div>", unsafe_allow_html=True)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    # åˆ›å»ºä¾§è¾¹æ èœå•
    menu = st.sidebar.selectbox(
        "åŠŸèƒ½èœå•",
        ["æ–‡æ¡£ç®¡ç†", "å›¾æ•°æ®æ£€ç´¢", "å‘é‡æ£€ç´¢", "ç»“æ„åŒ–æ£€ç´¢", "å±æ€§å›¾æ£€ç´¢"]
    )
    
    if menu == "æ–‡æ¡£ç®¡ç†":
        display_document_management()
    elif menu == "å›¾æ•°æ®æ£€ç´¢":
        st.header("å›¾æ•°æ®æ£€ç´¢")
        # æ˜¾ç¤ºå·²è§£æçš„æ–‡æ¡£
        display_parsed_documents()
        
        # æ·»åŠ æœç´¢æ¡†
        query = st.text_input("è¯·è¾“å…¥æœç´¢å†…å®¹ï¼ˆæ”¯æŒæŒ‰æ‚£è€…å§“åã€ç—‡çŠ¶ã€è¯Šæ–­ç­‰æœç´¢ï¼‰")
        if query:
            results = search_graph_data(query)
            display_graph_results(results, query)
    elif menu == "å‘é‡æ£€ç´¢":
        display_vector_search()
    elif menu == "å±æ€§å›¾æ£€ç´¢":
        display_property_graph_search()
    else:  # ç»“æ„åŒ–æ£€ç´¢
        display_structured_search()

if __name__ == "__main__":
    main()

