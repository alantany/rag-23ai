"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - Oracle Vector Storeç‰ˆæœ¬
"""

import streamlit as st
from utils.oracle_vector_store import OracleVectorStore
from sentence_transformers import SentenceTransformer
import openai
import PyPDF2
import docx
from io import BytesIO
import os
from dotenv import load_dotenv
import logging
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®oracle_vector_storeçš„æ—¥å¿—çº§åˆ«
logging.getLogger('utils.oracle_vector_store').setLevel(logging.INFO)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ–OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

# åˆå§‹åŒ–å‘é‡æ¨¡å‹
embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - by Huaiyuan Tan",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# é¡µé¢æ ·å¼è®¾ç½®
st.markdown("<h6 style='text-align: right; color: gray;'>å¼€å‘è€…: Huaiyuan Tan</h6>", unsafe_allow_html=True)
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
    header {visibility: hidden;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

def read_file_content(uploaded_file) -> str:
    """è¯»å–ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹"""
    file_type = uploaded_file.name.split('.')[-1].lower()
    content = ""
    
    try:
        if file_type == 'txt':
            content = uploaded_file.getvalue().decode('utf-8')
        elif file_type == 'pdf':
            pdf_reader = PyPDF2.PdfReader(BytesIO(uploaded_file.getvalue()))
            for page in pdf_reader.pages:
                content += page.extract_text() + "\n"
        elif file_type == 'docx':
            doc = docx.Document(BytesIO(uploaded_file.getvalue()))
            for para in doc.paragraphs:
                content += para.text + "\n"
        return content
    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
        return ""

def process_document(file_name: str, content: str) -> tuple:
    """å¤„ç†æ–‡æ¡£å†…å®¹ï¼Œè¿”å›æ–‡æ¡£å—å’Œå‘é‡"""
    # ä¸å†åˆ†å‰²æ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªå†…å®¹
    chunks = [content]  # æ”¹ä¸ºå•ä¸ªå—
    
    # ç”Ÿæˆå‘é‡åµŒå…¥
    vectors = embeddings_model.encode(chunks)
    
    # å‡†å¤‡æ–‡æ¡£æ•°æ®
    documents = [{
        'file_path': file_name,
        'content': content,
        'metadata': {'chunk_id': 0}  # åªæœ‰ä¸€ä¸ªå—ï¼Œidä¸º0
    }]
    
    return vectors, documents

def get_existing_documents():
    """è·å–å·²å­˜åœ¨çš„æ–‡æ¡£åˆ—è¡¨"""
    with OracleVectorStore() as vector_store:
        try:
            documents = vector_store.list_documents()
            logger.info(f"è·å–åˆ°å·²å­˜åœ¨æ–‡æ¡£: {len(documents)}ä¸ª")
            return documents
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨é”™è¯¯: {str(e)}")
            return []

def handle_file_upload():
    """å¤„ç†æ–‡ä»¶ä¸Šä¼ """
    # æ˜¾ç¤ºå·²æœ‰æ–‡æ¡£
    st.subheader("å·²æœ‰æ–‡æ¡£:")
    existing_docs = get_existing_documents()
    if existing_docs:
        for doc in existing_docs:
            st.text(f"ğŸ“„ {doc['file_path']} (åˆ†å—æ•°: {doc['chunk_count']})")
            logger.info(f"æ˜¾ç¤ºæ–‡æ¡£: {doc}")
    else:
        st.text("æš‚æ— æ–‡æ¡£")
        logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
    
    st.subheader("ä¸Šä¼ æ–°æ–‡æ¡£:")
    uploaded_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=['txt', 'pdf', 'docx'], key="file_uploader")
    
    # ä½¿ç”¨session_stateæ¥è·Ÿè¸ªä¸Šä¼ çŠ¶æ€
    if "file_processed" not in st.session_state:
        st.session_state.file_processed = False
        
    if uploaded_file and not st.session_state.file_processed:
        logger.info(f"æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ : {uploaded_file.name}")
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
            content = read_file_content(uploaded_file)
            if content:
                vectors, documents = process_document(uploaded_file.name, content)
                
                # å­˜å‚¨åˆ°Oracle
                with OracleVectorStore() as vector_store:
                    try:
                        vector_store.init_schema()
                        # å…ˆåˆ é™¤åŒåæ–‡ä»¶çš„æ‰€æœ‰è®°å½•
                        deleted = vector_store.delete_document(uploaded_file.name)
                        if deleted:
                            logger.info(f"åˆ é™¤å·²å­˜åœ¨çš„æ–‡æ¡£: {uploaded_file.name}")
                        # æ·»åŠ æ–°çš„æ–‡æ¡£
                        vector_store.add_vectors(vectors, documents)
                        logger.info(f"æˆåŠŸæ·»åŠ æ–‡æ¡£: {uploaded_file.name}")
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} å¤„ç†å®Œæˆ!")
                        st.session_state.file_processed = True
                        st.rerun()
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£å¤„ç†é”™è¯¯: {str(e)}", exc_info=True)
                        st.error(f"æ–‡æ¡£å¤„ç†é”™è¯¯: {str(e)}")
    
    # å¦‚æœæ–‡ä»¶å·²å¤„ç†ï¼Œé‡ç½®çŠ¶æ€
    if not uploaded_file and st.session_state.file_processed:
        st.session_state.file_processed = False

def search_similar_documents(query: str, top_k: int = 3, preview_only: bool = False) -> list:
    """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
    logger.info(f"æœç´¢æ–‡æ¡£ï¼Œé—®é¢˜ï¼š{query}")
    
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_vector = embeddings_model.encode([query])[0]
    
    # åœ¨Oracleä¸­æœç´¢
    with OracleVectorStore() as vector_store:
        try:
            results = vector_store.search_vectors(
                query_vector=query_vector,
                top_k=top_k,
                preview_only=preview_only,
                similarity_threshold=0.99  # æ”¾å®½é˜ˆå€¼ï¼Œå…è®¸æ›´å¤šçš„åŒ¹é…ç»“æœ
            )
            return results
        except Exception as e:
            logger.error(f"æœç´¢é”™è¯¯: {str(e)}")
            st.error(f"æœç´¢é”™è¯¯: {str(e)}")
            return []

def generate_answer(query: str, context: str) -> str:
    """ç”ŸæˆAIå›ç­”"""
    try:
        # ä½¿ç”¨æŒ‡å®šçš„API keyå’Œbase_url
        client = openai.OpenAI(
            api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn",
            base_url="https://api.chatanywhere.tech/v1"
        )
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"},
                {"role": "user", "content": f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context}\n\né—®é¢˜: {query}"}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"

def main():
    st.title("ç”µå­ç—…å†é—®ç­”ç³»ç»Ÿ")
    
    # åˆå§‹åŒ–session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "file_processed" not in st.session_state:
        st.session_state.file_processed = False
    
    # æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
    handle_file_upload()
    
    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:"):
        # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²è®°å½•
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³å†…å®¹..."):
                # æœç´¢ç›¸å…³æ–‡æ¡£
                results = search_similar_documents(prompt, top_k=1, preview_only=False)
                
                if not results:
                    error_message = "æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£ã€‚è¯·ç¡®è®¤ï¼š\n1. æ–‡æ¡£æ˜¯å¦å·²ä¸Šä¼ \n2. æ‚£è€…å§“åæ˜¯å¦æ­£ç¡®\n3. é—®é¢˜æ˜¯å¦å‡†ç¡®"
                    st.warning(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
                    return
                
                # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£
                result = results[0]
                similarity_score = 1 - result['similarity']
                
                # æ˜¾ç¤ºåŒ¹é…çš„æ–‡æ¡£ä¿¡æ¯
                st.info(f"åŒ¹é…æ–‡æ¡£: {result['file_path']} (ç›¸ä¼¼åº¦: {similarity_score:.2%})")
                
                if similarity_score < 0.5:
                    st.warning("âš ï¸ æ³¨æ„ï¼šå½“å‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦è¾ƒä½ï¼Œå›ç­”å¯èƒ½ä¸å¤Ÿå‡†ç¡®ã€‚")
                
                # ç”ŸæˆAIå›ç­”
                with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                    answer = generate_answer(prompt, result['content'])
                    st.markdown(answer)
                    
                    # ä¿å­˜åŠ©æ‰‹å›ç­”åˆ°å†å²è®°å½•
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer
                    })

if __name__ == "__main__":
    main()

