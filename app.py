"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - Oracle Vector Storeç‰ˆæœ¬
"""

import streamlit as st
from utils.oracle_vector_store import OracleVectorStore
from utils.oracle_json_store import OracleJsonStore
from utils.medical_record_parser import MedicalRecordParser
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import shutil
from pathlib import Path
import logging
import openai
import pdfplumber
import json
import datetime
import hashlib
from typing import Dict, Any, List
from decimal import Decimal
from openai import OpenAI
from utils.oracle_graph_store import OracleGraphStore
from utils.medical_graph_parser import MedicalGraphParser
from pyvis.network import Network
import streamlit.components.v1 as components
import tempfile
import networkx as nx

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è·å–OpenAIé…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

# è®¾ç½®æ–‡æ¡£å­˜å½•
UPLOAD_DIR = Path("uploaded_documents")
UPLOAD_DIR.mkdir(exist_ok=True)

# è®¾ç½®JSONç¼“å­˜ç›®å½•
CACHE_DIR = Path("json_cache")
CACHE_DIR.mkdir(exist_ok=True)

# ä½¿ç”¨ st.cache_resource ç¼“å­˜æ¨¡å‹ï¼Œå¹¶éšè—
@st.cache_resource(show_spinner=False)
def load_embeddings_model():
    """åŠ è½½å‘é‡åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰"""
    logger.debug("åŠ è½½å‘é‡åŒ–æ¨¡å‹")
    return SentenceTransformer('all-MiniLM-L6-v2')

# åˆå§‹åŒ–æ¨¡å‹
embeddings_model = load_embeddings_model()

def get_cache_path(file_path: str) -> Path:
    """æ ¹æ®æ–‡ä»¶è·¯å¾„ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # è·å–åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    original_name = Path(file_path).stem
    return CACHE_DIR / f"{original_name}.json"

def save_to_cache(file_path: str, data: Dict) -> None:
    """ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°ç¼“å­˜"""
    cache_path = get_cache_path(file_path)
    with open(cache_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"ç¼“å­˜ä¿å­˜åˆ°: {cache_path}")

def load_from_cache(file_path: str) -> Dict:
    """ä»ç¼“å­˜åŠ è½½ç»“æ„åŒ–æ•°æ®"""
    cache_path = get_cache_path(file_path)
    if cache_path.exists():
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"ä»ç¼“å­˜åŠ è½½: {cache_path}")
        return data
    return None

def save_uploaded_file(uploaded_file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•"""
    file_path = UPLOAD_DIR / uploaded_file.name
    with open(file_path, "wb") as f:
        shutil.copyfileobj(uploaded_file, f)
    return str(file_path)

def read_file_content(file_path):
    """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨ pdfplumber å¤„ç† PDF"""
    try:
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_extension = str(file_path).lower().split('.')[-1]
        
        # PDFæ–‡ä»¶å¤„ç†
        if file_extension == 'pdf':
            with pdfplumber.open(file_path) as pdf:
                text = ''
                for page in pdf.pages:
                    text += page.extract_text() + '\n'
                return text.strip()
        
        # æ–‡æœ¬æ–‡ä»¶å¤„ç†
        else:
            encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read().strip()
                except UnicodeDecodeError:
                    continue
        
        return None
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        return None

def get_uploaded_files():
    """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
    return list(UPLOAD_DIR.glob("*.*"))

def vectorize_document(file_path):
    """å‘é‡åŒ–æ–‡æ¡£"""
    try:
        content = read_file_content(file_path)
        # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹è¿›è¡Œå‘é‡åŒ–
        vector = embeddings_model.encode([content])[0]
        documents = [{"file_path": str(file_path), "content": content}]
        
        with OracleVectorStore() as vector_store:
            vector_store.add_vectors([vector], documents)
        return True
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤±è´¥: {str(e)}")
        return False

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    try:
        with OracleJsonStore() as json_store:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_table_sql = """
            SELECT COUNT(*) as count
            FROM user_tables 
            WHERE table_name = 'DOCUMENT_JSON'
            """
            result = json_store.execute_search(check_table_sql)
            table_exists = result[0]['count'] > 0 if result else False
            
            if not table_exists:
                # åˆ›å»ºç»“æ„åŒ–æ–‡æ¡£è¡¨
                create_json_table_sql = """
                CREATE TABLE DOCUMENT_JSON (
                    id NUMBER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                    doc_info VARCHAR2(500),
                    doc_json JSON
                )
                """
                json_store.execute_sql(create_json_table_sql)
                logger.info("æˆåŠŸåˆ›å»ºDOCUMENT_JSONè¡¨")
            else:
                logger.debug("DOCUMENT_JSONè¡¨å·²å­˜åœ¨")  # æ”¹ç”¨debugçº§åˆ«çš„æ—¥å¿—

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")
        st.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {str(e)}")

class MedicalRecordParser:
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )

    def parse_medical_record(self, text: str, doc_info: str) -> Dict[str, Any]:
        """è§£æåŒ»ç–—è®°å½•æ–‡æœ¬ï¼Œç”Ÿæˆç»“æ„åŒ–JSONæ•°æ®å’ŒSQLæ’å…¥è¯­å¥"""
        try:
            prompt = f"""è¯·å°†ä»¥ä¸‹ç—…å†å†…å®¹ç»“æ„åŒ–ä¸ºJSONæ ¼å¼ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„SQLæ’å…¥è¯­å¥ã€‚

ç—…å†å†…å®¹ï¼š
{text}

è¦æ±‚ï¼š
1. JSONç»“æ„ï¼š
{{
    "æ‚£è€…å§“å": "æŸæŸï¼ˆä¿æŠ¤éšç§ï¼‰",
    "æ€§åˆ«": "æ€§åˆ«",
    "å¹´é¾„": "æ•°å­—",
    "å…¥é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "å‡ºé™¢æ—¥æœŸ": "YYYY-MM-DD",
    "ä¸»è¯‰": "ä¸»è¦ç—‡çŠ¶",
    "ç°ç—…å²": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "å…¥è¯Š": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‡ºé™¢è¯Š": ["æ–­1", "è¯Šæ–­2"],
    "ç”Ÿå‘½ä½“å¾": {{
        "ä½“æ¸©": "åŒ…å«å•ä½",
        "è¡€å‹": "åŒ…å«å•ä½"
    }},
    "ç”ŸåŒ–æŒ‡æ ‡": {{
        "æŒ‡æ ‡å": "å€¼å’Œå•ä½ï¼Œå¼‚å¸¸æ ‡â†‘â†“"
    }},
    "è¯Šç–—ç»è¿‡": "æ²»ç–—è¿‡ç¨‹æè¿°",
    "å‡ºé™¢åŒ»å˜±": ["åŒ»å˜±1", "åŒ»å˜±2"]
}}

2. SQLè¦æ±‚ï¼š
- è¡¨åï¼šDOCUMENT_JSON
- å­—æ®µï¼š(id NUMBERè‡ªå¢, doc_info VARCHAR2(500), doc_json JSON)
- doc_infoå€¼ï¼š{doc_info}
- ä½¿ç”¨JSON_OBJECTæˆ–FORMAT JSONè¯­æ³•

è¯·è¿”å›ï¼š
{{
    "structured_data": JSONæ ¼å¼çš„ç—…å†æ•°æ®,
    "sql_statement": Oracleæ’å…¥è¯­å¥
}}"""

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯åŒ»ç–—æ•°æ®ç»“æ„åŒ–ä¸“å®¶ï¼Œæ“…é•¿è§£æç—…å†æ–‡æœ¬å¹¶ç”Ÿæˆè§„èŒƒçš„JSONå’ŒSQLã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )

            # è§£æè¿”å›çš„JSON
            result = json.loads(response.choices[0].message.content)
            
            # éªŒè¯ç»“æœæ ¼å¼
            if not isinstance(result, dict) or 'structured_data' not in result or 'sql_statement' not in result:
                raise ValueError("GPTè¿”å›ï¿½ï¿½ï¿½æ•°æ®æ ¼å¼ä¸æ­£ç¡®")

            # æ·»åŠ å…ƒæ•°æ®
            if 'metadata' not in result['structured_data']:
                result['structured_data']['metadata'] = {}
            result['structured_data']['metadata'].update({
                'import_time': datetime.datetime.now().isoformat(),
                'source_type': 'text',
                'last_updated': datetime.datetime.now().isoformat()
            })

            return result

        except Exception as e:
            logger.error(f"è§£æåŒ»ç–—è®°å½•å¤±è´¥: {str(e)}")
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}

def parse_document_to_json(file):
    """è§£ææ–‡æ¡£ä¸ºJSONæ ¼å¼"""
    try:
        content = read_file_content(file)
        
        # è§£æä¸ºJSON
        parser = MedicalRecordParser()
        json_result = parser.parse_medical_record(content)
        
        # è§£æä¸ºå›¾æ•°æ®
        graph_parser = MedicalGraphParser()
        graph_result = graph_parser.parse_to_graph(content, file.name)
        
        if "error" in json_result or "error" in graph_result:
            return False
            
        # ä¿å­˜JSONç»“æœ
        with OracleJsonStore() as json_store:
            json_store.add_document(file.name, json_result)
            
        return True
    except Exception as e:
        logger.error(f"è§£ææ–‡æ¡£å¤±è´¥: {str(e)}")
        return False

def search_similar_documents(query: str, top_k: int = 3):
    """å‘é‡æœç´¢å¹¶ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£"""
    try:
        # 1. å‘é‡æœç´¢
        vector = embeddings_model.encode([query])[0]
        with OracleVectorStore() as vector_store:
            results = vector_store.search_vectors([vector], top_k=top_k)
        
        if not results:
            return []

        # 2. ä½¿ç”¨ GPT åˆ†ææœ€ç›¸å…³çš„æ–‡æ¡£
        best_match = results[0]  # å–ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£
        
        prompt = f"""
        åŸºäºä»¥ä¸‹åŒ»ç–—æ–‡æ¡£å†…å®¹ï¼Œå›ç­”é—®é¢˜ï¼š{query}

        æ–‡æ¡£å†…å®¹ï¼š
        {best_match['content']}

        è¯·æä¾›è¯¦ç»†ä¸“ä¸šåˆ†æå’Œç­”æ¡ˆã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚
        """

        # ä½¿ç”¨æ–° OpenAI API
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æåŒ»ç–—æ–‡æ¡£å¹¶ä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )

        # 3. å°† GPT åˆ†æç»“æœæ·»åŠ åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ä¸­
        best_match['gpt_analysis'] = response.choices[0].message.content
        
        # è¿”å›æ‰€æœ‰æ£€ç´¢ç»“æœï¼Œä½†åªæœ‰ç›¸å…³çš„æ–‡åŒ…å« GPT åˆ†æ
        return results

    except Exception as e:
        logger.error(f"æœç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []

def analyze_query_with_gpt(query_text):
    """ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾å¹¶ç”ŸæˆOracle 23c JSONæŸ¥è¯¢æ¡ä»¶"""
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
        
        # æä¾›å®Œæ•´çš„JSONç»“æ„ä¿¡æ¯
        json_structure = """
{
    "æ‚£è€…å§“å": "å§“å",
    "æ€§åˆ«": "ç”·/å¥³",
    "å¹´é¾„": "æ•°å­—",
    "å…¥é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "é™¢æ—¥æœŸ": "YYYY-MM-DD",
    "ä¸»è¯‰": "ä¸»è¦çŠ¶",
    "ç°ç—…å²": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "å…¥é™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‡ºé™¢è¯Šæ–­": ["è¯Šæ–­1", "è¯Šæ–­2"],
    "å‘½ä½“å¾": {
        "ä½“æ¸©": "åŒ…å«å•ä½",
        "è¡€å‹": "åŒ…å«å•ä½"
    },
    "ç”ŸåŒ–æŒ‡æ ‡": {
        "å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶": "å€¼å’Œå•ä½",
        "ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶": "å€¼å’Œå•ä½",
        "ç™½ç»†èƒ": "å€¼å’Œå•ä½",
        "æ·‹å·´ç»†ï¿½ï¿½åˆ†æ¯”": "å€¼å•ä½",
        "ä¸­æ€§ç²’ç»†èƒç™¾åˆ†æ¯”": "å€¼å’Œå•ä½",
        "è¡€çº¢è›‹ç™½": "å€¼å’Œå•ä½",
        "è¡€å°æ¿": "å€¼å’Œå•ä½"
    },
    "è¯Šç–—ç»è¿‡": "æ²»ç–—è¿‡ç¨‹æè¿°",
    "å‡ºé™¢åŒ»å˜±": ["åŒ»å˜±1", "åŒ»2"]
}"""
        
        messages = [
            {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä¸ªOracle 23c JSONæŸ¥è¯¢ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹çš„JSONæ–‡æ¡£ç»“æ„ï¼Œå¸®åŠ©ç”Ÿæˆå‡†ç¡®çš„Oracle JSONæŸ¥è¯¢æ¡ä»¶ã€‚

æ–‡æ¡£ç»“æ„ï¼š
{json_structure}

æ•°æ®åº“ä¿¡æ¯ï¼š
- ä½¿ç”¨Oracle Database 23c
- è¡¨åï¼šDOCUMENT_JSON
- å­—æ®µï¼šdoc_info (VARCHAR2), doc_json (JSON)
- JSONæ•°æ®å­˜å‚¨åœ¨doc_jsonå­—æ®µä¸­

Oracle 23c JSONæŸ¥è¯¢ç‰¹ï¼š
1. ä½¿ç”¨ JSON_EXISTS è¿›è¡Œæ¡ä»¶åŒ¹é…
2. ä½¿ç”¨ JSON_VALUE æå–å•ä¸ªå€¼
3. ä½¿ç”¨ JSON_QUERY æå–JSONæ•°ç»„æˆ–å¯¹è±¡
4. æ”¯æŒç‚¹å·è®¿é—®åµŒå¥—å±æ€§
5. æ”¯æŒç»„ç´¢å¼•è®¿é—®
6. æ”¯æŒæ¡ä»¶è¿‡æ»¤ï¼š?(@ == "value")
7. æ”¯æŒé”®ååŒ¹é…ï¼š
   - ä½¿ç”¨ $.ç”ŸåŒ–æŒ‡æ ‡.* éå†æ‰€æœ‰æŒ‡æ ‡
   - ä½¿ç”¨ EXISTS å’Œ OR ç»„åˆå¤šä¸ªæ¡ä»¶

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
2. æ ¹æ®æ¡£ç»“æ„å’ŒOracle 23cç‰¹æ€§ï¼Œç”Ÿæˆæœ€ä¼˜çš„æŸ¥è¯¢æ¡ä»¶
3. å¯¹äºåŒ»å­¦æœ¯è¯­ï¼Œè€ƒè™‘åŒä¹‰è¯å’Œç®€å†™ï¼ˆå¦‚"è½¬æ°¨é…¶"å¯èƒ½æŒ‡"å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"æˆ–"ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"ï¼‰
4. è¿”å›æ ¼å¼ä¸ºJSONï¼š
{{
    "query_type": "æŸ¥è¯¢ç±»å‹",
    "conditions": ["Oracle JSONæŸ¥è¯¢æ¡ä»¶"],
    "fields": ["éœ€è¦è¿”å›çš„æ®µ"],
    "keywords": ["é”®"]
}}

ç¤ºä¾‹1ï¼š
è¾“å…¥ï¼š"é©¬æŸæŸçš„è½¬æ°¨é…¶æŒ‡æ ‡"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@ == \"é©¬æŸæŸ\")')",
        "(JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.å¤©å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶') OR JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶'))"
    ],
    "fields": ["ç”ŸåŒ–æ ‡.å¤©æ°¨é…¸æ°¨åŸºè½¬é…¶", "ç”ŸåŒ–æŒ‡æ ‡.ä¸™æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶"],
    "keywords": ["é©¬æŸæŸ", "è½¬æ°¨é…¶"]
}}

ç¤ºä¾‹2ï¼š
è¾“å…¥ï¼š"æŸ¥çœ‹é©¬æŸæŸçš„æ·‹å·´ç»†èƒæ¯”ä¾‹"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£å§“å?(@ == \"é©¬æŸæŸ\")')",
        "JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.æ·‹å·´ç»†èƒç™¾åˆ†æ¯”')"
    ],
    "fields": ["ç”ŸåŒ–æŒ‡æ ‡.æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"],
    "keywords": ["é©¬æŸæŸ", "æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"]
}}

ç¤ºä¾‹3ï¼š
è¾“å…¥ï¼š"é©¬æŸæŸçš„è¡€æ¶²ç›¸å…³æŒ‡æ ‡"
è¾“å‡ºï¼š
{{
    "query_type": "æ£€éªŒç»“æœ",
    "conditions": [
        "JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@ == \"é©¬æŸæŸ\")')",
        "(JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.è¡€çº¢è›‹ç™½') OR JSON_EXISTS(doc_json, '$.ç”ŸåŒ–æŒ‡æ ‡.è¡€å°æ¿'))"
    ],
    "fields": [
        "ç”ŸæŒ‡æ ‡.ç™½ç»†èƒ",
        "ç”ŸåŒ–æŒ‡æ ‡.è¡€çº¢è›‹ç™½",
        "ç”ŸåŒ–æŒ‡æ ‡.è¡€å°æ¿"
    ],
    "keywords": ["é©¬æŸæŸ"]
}}"""},
            {"role": "user", "content": query_text}
        ]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPTåˆ†æè¯¢å¤±è´¥: {str(e)}")
        return None

# é…ç½®å¸¸é‡
TOP_K = 5  # æœç´¢ç»“æœè¿”å›çš„æœ€å¤§æ•°é‡

def normalize_medical_term(query_text):
    """ä½¿ç”¨ GPT å°†ç”¨ï¿½ï¿½ï¿½ï¿½ï¿½è¯¢çš„æ ‡åç§°æ ‡å‡†åŒ–"""
    try:
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
        
        messages = [
            {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æŒ‡æ ‡åç§°æ ‡å‡†åŒ–ä¸“å®¶ã€‚
è¯·å°†ç”¨æˆ·æŸ¥è¯¢ä¸­çš„æŒ‡æ ‡åç§°ä¸ºæ ‡çš„ç–—æ ‡ç§°

è§„åˆ™ï¼š
1. æŸ¥è¯¢ä¸­åŒ…å«æŸä¸ªæ£€éªŒæŒ‡æ ‡çš„åŒä¹‰è¯æˆ–è¿‘ä¹‰è¯ï¼Œè¿”å›æ ‡å‡†åç§°
2. å¦‚æœä¸ç¡®å®šï¼Œè¿”å›åŸå§‹è¯è¯­
3. è¿”å›æ ¼å¼ä¸º JSONï¼š{"standard_term": "æ ‡å‡†åç§°"}

ç¤ºï¼š
è¾“å…¥ï¼š"æ·‹å·´ç»†èƒæ¯”ä¾‹"
è¾“å‡ºï¼š{"standard_term": "æ·‹å·´ç»†èƒç™¾åˆ†æ¯”"}

è¾“å…¥ï¼š"ç™½ç»†èƒè®¡æ•°"
è¾“å‡ºï¼š{"standard_term": "ç™½ï¿½ï¿½ï¿½èƒ"}

è¾“å…¥ï¼š"è¡€çº¢è›‹ç™½å«é‡"
è¾“å‡ºï¼š{"standard_term": "è¡€çº¢è›‹ç™½"}"""},
            {"role": "user", "content": query_text}
        ]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        return result.get('standard_term', query_text)
        
    except Exception as e:
        logger.error(f"æŒ‡æ ‡åç§°æ ‡å‡†åŒ–: {str(e)}")
        return query_text

def search_documents(query_text):
    """åŸºäºGPTç”Ÿæˆçš„æŸ¥è¯¢æ¡ä»¶æœç´¢æ–‡æ¡£ï¼Œæ”¯æŒç»“æ„åŒ–æ•°æ®å’Œå…¨æ–‡æœç´¢"""
    try:
        # ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾å¹¶ç”ŸæˆæŸ¥è¯¢æ¡ä»¶
        analysis_result = analyze_query_with_gpt(query_text)
        logger.info(f"GPTåˆ†æç»“æœ: {json.dumps(analysis_result, ensure_ascii=False)}")
        
        if not analysis_result or 'conditions' not in analysis_result:
            logger.error("GPTåˆ†æç»“æœæ ¼å¼é”™è¯¯")
            return []
            
        # ä½¿ç”¨GPTç”Ÿæˆçš„æ¡ä»¶æ„å»ºæŸ¥è¯¢
        conditions = analysis_result.get('conditions', [])
        keywords = analysis_result.get('keywords', [])
        
        # ç»„åˆæ¡ä»¶ï¼ˆä½¿ç”¨ANDè¿æ¥å§“åå’Œå…¶ä»–æ¡ä»¶ï¼‰
        name_conditions = [c for c in conditions if 'æ‚£è€…å§“å' in c]
        other_conditions = [c for c in conditions if 'æ‚£è€…å§“å' not in c]
        
        # æ„å»ºJSONæŸ¥æ¡ä»¶
        if name_conditions and other_conditions:
            json_where = f"({' OR '.join(name_conditions)}) AND ({' OR '.join(other_conditions)})"
        else:
            json_where = " OR ".join(conditions) if conditions else "1=1"
            
        # æ„å»ºå…¨æ–‡æœç´¢æ¡ä»¶ï¼ˆæ’é™¤å§“åå…³é”®è¯ï¼‰
        content_conditions = []
        for keyword in keywords:
            if keyword not in ["é©¬æŸæŸ", "å‘¨æŸæŸ"]:
                content_conditions.append(f"CONTAINS(content, '{keyword}') > 0")
        
        content_where = " OR ".join(content_conditions) if content_conditions else "1=1"
        
        # æ„å»ºå®Œæ•´çš„æŸ¥è¯¢è¯­å¥
        query = """
        SELECT d.doc_info,
               d.doc_json,
               d.content
        FROM DOCUMENT_JSON d
        WHERE 
            -- é¦–å…ˆåŒ¹é…æ‚£è€…å§“åï¼ˆJSONæˆ–å…¨æ–‡ï¼‰
            (
                JSON_EXISTS(doc_json, '$.æ‚£è€…å§“å?(@=="æ¨æŸæŸ")') 
                OR CONTAINS(content, :1) > 0
            )
        ORDER BY id DESC
        FETCH FIRST :2 ROWS ONLY
        """
        
        # è·å–å§“åå…³é”®è¯ï¼ˆå¦‚æœæœ‰ï¼‰
        name_keyword = next((k for k in keywords if k in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]), None)
        
        # æ‰§è¡ŒæŸ¥è¯¢
        with OracleJsonStore() as json_store:
            if name_keyword:
                # æ„å»ºå¸¦æœ‰å…·ä½“å§“åçš„æŸ¥è¯¢
                actual_query = query.replace('æ¨æŸæŸ', name_keyword)
                results = json_store.execute_search(actual_query, [name_keyword, TOP_K])
            else:
                results = json_store.execute_search(query, [TOP_K])
                
        if not results:
            logger.info("æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
            return []
                
        # å¤„ç†æŸ¥è¯¢ç»“æœ
        processed_results = []
        for row in results:
            try:
                doc_json = json.loads(json.dumps(row['doc_json'], cls=DecimalEncoder))
                processed_results.append({
                    'doc_info': row['doc_info'],
                    'doc_json': doc_json,
                    'content': row['content']
                })
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡æ¡£ç»“æœæ—¶å‡ºé”™: {str(e)}")
                continue
                    
        logger.info(f"æ•°æ®åº“çš„æ–‡æ¡£æ•°é‡: {len(processed_results)}")
        for result in processed_results:
            logger.info(f"æ–‡æ¡£è·¯å¾„: {result['doc_info']}")
                
        return processed_results
        
    except Exception as e:
        logger.error(f"JSONæ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
        return []

class DecimalEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Decimal):
            return str(obj)
        return super(DecimalEncoder, self).default(obj)

def generate_answer(query_text, doc_json, content=None):
    """æ ¹æ®æŸ¥è¯¢æ„å›¾ç”Ÿæˆç­”æ¡ˆï¼Œæ”¯æŒç»“æ„åŒ–æ•°æ®å’Œå…¨æ–‡å†…å®¹"""
    try:
        # ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾
        analysis_result = analyze_query_with_gpt(query_text)
        if not analysis_result:
            return None
            
        query_type = analysis_result.get('query_type', '')
        fields = analysis_result.get('fields', [])
        keywords = analysis_result.get('keywords', [])
        
        # è·å–æ‚£è€…å§“å
        name = next((k for k in keywords if k in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]), "æ‚£è€…")
        
        # å¦‚æœæŸ¥è¯¢åŒ…å«ç—‡çŠ¶æˆ–è¯Šæ–­ç›¸å…³çš„å…³é”®è¯ï¼Œä¼˜å…ˆä»å…¨æ–‡å†…å®¹ä¸­æå–
        symptom_keywords = [k for k in keywords if k not in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]]
        if symptom_keywords and content:
            try:
                # ä½¿ç”¨GPTä»å…¨æ–‡ä¸­æå–ç›¸å…³ä¿¡æ¯
                extract_prompt = f"""
ä»ä»¥ä¸‹åŒ»ç–—æ–‡æ¡£ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼š

é—®é¢˜ï¼š{query_text}
å…³æ³¨çš„ç—‡çŠ¶/è¯Šæ–­ï¼š{', '.join(symptom_keywords)}

æ–‡æ¡£å†…å®¹ï¼š
{content}

è¯·æä¾›ç®€æ´çš„ç­”æ¡ˆï¼Œé‡ç‚¹å…³æ³¨é—®é¢˜ä¸­æåˆ°çš„ç—‡çŠ¶æˆ–è¯Šæ–­ã€‚å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›å¤"æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚
"""
                
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—ä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»æ–‡æ¡£ä¸­å‡†ç¡®æå–ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ¨æµ‹çš„å†…å®¹ã€‚"},
                        {"role": "user", "content": extract_prompt}
                    ],
                    temperature=0.1
                )
                
                answer = response.choices[0].message.content.strip()
                if answer != "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯":
                    return f"{name}çš„æƒ…å†µï¼š{answer}"
            except Exception as e:
                logger.error(f"GPTåˆ†æå¤±è´¥: {str(e)}")
                # æœGPTåˆ†æå¤±è´¥ï¼Œç»§ç»­å°è¯•ç»“æ„åŒ–æ•°æ®
                
        # å¦‚æœæ²¡ä»å…¨æ–‡ä¸­æ‰¾åˆ°ä¿¡æ¯ï¼Œæˆ–GPTåˆ†æå¤±è´¥ï¼Œå°è¯•ä»ç»“æ„åŒ–æ•°æ®ä¸­è·å–
        info = []
        for field in fields:
            if '.' in field:  # ç†åµŒå¥—å­—æ®µ
                parent, child = field.split('.')
                if parent in doc_json and child in doc_json[parent]:
                    info.append(f"{child}æ˜¯{doc_json[parent][child]}")
            else:  # å¤„ç†é¡¶å±‚å­—æ®µ
                if field in doc_json:
                    info.append(f"{field}æ˜¯{doc_json[field]}")
        
        if info:
            if query_type:
                return f"{name}çš„{query_type}ï¼š" + "ï¼Œ".join(info)
            else:
                return f"{name}çš„ä¿¡æ¯ï¼š" + "ï¼Œ".join(info)
                
        return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}")
        return "æŠ±æ­‰ï¼Œå¤„ç†ï¿½ï¿½çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯"

def display_search_results(query_text, results):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    if not results:
        st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return

    # å°è¯•ç”Ÿæˆç²¾ç¡®ç­”æ¡ˆ
    for result in results:
        answer = generate_answer(query_text, result['doc_json'], result['content'])
        if answer and not answer.startswith("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"):
            st.success(answer)
        else:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")

def get_patient_metadata(patient_name: str) -> Dict[str, Any]:
    """è·å–æ‚£è€…çš„å®é™…æ•°æ®ç»“æ„"""
    try:
        with OracleGraphStore() as graph_store:
            patient_info = graph_store.get_patient_info(patient_name)
            if not patient_info:
                return {}
            return patient_info
    except Exception as e:
        logger.error(f"è·å–æ‚£è€…å…ƒæ•°æ®å¤±è´¥: {str(e)}")
        return {}

def analyze_graph_query(query_text: str) -> Dict[str, Any]:
    """ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå›¾æ•°æ®æŸ¥è¯¢æ„å›¾"""
    try:
        # ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–æ‚£è€…å§“å
        patient_name = None
        for name in ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"]:
            if name in query_text:
                patient_name = name
                break
        
        if not patient_name:
            logger.warning("æœªèƒ½ä»æŸ¥è¯¢ä¸­è¯†åˆ«å‡ºæ‚£è€…å§“å")
            return {
                "query_type": "åŸºæœ¬ä¿¡æ¯",
                "field": "all",
                "patient_name": None,
                "explanation": "æœªèƒ½è¯†åˆ«æ‚£è€…å§“å"
            }
            
        # è·å–è¯¥æ‚£è€…çš„å®é™…æ•°æ®ç»“æ„
        patient_data = get_patient_metadata(patient_name)
        if not patient_data:
            logger.warning(f"æœªæ‰¾åˆ°æ‚£è€… {patient_name} çš„æ•°æ®")
            return {
                "query_type": "åŸºæœ¬ä¿¡æ¯",
                "field": "all",
                "patient_name": patient_name,
                "explanation": f"æœªæ‰¾åˆ°æ‚£è€… {patient_name} çš„æ•°æ®"
            }
            
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹åŒ»ç–—æŸ¥è¯¢ï¼Œæå–æŸ¥è¯¢æ„å›¾å’Œå…³é”®ä¿¡æ¯ã€‚ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ¼å¼æˆ–ä»£ç å—æ ‡è®°ã€‚

        æŸ¥è¯¢æ–‡æœ¬ï¼š{query_text}

        æ‚£è€… {patient_name} çš„å®é™…æ•°æ®ç»“æ„å¦‚ä¸‹ï¼š
        {json.dumps(patient_data, ensure_ascii=False, indent=2)}

        ä½ éœ€è¦åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ˆä¸è¦æ·»åŠ ä»»ä½•markdownæ ¼å¼æˆ–ä»£ç å—æ ‡è®°ï¼‰ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - query_type: æŸ¥è¯¢ç±»å‹ï¼Œå¿…ï¿½ï¿½ï¿½ä»¥ä¸‹ä¹‹ä¸€ï¼šåŸºæœ¬ä¿¡æ¯/ä¸»è¯‰ä¸è¯Šæ–­/ç°ç—…å²/ç”Ÿå‘½ä½“å¾/ç”ŸåŒ–æŒ‡æ ‡/è¯Šç–—ç»è¿‡
        - field: å…·ä½“æŸ¥è¯¢çš„å­—æ®µåï¼Œå¦‚æœæ˜¯æŸ¥è¯¢æ•´ä¸ªç±»åˆ«ï¼Œè¯·è¿”å›"all"
        - patient_name: æ‚£è€…å§“å
        - explanation: æŸ¥è¯¢æ„å›¾çš„è§£é‡Š

        å¦‚æœæ˜¯æŸ¥è¯¢ä¸ªäººä¿¡æ¯ï¼Œåº”è¿”å›ï¼š
        {{
            "query_type": "åŸºæœ¬ä¿¡æ¯",
            "field": "all",
            "patient_name": "{patient_name}",
            "explanation": "æŸ¥è¯¢æ‚£è€…çš„æ‰€æœ‰åŸºæœ¬ä¿¡æ¯"
        }}

        å¦‚æœæ˜¯æŸ¥è¯¢ä¸ªç±»åˆ«çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆå¦‚"ç”ŸåŒ–æŒ‡æ ‡"ã€"ä¸»è¯‰ä¸è¯Šæ–­"ç­‰ï¼‰ï¼Œè¯·å°†fieldè®¾ç½®ä¸º"all"ã€‚
        å¦‚æœæ˜¯æŸ¥è¯¢å…·ä½“çš„æŒ‡æ ‡æˆ–ç—‡çŠ¶ï¼ˆå¦‚"ç™½ç»†èƒ"ã€"è¡€å‹"ç­‰ï¼‰ï¼Œè¯·å°†fieldè®¾ç½®ä¸ºå…·ä½“çš„æŒ‡æ ‡åç§°ã€‚

        è¯·åˆ†æè¿™ä¸ªæŸ¥è¯¢å¹¶è¿”å›JSONï¼ˆä¸è¦æ·»åŠ markdownæ ¼å¼ï¼‰
        """

        client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )
        
        response = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ¼å¼æˆ–ä»£ç å—æ ‡è®°ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content.strip()
        logger.info(f"OpenAI APIè¿”å›å†…å®¹: {content}")
        
        # å°è¯•è§£æè¿”å›å†…å®¹
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            try:
                result = json.loads(content)
            except json.JSONDecodeError:
                logger.warning("ç›´æ¥è§£æJSONå¤±è´¥ï¼Œå°è¯•æ¸…ç†å†…å®¹åé‡æ–°è§£æ")
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ¸…ç†å†…å®¹ï¼ˆå»é™¤å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦ç­‰ï¼‰
                cleaned_content = content.replace('\n', '').replace('\r', '').strip()
                result = json.loads(cleaned_content)
            
            # å¦‚æœç»“æœè¢«åŒ…è£…åœ¨responseå­—æ®µä¸­ï¼Œæå–å†…å±‚JSON
            if isinstance(result, dict) and "response" in result:
                try:
                    inner_content = result["response"]
                    if isinstance(inner_content, str):
                        # æ¸…ç†å†…å±‚JSONå­—ç¬¦ä¸²
                        inner_content = inner_content.replace('\n', '').replace('\r', '').strip()
                        result = json.loads(inner_content)
                    elif isinstance(inner_content, dict):
                        result = inner_content
                except json.JSONDecodeError as e:
                    logger.error(f"è§£æresponseå­—æ®µå¤±è´¥: {str(e)}")
                    result = {
                        "query_type": "åŸºæœ¬ä¿¡æ¯",
                        "field": "all",
                        "patient_name": patient_name,
                        "explanation": "è§£ææŸ¥è¯¢æ„å›¾æ—¶å‡ºç°é”™è¯¯"
                    }
            
            # éªŒè¯ç»“æœæ ¼å¼
            if not isinstance(result, dict):
                raise ValueError("è¿”å›ç»“æœä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ["query_type", "field", "patient_name", "explanation"]
            missing_fields = [field for field in required_fields if field not in result]
            if missing_fields:
                logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
                for field in missing_fields:
                    if field == "query_type":
                        result[field] = "åŸºæœ¬ä¿¡æ¯"
                    elif field == "field":
                        result[field] = "all"
                    elif field == "patient_name":
                        result[field] = patient_name
                    elif field == "explanation":
                        result[field] = "æŸ¥è¯¢æ‚£è€…ä¿¡æ¯"
            
            # éªŒè¯query_typeæ˜¯å¦ä¸ºæœ‰æ•ˆå€¼
            valid_query_types = ["åŸºæœ¬ä¿¡æ¯", "ä¸»è¯‰ä¸è¯Šæ–­", "ç°ç—…å²", "ç”Ÿå‘½ä½“å¾", "ç”ŸåŒ–æŒ‡æ ‡", "è¯Šç–—ç»è¿‡"]
            if result["query_type"] not in valid_query_types:
                logger.warning(f"æ— æ•ˆï¿½ï¿½query_type: {result['query_type']}, ä½¿ç”¨é»˜è®¤å€¼")
                result["query_type"] = "åŸºæœ¬ä¿¡æ¯"
            
            # ç¡®ä¿patient_nameä¸æŸ¥è¯¢ä¸­è¯†åˆ«çš„ä¸€è‡´
            if result["patient_name"] != patient_name:
                logger.warning(f"patient_nameä¸åŒ¹é…: {result['patient_name']} != {patient_name}")
                result["patient_name"] = patient_name
            
            # å¤„ç†fieldå€¼
            if result["field"] == result["query_type"] or result["field"] in valid_query_types:
                logger.info(f"å°†fieldä» {result['field']} ä¿®æ”¹ä¸º all")
                result["field"] = "all"
            
            logger.info(f"æŸ¥è¯¢æ„å›¾åˆ†æç»“æœ: {json.dumps(result, ensure_ascii=False)}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {str(e)}, åŸå§‹å†…å®¹: {content}")
            return {
                "query_type": "åŸºæœ¬ä¿¡æ¯",
                "field": "all",
                "patient_name": patient_name,
                "explanation": "è§£ææŸ¥è¯¢æ„å›¾æ—¶å‡ºç°é”™è¯¯"
            }
            
    except Exception as e:
        logger.error(f"åˆ†ææŸ¥è¯¢æ„å›¾å¤±è´¥: {str(e)}")
        return {
            "query_type": "åŸºæœ¬ä¿¡æ¯",
            "field": "all",
            "patient_name": patient_name if 'patient_name' in locals() else None,
            "explanation": "åˆ†ææŸ¥è¯¢æ„å›¾æ—¶å‡ºç°é”™è¯¯"
        }

def search_graph_data(query_text: str) -> List[Dict[str, Any]]:
    """åŸºäºå›¾æ•°æ®çš„æœç´¢"""
    try:
        # ä½¿ç”¨GPTåˆ†ææŸ¥è¯¢æ„å›¾
        analysis = analyze_graph_query(query_text)
        query_type = analysis.get("query_type")
        field = analysis.get("field")
        patient_name = analysis.get("patient_name")
        
        if not all([query_type, patient_name]):
            logger.error("æŸ¥è¯¢æ„å›¾åˆ†æç»“æœä¸å®Œæ•´")
            return []
        
        # ä½¿ç”¨å›¾æ•°æ®åº“æœç´¢
        with OracleGraphStore() as graph_store:
            # è·å–æ‚£è€…ä¿¡æ¯
            patient_info = graph_store.get_patient_info(patient_name)
            if not patient_info:
                return []
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹è¿”å›ç»“æœ
            if query_type == "åŸºæœ¬ä¿¡æ¯":
                if field == "all":
                    # è¿”å›æ‰€æœ‰åŸºæœ¬ä¿¡æ¯
                    info = patient_info.get("æ‚£è€…", {}).get("åŸºæœ¬ä¿¡æ¯", {})
                    if not info:
                        info = patient_info.get("åŸºæœ¬ä¿¡æ¯", {})
                    if info:
                        result = []
                        for k, v in info.items():
                            result.append(f"{k}ï¼š{v}")
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„åŸºæœ¬ä¿¡æ¯ï¼š\n" + "\n".join(result),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    # æŸ¥è¯¢ç‰¹å®šå­—æ®µ
                    value = None
                    # å…ˆå°è¯•ä»æ‚£è€….åŸºæœ¬ä¿¡æ¯ä¸­è·å–
                    info = patient_info.get("æ‚£è€…", {}).get("åŸºæœ¬ä¿¡æ¯", {})
                    if not info:
                        # å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»åŸºæœ¬ä¿¡æ¯ä¸­è·å–
                        info = patient_info.get("åŸºæœ¬ä¿¡æ¯", {})
                    value = info.get(field)
                    if value:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}æ˜¯{value}",
                            "explanation": analysis.get("explanation")
                        }}]
            
            elif query_type == "ä¸»è¯‰ä¸è¯Šæ–­":
                items = patient_info.get("ä¸»è¯‰ä¸è¯Šæ–­", [])
                if field == "all":
                    results = []
                    # åˆ†ç±»å¤„ç†ä¸»è¯‰å’Œè¯Šæ–­
                    chief_complaints = []
                    admission_diagnoses = []
                    discharge_diagnoses = []
                    for item in items:
                        if item.get("ç±»å‹") == "ä¸»è¯‰":
                            chief_complaints.append(item.get("å†…å®¹"))
                        elif item.get("ç±»å‹") == "å…¥é™¢è¯Šæ–­":
                            admission_diagnoses.append(item.get("å†…å®¹"))
                        elif item.get("ç±»å‹") == "å‡ºé™¢è¯Šæ–­":
                            discharge_diagnoses.append(item.get("å†…å®¹"))
                    
                    # ç»„ç»‡è¿”å›å†…å®¹
                    if chief_complaints:
                        results.append("ä¸»è¯‰ï¼š")
                        results.extend([f"- {complaint}" for complaint in chief_complaints])
                    if admission_diagnoses:
                        results.append("\nå…¥é™¢è¯Šæ–­ï¼š")
                        results.extend([f"- {diagnosis}" for diagnosis in admission_diagnoses])
                    if discharge_diagnoses:
                        results.append("\nå‡ºé™¢è¯Šæ–­ï¼š")
                        results.extend([f"- {diagnosis}" for diagnosis in discharge_diagnoses])
                        
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ä¸»è¯‰ä¸è¯Šæ–­ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    results = []
                    for item in items:
                        if field.lower() in item.get("ç±»å‹", "").lower():
                            results.append(item.get("å†…å®¹"))
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}ï¼š\n" + "\n".join([f"- {r}" for r in results]),
                            "explanation": analysis.get("explanation")
                        }}]
            
            elif query_type == "ç°ç—…å²":
                items = patient_info.get("ç°ç—…å²", [])
                if field == "all":
                    results = []
                    for item in items:
                        symptom = item.get("ç—‡çŠ¶", "")
                        description = item.get("æè¿°", "")
                        if description:
                            results.append(f"- {symptom}ï¼š{description}")
                        else:
                            results.append(f"- {symptom}")
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ç°ç—…å²ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    results = []
                    for item in items:
                        if field.lower() in item.get("ç—‡çŠ¶", "").lower():
                            description = item.get("æè¿°", "")
                            if description:
                                results.append(f"{item.get('ç—‡çŠ¶')}ï¼š{description}")
                            else:
                                results.append(item.get('ç—‡çŠ¶'))
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
            
            elif query_type == "ç”Ÿå‘½ä½“å¾":
                items = patient_info.get("ç”Ÿå‘½ä½“å¾", [])
                if field == "all":
                    results = []
                    for item in items:
                        results.append(f"- {item.get('æŒ‡æ ‡')}ï¼š{item.get('æ•°å€¼')}{item.get('å•ä½', '')}")
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ç”Ÿå‘½ä½“å¾ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    for item in items:
                        if field.lower() in item.get("æŒ‡æ ‡", "").lower():
                            return [{"type": "answer", "data": {
                                "question": query_text,
                                "answer": f"{patient_name}çš„{field}æ˜¯ï¼š{item.get('æ•°å€¼')}{item.get('å•ä½', '')}",
                                "explanation": analysis.get("explanation")
                            }}]
            
            elif query_type == "ç”ŸåŒ–æŒ‡æ ‡":
                items = patient_info.get("ç”ŸåŒ–æŒ‡æ ‡", [])
                if field == "all":
                    results = []
                    # æŒ‰ç…§å‚è€ƒèŒƒå›´åˆ†ç±»
                    abnormal_items = []
                    normal_items = []
                    for item in items:
                        result_str = f"- {item.get('é¡¹ç›®')}ï¼š{item.get('ç»“æœ')}{item.get('å•ä½', '')}"
                        if item.get('å‚è€ƒèŒƒå›´') == 'å¼‚å¸¸':
                            abnormal_items.append(result_str + " (å¼‚å¸¸)")
                        else:
                            normal_items.append(result_str + " (æ­£å¸¸)")
                    
                    # ç»„ç»‡è¿”å›å†…å®¹
                    if abnormal_items:
                        results.append("å¼‚å¸¸æŒ‡æ ‡ï¼š")
                        results.extend(abnormal_items)
                    if normal_items:
                        if results:  # å¦‚æœå·²ç»æœ‰å¼‚å¸¸æŒ‡æ ‡ï¼Œæ·»åŠ ç©ºè¡Œ
                            results.append("")
                        results.append("æ­£å¸¸æŒ‡æ ‡ï¼š")
                        results.extend(normal_items)
                        
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„ç”ŸåŒ–æŒ‡æ ‡ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    for item in items:
                        if field.lower() in item.get("é¡¹ç›®", "").lower():
                            return [{"type": "answer", "data": {
                                "question": query_text,
                                "answer": f"{patient_name}çš„{field}æ˜¯ï¼š{item.get('ç»“æœ')}{item.get('å•ä½', '')} ({item.get('å‚è€ƒèŒƒå›´', '')})",
                                "explanation": analysis.get("explanation")
                            }}]
            
            elif query_type == "è¯Šç–—ç»è¿‡":
                items = patient_info.get("è¯Šç–—ç»è¿‡", [])
                if field == "all":
                    results = []
                    # åˆ†ç±»å¤„ç†è¯Šç–—ç»è¿‡å’Œå‡ºé™¢åŒ»å˜±
                    diagnoses = []
                    advices = []
                    for item in items:
                        if item.get("ç±»å‹") == "è¯Šç–—ç»è¿‡":
                            diagnoses.append(item.get("å†…å®¹"))
                        elif item.get("ç±»å‹") == "å‡ºé™¢åŒ»å˜±":
                            advices.append(item.get("å†…å®¹"))
                    
                    # ç»„ç»‡è¿”å›å†…å®¹
                    if diagnoses:
                        results.append("è¯Šç–—ç»è¿‡ï¼š")
                        results.extend([f"- {diagnosis}" for diagnosis in diagnoses])
                    if advices:
                        if results:  # å¦‚æœå·²ç»æœ‰è¯Šç–—ç»è¿‡ï¼Œæ·»åŠ ç©ºè¡Œ
                            results.append("")
                        results.append("å‡ºé™¢åŒ»å˜±ï¼š")
                        results.extend([f"- {advice}" for advice in advices])
                        
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„è¯Šç–—ç»è¿‡ï¼š\n" + "\n".join(results),
                            "explanation": analysis.get("explanation")
                        }}]
                else:
                    results = []
                    for item in items:
                        if field.lower() in item.get("ç±»å‹", "").lower():
                            results.append(item.get("å†…å®¹"))
                    if results:
                        return [{"type": "answer", "data": {
                            "question": query_text,
                            "answer": f"{patient_name}çš„{field}ï¼š\n" + "\n".join([f"- {r}" for r in results]),
                            "explanation": analysis.get("explanation")
                        }}]
            
            return []
            
    except Exception as e:
        logger.error(f"å›¾æ•°æ®æœç´¢å¤±è´¥: {str(e)}")
        return []

def display_graph_results(results: List[Dict[str, Any]], query_text: str):
    """æ˜¾ç¤ºå›¾æ•°æ®æœç´¢ç»“æœ"""
    if not results:
        st.warning("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
        return
        
    for result in results:
        if result["type"] == "answer":
            data = result["data"]
            st.write(data["answer"])

def process_graph_data(file_path: str, content: str) -> bool:
    """å¤„ç†æ–‡æ¡£çš„å›¾æ•°æ®"""
    try:
        # åªå¤„ç†å›¾æ•°æ®
        graph_parser = MedicalGraphParser()
        graph_result = graph_parser.parse_to_graph(content, file_path)
        
        if "error" in graph_result:
            return False
            
        return True
    except Exception as e:
        logger.error(f"å¤„ç†å›¾æ•°æ®å¤±è´¥: {str(e)}")
        return False

def visualize_patient_graph(patient_info: Dict[str, Any]) -> str:
    """ä½¿ç”¨pyviså¯è§†åŒ–æ‚£è€…çš„å±æ€§å›¾"""
    try:
        # åˆ›å»ºç½‘ç»œå›¾
        net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="black", directed=False)
        
        # è®¾ç½®ç‰©ç†å¸ƒå±€é€‰é¡¹
        net.set_options("""
        {
          "nodes": {
            "font": {
              "size": 14,
              "face": "Microsoft YaHei"
            }
          },
          "edges": {
            "color": {
              "color": "#666666",
              "highlight": "#000000"
            }
          },
          "physics": {
            "enabled": true,
            "solver": "forceAtlas2Based",
            "forceAtlas2Based": {
              "gravitationalConstant": -50,
              "centralGravity": 0.01,
              "springLength": 100,
              "springConstant": 0.08,
              "damping": 0.4,
              "avoidOverlap": 0.5
            }
          }
        }
        """)
        
        # æ·»åŠ æ‚£è€…èŠ‚ç‚¹ï¼ˆä¸­å¿ƒèŠ‚ç‚¹ï¼‰
        patient_name = patient_info.get('å§“å', 'æœªçŸ¥æ‚£è€…')
        
        net.add_node(patient_name, 
                    label=patient_name,
                    color='#add8e6',  # lightblue
                    size=30,
                    shape='circle')
        
        # æ·»åŠ åŸºæœ¬ä¿¡æ¯èŠ‚ç‚¹
        basic_info = patient_info.get('åŸºæœ¬ä¿¡æ¯', {})
        if basic_info:
            for key, value in basic_info.items():
                node_id = f'basic_{key}'
                net.add_node(node_id,
                            label=f'{key}ï¼š{value}',
                            color='#90EE90',  # lightgreen
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='åŸºæœ¬ä¿¡æ¯')
        
        # æ·»åŠ ä¸»è¯‰ä¸è¯Šæ–­èŠ‚ç‚¹
        if 'ä¸»è¯‰ä¸è¯Šæ–­' in patient_info:
            for i, item in enumerate(patient_info['ä¸»è¯‰ä¸è¯Šæ–­']):
                node_id = f'diag_{i}'
                net.add_node(node_id,
                            label=f"{item.get('ç±»å‹')}ï¼š{item.get('å†…å®¹')}",
                            color='#FFB6C1',  # lightpink
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ä¸»è¯‰ä¸è¯Šæ–­')
        
        # æ·»åŠ ç°ç—…å²èŠ‚ç‚¹
        if 'ç°ç—…å²' in patient_info:
            for i, item in enumerate(patient_info['ç°ç—…å²']):
                node_id = f'hist_{i}'
                net.add_node(node_id,
                            label=f"{item.get('ç—‡çŠ¶')}ï¼š{item.get('æè¿°')}",
                            color='#FFFFE0',  # lightyellow
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ç°ç—…å²')

        # æ·»åŠ ç”Ÿå‘½ä½“å¾ï¿½ï¿½ï¿½ç‚¹
        if 'ç”Ÿå‘½ä½“å¾' in patient_info:
            for i, item in enumerate(patient_info['ç”Ÿå‘½ä½“å¾']):
                node_id = f'vital_{i}'
                net.add_node(node_id,
                            label=f"{item.get('æŒ‡æ ‡')}ï¼š{item.get('æ•°å€¼')}",
                            color='#F08080',  # lightcoral
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ç”Ÿå‘½ä½“å¾')
        
        # æ·»ï¿½ï¿½ç”ŸåŒ–æŒ‡æ ‡èŠ‚ç‚¹
        if 'ç”ŸåŒ–æŒ‡æ ‡' in patient_info:
            for i, item in enumerate(patient_info['ç”ŸåŒ–æŒ‡æ ‡']):
                node_id = f'biochem_{i}'
                net.add_node(node_id,
                            label=f"{item.get('é¡¹ç›®')}ï¼š{item.get('ç»“æœ')}",
                            color='#DDA0DD',  # plum
                            size=20,
                            shape='box')
                net.add_edge(patient_name, node_id, title='ç”ŸåŒ–æŒ‡æ ‡')
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜HTML
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.html', encoding='utf-8') as f:
            try:
                net.save_graph(f.name)
                return f.name
            except Exception as e:
                raise
                
    except Exception as e:
        raise

def display_parsed_documents():
    """æ˜¾ç¤ºå·²è§£æçš„æ–‡æ¡£"""
    st.subheader("æŸ¥çœ‹å·²è§£æçš„æ–‡æ¡£")
    
    try:
        with OracleGraphStore() as graph_store:
            patients = graph_store.get_all_patients()
            
            if not patients:
                st.info("ğŸ“­ æ•°æ®åº“ä¸­æš‚æ— ç»“æ„åŒ–æ–‡æ¡£ï¼Œè¯·å…ˆåœ¨æ–‡æ¡£ç®¡ç†ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
                return
                
            st.write("å·²è§£æçš„æ–‡æ¡£ä¸­åŒ…å«ä»¥ä¸‹æ‚£è€…ï¼š")
            
            # æ˜¾ç¤ºæ‚£è€…åˆ—è¡¨
            for patient in patients:
                patient_name = patient.get('å§“å', 'æœªçŸ¥æ‚£è€…')
                # ä½¿ç”¨expanderä½¿æ¯ä¸ªæ‚£è€…çš„ä¿¡æ¯é»˜è®¤æŠ˜å 
                with st.expander(f"ğŸ“‹ {patient_name}", expanded=False):
                    # è·å–æ‚£è€…çš„å®Œæ•´ä¿¡æ¯
                    patient_info = graph_store.get_patient_info(patient_name)
                    if patient_info:
                        # åˆ›å»ºä¸¤ä¸ªæ ‡ç­¾é¡µ
                        tab1, tab2 = st.tabs(["çŸ¥è¯†å›¾è°±", "å®Œæ•´æ•°æ®"])
                        
                        with tab1:
                            try:
                                # åˆ›å»ºå¹¶æ˜¾ç¤ºäº¤äº’å¼ç½‘ç»œå›¾
                                html_path = visualize_patient_graph(patient_info)
                                with open(html_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                components.html(html_content, height=600)
                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                os.unlink(html_path)
                            except Exception as e:
                                st.error(f"æ˜¾ç¤ºå›¾å½¢æ—¶å‡ºé”™: {str(e)}")
                        
                        with tab2:
                            st.json(patient_info)
                    else:
                        st.error("æ— è·å–æ‚£è€…è¯¦ç»†ä¿¡æ¯")
            
    except Exception as e:
        logger.error(f"æ˜¾ç¤ºå·²è§£ææ–‡æ¡£å¤±è´¥: {str(e)}")
        st.error("æ˜¾ç¤ºå·²è§£ææ–‡æ¡£æ—¶å‡ºç°é”™è¯¯")

def display_document_management():
    """æ˜¾ç¤ºæ–‡æ¡£ç®¡ç†ç•Œé¢"""
    st.header("æ–‡æ¡£ç®¡ç†")
    
    # æ˜¾ç¤ºå·²ä¸Šä¼ çš„æ–‡æ¡£
    st.subheader("å·²ä¸Šä¼ çš„æ–‡æ¡£")
    files = list(UPLOAD_DIR.glob("*.*"))
    if files:
        for file in files:
            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
            with col1:
                st.write(file.name)
            with col2:
                if st.button("å‘é‡åŒ–", key=f"vec_{file.name}"):
                    with st.spinner("æ­£åœ¨å‘é‡åŒ–..."):
                        if vectorize_document(file):
                            st.success("å‘é‡åŒ–æˆåŠŸ")
                        else:
                            st.error("å‘é‡åŒ–å¤±è´¥")
            with col3:
                if st.button("ç»“æ„åŒ–", key=f"struct_{file.name}"):
                    with st.spinner("æ­£åœ¨ç»“æ„åŒ–..."):
                        # è¯»å–æ–‡ä»¶å†…å®¹
                        content = read_file_content(file)
                        file_obj = type('FileObject', (), {
                            'name': file.name,
                            'type': 'application/pdf' if file.suffix == '.pdf' else 'text/plain',
                            'read': lambda: open(file, 'rb').read()
                        })()
                        if parse_document_to_json(file_obj):
                            st.success("ç»“æ„åŒ–æˆåŠŸ")
                        else:
                            st.error("ç»“æ„åŒ–å¤±è´¥")
            with col4:
                if st.button("å›¾æ•°æ®", key=f"graph_{file.name}"):
                    with st.spinner("æ­£åœ¨å¤„ç†å›¾æ•°æ®..."):
                        # è¯»å–æ–‡ä»¶å†…å®¹
                        content = read_file_content(file)
                        if process_graph_data(file.name, content):
                            st.success("å›¾æ•°æ®å¤„ç†æˆåŠŸ")
                        else:
                            st.error("å›¾æ•°æ®å¤„ç†å¤±è´¥")
            st.markdown("---")
    else:
        st.info("æš‚æ— ä¸Šä¼ çš„æ–‡æ¡£")
        
    # æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
    uploaded_file = st.file_uploader("ä¸Šä¼ åŒ»ç–—æ–‡æ¡£", type=["pdf", "docx", "txt"])
    
    if uploaded_file:
        col1, col2 = st.columns([2, 1])
        with col1:
            st.write(f"å·²é€‰æ‹© {uploaded_file.name}")
        with col2:
            if st.button("ä¿å­˜æ–‡æ¡£"):
                with st.spinner(f"æ­£åœ¨ä¿å­˜ {uploaded_file.name}..."):
                    file_path = save_uploaded_file(uploaded_file)
                    if file_path:
                        st.success(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
                    else:
                        st.error("æ–‡ä»¶ä¿å­˜å¤±è´¥")

def display_vector_search():
    """æ˜¾ç¤ºå‘é‡æ£€ç´¢ç•Œé¢"""
    st.header("å‘é‡æ£€ç´¢")
    
    # æ˜¾ç¤ºå·²å‘é‡åŒ–çš„æ–‡æ¡£åˆ—è¡¨
    with st.expander("æŸ¥çœ‹å·²å‘é‡åŒ–çš„æ–‡æ¡£", expanded=True):
        with OracleVectorStore() as vector_store:
            documents = vector_store.list_documents()
            if documents:
                st.write("å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼š")
                for doc in documents:
                    st.write(f"- {doc['file_path']} (å‘é‡æ•°: {doc['chunk_count']})")
            else:
                st.info("æš‚æ— å‘é‡åŒ–æ–‡æ¡£")
    
    # æœç´¢åŠŸèƒ½
    query = st.text_input("è¯·è¾“å…¥æœç´¢å†…å®¹")
    if query:
        with st.spinner("æ­£åœ¨æœç´¢å¹¶åˆ†æ..."):
            results = search_similar_documents(query, top_k=3)
            
            if results:
                # æ˜¾ç¤ºæœ€ç›¸å…³æ–‡æ¡£çš„ GPT åˆ†æç»“æœ
                st.subheader("GPT åˆ†æç»“æœ")
                st.write(results[0]['gpt_analysis'])
                
                # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æ–‡æ¡£
                st.subheader("ç›¸å…³æ–‡æ¡£")
                for i, result in enumerate(results, 1):
                    similarity = 1 - result['similarity']
                    with st.expander(f"æ–‡æ¡£ {i}: {result['file_path']} (ç›¸ä¼¼åº¦: {similarity:.2%})"):
                        st.write(result['content'])
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

def display_structured_search():
    """æ˜¾ç¤ºç»“æ„åŒ–æ£€ç´¢ç•Œé¢"""
    st.header("ç»“æ„åŒ–æ£€ç´¢")
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­çš„ç»“æ„åŒ–æ–‡æ¡£
    with OracleJsonStore() as json_store:
        try:
            # é¦–å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_table_sql = """
            SELECT COUNT(*) as count
            FROM user_tables 
            WHERE table_name = 'DOCUMENT_JSON'
            """
            result = json_store.execute_search(check_table_sql)
            table_exists = result[0]['count'] > 0 if result else False
            
            if not table_exists:
                st.warning("æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆï¿½ï¿½æ–‡æ¡£ç®¡ç†ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
                return
            
            # è·å–æ‰€æœ‰æ–‡æ¡£
            check_sql = """
            SELECT doc_info, doc_json, content 
            FROM DOCUMENT_JSON 
            ORDER BY id DESC
            """
            all_docs = json_store.execute_search(check_sql)
            
            # æ˜¾ç¤ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
            st.subheader("ğŸ“š æ•°æ®åº“ä¸­æ‰€æœ‰æ–‡æ¡£")
            if all_docs:
                st.write(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£")
                for doc in all_docs:
                    if isinstance(doc['doc_json'], dict):
                        data = doc['doc_json']
                        patient_name = data.get("æ‚£è€…å", Path(doc['doc_info']).stem)
                        
                        # ä½¿ç”¨expanderä¸ºæ¯ä¸ªæ‚£è€…åˆ›å»ºæŠ˜å é¢æ¿
                        with st.expander(f"ğŸ“‹ {patient_name}", expanded=False):
                            # åˆ›å»ºæ ‡ç­¾é¡µ
                            tabs = st.tabs([
                                "åŸºæœ¬ä¿¡æ¯", "ä¸»è¯‰ä¸è¯Šæ–­", "ç°ç—…å²", 
                                "ç”Ÿå‘½ä½“å¾", "ç”ŸåŒ–æŒ‡æ ‡", "è¯Šç–—ç»è¿‡", "å…¨æ–‡å†…å®¹"
                            ])
                            
                            with tabs[0]:
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.markdown("**æ‚£è€…ä¿¡æ¯**")
                                    info = {
                                        "å§“å": data.get("æ‚£è€…å§“å", "æœªçŸ¥"),
                                        "æ€§åˆ«": data.get("æ€§åˆ«", "æœªçŸ¥"),
                                        "å¹´é¾„": data.get("å¹´é¾„", "æœªçŸ¥"),
                                        "æ°‘æ—": data.get("æ°‘æ—", "æœªçŸ¥"),
                                        "èŒä¸š": data.get("èŒä¸š", "æœªçŸ¥"),
                                        "å©šå§»çŠ¶å†µ": data.get("å©šå§»çŠ¶å†µ", "æœªçŸ¥")
                                    }
                                    st.json(info)
                                with col2:
                                    st.markdown("**ä½é™¢ä¿¡æ¯**")
                                    info = {
                                        "å…¥é™¢æ—¥æœŸ": data.get("å…¥é™¢æ—¥æœŸ", "æœªçŸ¥"),
                                        "å‡ºé™¢æ—¥æœŸ": data.get("å‡ºé™¢æ—¥æœŸ", "æœªçŸ¥"),
                                        "ä½é™¢å¤©æ•°": data.get("ä½é™¢å¤©æ•°", "æœªçŸ¥"),
                                        "å‡ºé™¢æƒ…å†µ": data.get("å‡ºé™¢æƒ…å†µ", "æœªçŸ¥")
                                    }
                                    st.json(info)
                            
                            with tabs[1]:
                                st.markdown("**ä¸»è¯‰**")
                                st.write(data.get("ä¸»è¯‰", "æœªè®°å½•"))
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.markdown("**å…¥é™¢è¯Šæ–­**")
                                    for diag in data.get("å…¥é™¢è¯Šæ–­", []):
                                        st.write(f"- {diag}")
                                with col2:
                                    st.markdown("**å‡ºé™¢è¯Šæ–­**")
                                    for diag in data.get("å‡ºé™¢è¯Šæ–­", []):
                                        st.write(f"- {diag}")
                            
                            with tabs[2]:
                                st.markdown("**ç°ç—…å²**")
                                for item in data.get("ç°ç—…å²", []):
                                    st.write(f"- {item}")
                            
                            with tabs[3]:
                                if "ç”Ÿå‘½ä½“å¾" in data:
                                    st.json(data["ç”Ÿå‘½ä½“å¾"])
                                else:
                                    st.info("æœªè®°å½•ç”Ÿå‘½ä½“å¾")
                            
                            with tabs[4]:
                                if "ç”ŸåŒ–æŒ‡æ ‡" in data:
                                    st.json(data["ç”ŸåŒ–æŒ‡æ ‡"])
                                else:
                                    st.info("æœªè®°å½•ç”ŸåŒ–æŒ‡æ ‡")
                            
                            with tabs[5]:
                                st.markdown("**è¯Šç–—ç»è¿‡**")
                                st.write(data.get("è¯Šç–—ç»è¿‡", "æœªè®°å½•"))
                                if "å‡ºé™¢åŒ»å˜±" in data:
                                    st.markdown("**å‡ºé™¢åŒ»å˜±**")
                                    for advice in data["å‡ºé™¢åŒ»å˜±"]:
                                        st.write(f"- {advice}")
                                        
                            with tabs[6]:
                                st.markdown("**æ–‡æ¡£å…¨æ–‡**")
                                if "content" in doc:
                                    st.text_area("", doc["content"], height=400)
                                else:
                                    st.info("æœªæ‰¾åˆ°æ–‡æ¡£å…¨æ–‡å†…å®¹")
            else:
                st.info("ğŸ“­ æ•°æ®åº“æš‚æ— ç»“æ„åŒ–æ–‡æ¡£ï¼Œè¯·å…ˆåœ¨æ–‡æ¡£ç®¡ç†ä¸­ä¸Šä¼ å¹¶ç»“æ„åŒ–æ–‡æ¡£")
            
            # æœç´¢åŠŸèƒ½
            st.divider()
            st.subheader("ğŸ” æ™ºèƒ½æœç´¢")
            query = st.text_input("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼ˆæ”¯æŒç»“æ„åŒ–æ•°æ®å’Œå…¨æ–‡æœç´¢ï¼‰")
            
            if query:
                with st.spinner("æ­£åœ¨åˆ†ææŸ¥è¯¢å¹¶æœç´¢..."):
                    results = search_documents(query)
                    if results:
                        for result in results:
                            answer = generate_answer(query, result['doc_json'], result['content'])
                            if answer:
                                st.success(answer)
                                with st.expander("æŸ¥çœ‹å®Œæ•´æ–‡æ¡£"):
                                    display_search_results(query, [result])
                    else:
                        st.warning("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                    
        except Exception as e:
            logger.error(f"æ£€ç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.error(f"æ£€ç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

def display_property_graph_search():
    """ï¿½ï¿½ç¤ºå±æ€§å›¾æ£€ç´¢ç•Œé¢"""
    st.header("å±æ€§å›¾æ£€ç´¢")
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2 = st.tabs(["æŸ¥è¯¢æ¨¡æ¿", "è‡ªå®šä¹‰æŸ¥è¯¢"])
    
    with tab1:
        st.subheader("å¸¸ç”¨æŸ¥è¯¢æ¨¡æ¿")
        query_type = st.selectbox(
            "é€‰æ‹©æŸ¥è¯¢ç±»å‹",
            [
                "æ‚£è€…ç›¸ä¼¼ç—‡çŠ¶åˆ†æ",
                "æ‚£è€…ç”ŸåŒ–æŒ‡æ ‡å¼‚å¸¸å…³è”",
                "æ‚£è€…è¯Šæ–­å…³ç³»ç½‘ç»œ",
                "æ‚£è€…ç”¨è¯å…³è”åˆ†æ",
                "æ‚£è€…æ²»ç–—æ–¹æ¡ˆå¯¹æ¯”"
            ]
        )
        
        if query_type == "æ‚£è€…ç›¸ä¼¼ç—‡çŠ¶åˆ†æ":
            patient_name = st.selectbox("é€‰æ‹©æ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æç›¸ä¼¼ç—‡çŠ¶..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨PGQLæŸ¥è¯¢ç›¸ä¼¼ç—‡çŠ¶
                            query = """
                            v1.entity_name AS patient1, 
                            v2.entity_name AS patient2,
                            e1.relation_type AS symptom1,
                            e2.relation_type AS symptom2
                            MATCH (v1) -[e1]-> () <-[e2]- (v2)
                            WHERE v1.entity_type = 'PATIENT' 
                              AND v2.entity_type = 'PATIENT'
                              AND v1.entity_name = :patient_name
                              AND v1.entity_name != v2.entity_name
                              AND e1.relation_type = 'ç°ç—…å²'
                              AND e2.relation_type = 'ç°ç—…å²'
                            """
                            results = graph_store.execute_pgql(query, {"patient_name": patient_name})
                            if results:
                                st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼ç—‡çŠ¶")
                                for result in results:
                                    st.write(f"- {result['patient2']} ä¹Ÿæœ‰ '{result['symptom1']}' ç—‡çŠ¶")
                            else:
                                st.info("æœªæ‰¾åˆ°ç›¸ä¼¼ç—‡çŠ¶")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        
        elif query_type == "æ‚£è€…ç”ŸåŒ–æŒ‡æ ‡å¼‚å¸¸å…³è”":
            patient_name = st.selectbox("é€‰æ‹©æ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æç”ŸåŒ–æŒ‡æ ‡å¼‚å¸¸å…³è”..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨PGQLæŸ¥è¯¢å¼‚å¸¸ç”ŸåŒ–æŒ‡æ ‡
                            query = """
                            SELECT DISTINCT 
                                v.entity_name as patient,
                                e.indicator_name as indicator,
                                e.value as value,
                                e.unit as unit,
                                e.reference_range as reference
                            FROM (v) -[e:HAS_INDICATOR]-> ()
                            WHERE v.entity_type = 'PATIENT'
                            AND v.entity_name = :patient_name
                            AND e.reference_range = 'å¼‚å¸¸'
                            """
                            results = graph_store.execute_pgql(query, {"patient_name": patient_name})
                            if results:
                                st.success(f"æ‰¾åˆ° {len(results)} ä¸ªå¼‚å¸¸æŒ‡æ ‡")
                                for result in results:
                                    st.write(f"- {result['indicator']}: {result['value']}{result['unit']} (å¼‚å¸¸)")
                            else:
                                st.info("æœªæ‰¾åˆ°å¼‚å¸¸æŒ‡æ ‡")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        
        elif query_type == "æ‚£è€…è¯Šæ–­å…³ç³»ç½‘ç»œ":
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æè¯Šæ–­å…³ç³»ç½‘ç»œ..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨PGQLæŸ¥è¯¢è¯Šæ–­å…³ç³»
                            query = """
                            SELECT DISTINCT 
                                v1.entity_name AS patient1,
                                v2.entity_name AS patient2,
                                e1.relation_type AS diagnosis_type
                            MATCH (v1) -[e1]-> () <-[e2]- (v2)
                            WHERE v1.entity_type = 'PATIENT'
                              AND v2.entity_type = 'PATIENT'
                              AND v1.entity_name != v2.entity_name
                              AND e1.relation_type IN ('å…¥é™¢è¯Šæ–­', 'å‡ºé™¢è¯Šæ–­')
                              AND e2.relation_type = e1.relation_type
                            """
                            results = graph_store.execute_pgql(query)
                            if results:
                                # åˆ›å»ºç½‘ç»œå›¾
                                net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="black")
                                
                                # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
                                nodes = set()
                                for result in results:
                                    patient1 = result['patient1']
                                    patient2 = result['patient2']
                                    diagnosis = result['diagnosis_type']
                                    
                                    if patient1 not in nodes:
                                        net.add_node(patient1, label=patient1, color='#add8e6')
                                        nodes.add(patient1)
                                    if patient2 not in nodes:
                                        net.add_node(patient2, label=patient2, color='#add8e6')
                                        nodes.add(patient2)
                                        
                                    net.add_edge(patient1, patient2, title=diagnosis)
                                
                                # ä¿å­˜å¹¶æ˜¾ç¤ºç½‘ç»œå›¾
                                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.html', encoding='utf-8') as f:
                                    net.save_graph(f.name)
                                    with open(f.name, 'r', encoding='utf-8') as f:
                                        html_content = f.read()
                                    components.html(html_content, height=600)
                                    os.unlink(f.name)
                            else:
                                st.info("æœªæ‰¾åˆ°è¯Šæ–­å…³ç³»")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        
        elif query_type == "æ‚£è€…ç”¨è¯å…³è”åˆ†æ":
            patient_name = st.selectbox("é€‰æ‹©æ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if st.button("åˆ†æ"):
                with st.spinner("æ­£åœ¨åˆ†æç”¨è¯å…³è”..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨PGQLæŸ¥è¯¢ç”¨è¯å…³è”
                            query = """
                            SELECT DISTINCT 
                                v1.entity_name AS patient1,
                                v2.entity_name AS patient2,
                                e1.relation_type AS medication_type
                            MATCH (v1) -[e1]-> () <-[e2]- (v2)
                            WHERE v1.entity_type = 'PATIENT'
                              AND v2.entity_type = 'PATIENT'
                              AND v1.entity_name = :patient_name
                              AND v1.entity_name != v2.entity_name
                              AND e1.relation_type = 'ç”¨è¯è®°å½•'
                              AND e2.relation_type = e1.relation_type
                            """
                            results = graph_store.execute_pgql(query, {"patient_name": patient_name})
                            if results:
                                st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç”¨è¯å…³è”")
                                for result in results:
                                    st.write(f"- {result['patient2']} ä¹Ÿä½¿ç”¨äº† '{result['medication']}'")
                            else:
                                st.info("æœªæ‰¾åˆ°ç”¨è¯å…³è”")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                        
        elif query_type == "æ‚£è€…æ²»ç–—æ–¹æ¡ˆå¯¹æ¯”":
            patient1 = st.selectbox("é€‰æ‹©ç¬¬ä¸€ä¸ªæ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            patient2 = st.selectbox("é€‰æ‹©ç¬¬äºŒä¸ªæ‚£è€…", ["é©¬æŸæŸ", "å‘¨æŸæŸ", "åˆ˜æŸæŸ", "è’²æŸæŸ", "æ¨æŸæŸ"])
            if patient1 != patient2 and st.button("å¯¹æ¯”"):
                with st.spinner("æ­£åœ¨å¯¹æ¯”æ²»ç–—æ–¹æ¡ˆ..."):
                    try:
                        with OracleGraphStore() as graph_store:
                            # ä½¿ç”¨PGQLæŸ¥è¯¢æ²»ç–—æ–¹æ¡ˆ
                            query = """
                            SELECT 
                                v.entity_name AS patient,
                                e.relation_type AS treatment_type,
                                t.entity_value AS treatment_detail
                            MATCH (v) -[e]-> (t)
                            WHERE v.entity_type = 'PATIENT'
                              AND v.entity_name IN (:patient1, :patient2)
                              AND e.relation_type = 'è¯Šç–—ç»è¿‡'
                            """
                            results = graph_store.execute_pgql(query, {
                                "patient1": patient1,
                                "patient2": patient2
                            })
                            
                            if results:
                                # æŒ‰æ‚£è€…åˆ†ç»„æ²»ç–—æ–¹æ¡ˆ
                                treatments = {}
                                for result in results:
                                    patient = result['patient']
                                    if patient not in treatments:
                                        treatments[patient] = []
                                    treatments[patient].append({
                                        'treatment': result['treatment_detail'],
                                        'effect': result['effect']
                                    })
                                
                                # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.subheader(patient1)
                                    if patient1 in treatments:
                                        for t in treatments[patient1]:
                                            st.write(f"- {t['treatment']} (æ•ˆæœ: {t['effect']})")
                                    else:
                                        st.info("æ— æ²»ç–—æ–¹æ¡ˆè®°å½•")
                                        
                                with col2:
                                    st.subheader(patient2)
                                    if patient2 in treatments:
                                        for t in treatments[patient2]:
                                            st.write(f"- {t['treatment']} (æ•ˆæœ: {t['effect']})")
                                    else:
                                        st.info("æ— æ²»ç–—æ–¹æ¡ˆè®°å½•")
                            else:
                                st.info("æœªæ‰¾åˆ°æ²»ç–—æ–¹æ¡ˆè®°å½•")
                    except Exception as e:
                        st.error(f"å¯¹æ¯”å¤±è´¥: {str(e)}")
    
    with tab2:
        st.subheader("è‡ªå®šä¹‰PGQLæŸ¥è¯¢")
        st.markdown("""
        æ‚¨å¯ä»¥è¾“å…¥è‡ªå®šä¹‰çš„PGQLæŸ¥è¯¢è¯­å¥ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š
        
        1. æŸ¥è¯¢æ‚£è€…çš„æ‰€æœ‰ç—‡çŠ¶ï¼š
        ```sql
        SELECT v.entity_name, e.symptom
        FROM MATCH (v) -[e:HAS_SYMPTOM]-> ()
        WHERE v.entity_type = 'PATIENT'
        ```
        
        2. æŸ¥è¯¢ç‰¹å®šç—‡çŠ¶çš„æ‰€æœ‰æ‚£è€…ï¼š
        ```sql
        SELECT v.entity_name
        FROM MATCH (v) -[e:HAS_SYMPTOM]-> ()
        WHERE v.entity_type = 'PATIENT'
        AND e.symptom = 'å‘çƒ­'
        ```
        
        3. æŸ¥è¯¢æ‚£è€…çš„å¼‚å¸¸ç”ŸåŒ–æŒ‡æ ‡ï¼š
        ```sql
        SELECT v.entity_name, e.indicator_name, e.value
        FROM MATCH (v) -[e:HAS_INDICATOR]-> ()
        WHERE v.entity_type = 'PATIENT'
        AND e.reference_range = 'å¼‚å¸¸'
        ```
        """)
        
        query = st.text_area("è¾“å…¥PGQLæŸ¥è¯¢è¯­å¥", height=150)
        params = st.text_input("è¾“å…¥å‚æ•°ï¼ˆJSONæ ¼å¼ï¼Œå¯é€‰ï¼‰", "{}")
        
        if st.button("æ‰§è¡ŒæŸ¥è¯¢"):
            if query:
                with st.spinner("æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢..."):
                    try:
                        # è§£æå‚æ•°
                        try:
                            params = json.loads(params) if params.strip() else {}
                        except json.JSONDecodeError:
                            st.error("å‚æ•°æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æœ‰æ•ˆçš„JSONæ ¼å¼")
                            return
                        
                        # æ‰§è¡ŒæŸ¥è¯¢
                        with OracleGraphStore() as graph_store:
                            results = graph_store.execute_pgql(query, params)
                            if results:
                                st.success(f"æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
                                st.json(results)
                            else:
                                st.info("æŸ¥è¯¢æœªè¿”å›ä»»ä½•ç»“æœ")
                    except Exception as e:
                        st.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
            else:
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢è¯­å¥")

def main():
    st.title("åŒ»ç–—æ–‡æ¡£æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    # åˆ›å»ºä¾§è¾¹æ èœå•
    menu = st.sidebar.selectbox(
        "åŠŸèƒ½èœå•",
        ["æ–‡æ¡£ç®¡ç†", "å›¾æ•°æ®æ£€ç´¢", "å‘é‡æ£€ç´¢", "ç»“æ„åŒ–æ£€ç´¢", "å±æ€§å›¾æ£€ç´¢"]
    )
    
    if menu == "æ–‡æ¡£ç®¡ç†":
        display_document_management()
    elif menu == "å›¾æ•°æ®æ£€ç´¢":
        st.header("å›¾æ•°æ®æ£€ç´¢")
        # æ˜¾ç¤ºå·²è§£æçš„æ–‡æ¡£
        display_parsed_documents()
        
        # æ·»åŠ æœç´¢æ¡†
        query = st.text_input("è¯·è¾“å…¥æœç´¢å†…å®¹ï¼ˆæ”¯æŒæŒ‰æ‚£è€…å§“åã€ç—‡çŠ¶ã€è¯Šæ–­ç­‰æœç´¢ï¼‰")
        if query:
            results = search_graph_data(query)
            display_graph_results(results, query)
    elif menu == "å‘é‡æ£€ç´¢":
        display_vector_search()
    elif menu == "å±æ€§å›¾æ£€ç´¢":
        display_property_graph_search()
    else:  # ç»“æ„åŒ–æ£€ç´¢
        display_structured_search()

if __name__ == "__main__":
    main()

