"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿå®‰è£…æ–‡æ¡£

1. ç¯å¢ƒè¦æ±‚ï¼š
   - Python 3.7+
   - pip (PythonåŒ…ç®¡ç†å™¨)

2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ä½†æ¨èï¼‰ï¼š
   python -m venv venv
   source venv/bin/activate  # åœ¨Windowsä¸Šä½¿ç”¨: venv\Scripts\activate

3. å®‰è£…ä¾èµ–ï¼š
   pip install -r requirements.txt

4. requirements.txt æ–‡ä»¶å†…å®¹ï¼š
   streamlit
   openai
   sentence-transformers
   PyPDF2
   python-docx
   faiss-cpu
   tiktoken
   serpapi
   pandas
   sqlite3  # é€šå¸¸å·²åŒ…å«åœ¨Pythonæ ‡å‡†åº“ä¸­

5. å…¶ä»–ä¾èµ–ï¼š
   - ç¡®ä¿ä½ æœ‰æœ‰æ•ˆçš„OpenAI APIå¯†é’¥
   - å¦‚æœä½¿ç”¨Googleæœç´¢åŠŸèƒ½ï¼Œéœ€è¦æœ‰æ•ˆçš„SerpAPIå¯†é’¥

6. è¿è¡Œåº”ç”¨ï¼š
   streamlit run app.py

æ³¨æ„ï¼š
- è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å®‰è£…
- åœ¨ä»£ç ä¸­æ›¿æ¢OpenAI APIå¯†é’¥å’ŒSerpAPIå¯†é’¥ä¸ºä½ è‡ªå·±çš„å¯†é’¥
- å¯¹äºå¤§å‹æ–‡ä»¶å¤„ç†ï¼Œå¯èƒ½éœ€è¦å¢åŠ ç³»ç»Ÿå†…å­˜æˆ–ä½¿ç”¨æ›´å¼ºå¤§çš„ç¡¬ä»¶
"""

import streamlit as st
import sys
import os

# è®¾ç½®é¡µé¢é…ç½®å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - by Huaiyuan Tan",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# æ·»åŠ å¼€å‘è€…ä¿¡æ¯
st.markdown("<h6 style='text-align: right; color: gray;'>å¼€å‘è€…: Huaiyuan Tan</h6>", unsafe_allow_html=True)

# éšè— Streamlit é»˜è®¤çš„èœå•ã€é¡µè„šå’Œ Deploy æŒ‰é’®
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
    header {visibility: hidden;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

from openai import OpenAI
from sentence_transformers import SentenceTransformer
import multiprocessing
import PyPDF2
import docx
import faiss
import tiktoken
import os
import pickle
import numpy as np
import jieba
from collections import Counter
import sqlite3
import pandas as pd
from serpapi import GoogleSearch
import requests
import io
from sqlalchemy import create_engine, inspect
import plotly.express as px
from pyvis.network import Network
import streamlit.components.v1 as components

# åˆå§‹åŒ–
client = OpenAI(
    api_key="sk-2D0EZSwcWUcD4c2K59353b7214854bBd8f35Ac131564EfBa",
    base_url="https://free.gpt.ge/v1"
)

# åœ¨åˆå§‹åŒ– client åæ·»åŠ æµ‹è¯•ä»£ç 
try:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "è¯·å‘Šè¯‰æˆ‘ä½ çš„æ¨¡å‹åç§°"},
            {"role": "user", "content": "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ"}
        ]
    )
    st.write("å½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼š", response.model)  # æ˜¾ç¤ºå®é™…ä½¿ç”¨çš„æ¨¡å‹
    st.write("æ¨¡å‹å“åº”ï¼š", response.choices[0].message.content)
except Exception as e:
    st.error(f"æ¨¡å‹æµ‹è¯•å‡ºé”™: {str(e)}")

@st.cache_resource
def load_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

model = load_model()

# è®¡ç®—tokenæ•°é‡
def num_tokens_from_string(string: str) -> int:
    encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    return len(encoding.encode(string))

# æ–‡æ¡£å‘é‡åŒ–æ¨¡å—
def vectorize_document(file, max_tokens):
    text = ""
    if file.type == "application/pdf":
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = docx.Document(file)
        for para in doc.paragraphs:
            text += para.text + "\n"
    else:
        text = file.getvalue().decode("utf-8")
    
    chunks = []
    current_chunk = ""
    for sentence in text.split('.'):
        if num_tokens_from_string(current_chunk + sentence) > max_tokens:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
        else:
            current_chunk += sentence + '.'
    if current_chunk:
        chunks.append(current_chunk)
    
    vectors = model.encode(chunks)
    index = faiss.IndexFlatL2(384)  # 384æ˜¯å‘é‡ç»´åº¦,æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
    index.add(vectors)
    return chunks, index

# æ–°å¢å‡½æ•°ï¼šæå–å…³é”®è¯
def extract_keywords(text, top_k=5):
    words = jieba.cut(text)
    word_count = Counter(words)
    # è¿‡æ»¤æ‰åœç”¨è¯å’Œå•ä¸ªå­—ç¬¦
    keywords = [word for word, count in word_count.most_common(top_k*2) if len(word) > 1]
    return keywords[:top_k]

# æ–°å¢å‡½æ•°ï¼šåŸºäºå…³é”®è¯æœç´¢æ–‡æ¡£
def search_documents(keywords, file_indices):
    relevant_docs = []
    for file_name, (chunks, _) in file_indices.items():
        doc_content = ' '.join(chunks)
        if any(keyword in doc_content for keyword in keywords):
            relevant_docs.append(file_name)
    return relevant_docs

# ä¿®æ”¹çŸ¥è¯†é—®ç­”æ¨¡å—
def rag_qa(query, file_indices, relevant_docs=None):
    keywords = extract_keywords(query)
    if relevant_docs is None:
        relevant_docs = search_documents(keywords, file_indices)
    
    if not relevant_docs:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚", [], ""

    all_chunks = []
    chunk_to_file = {}
    combined_index = faiss.IndexFlatL2(384)
    
    offset = 0
    for file_name in relevant_docs:
        if file_name in file_indices:
            chunks, index = file_indices[file_name]
            all_chunks.extend(chunks)
            for i in range(len(chunks)):
                chunk_to_file[offset + i] = file_name
            combined_index.add(index.reconstruct_n(0, index.ntotal))
            offset += len(chunks)

    if not all_chunks:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ æ–‡æ¡£ã€‚", [], ""

    query_vector = model.encode([query])
    D, I = combined_index.search(query_vector, k=3)
    context = []
    context_with_sources = []
    for i in I[0]:
        if 0 <= i < len(all_chunks):  # ç¡®ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            chunk = all_chunks[i]
            context.append(chunk)
            file_name = chunk_to_file.get(i, "æœªçŸ¥æ–‡ä»¶")
            context_with_sources.append((file_name, chunk))

    context_text = "\n".join(context)
    
    # ç¡®ä¿æ€»tokenæ•°ä¸è¶…è¿‡4096
    max_context_tokens = 3000  # ä¸ºç³»ç»Ÿæ¶ˆæ¯ã€æŸ¥è¯¢å’Œå…¶ä»–å†…å®¹é¢„ç•™æ›´å¤šç©ºé—´
    while num_tokens_from_string(context_text) > max_context_tokens:
        context_text = context_text[:int(len(context_text)*0.9)]  # æ¯æ¬¡å‡å°‘10%çš„å†…å®¹
    
    if not context_text:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", [], ""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œæ— è®ºé—®é¢˜æ˜¯ä»€ä¹ˆè¯­è¨€ã€‚åœ¨å›ç­”ä¹‹åï¼Œè¯·åŠ¡å¿…æä¾›ä¸€æ®µæœ€ç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"},
            {"role": "user", "content": f"ä¸Šä¸‹æ–‡: {context_text}\n\né—®é¢˜: {query}\n\nè¯·æä¾›ä½ çš„å›ç­”ç„¶ååœ¨å›ç­”åé¢é™„ä¸Šç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"}
        ]
    )
    answer = response.choices[0].message.content
    
    # æ›´çµæ´»åœ°å¤„ç†å›ç­”æ ¼å¼
    if "ç›¸å…³åŸæ–‡ï¼š" in answer:
        answer_parts = answer.split("ç›¸å…³åŸæ–‡ï¼š", 1)
        main_answer = answer_parts[0].strip()
        relevant_excerpt = answer_parts[1].strip()
    else:
        main_answer = answer.strip()
        relevant_excerpt = ""
    
    # å¦‚æœAIæ²¡æœ‰æä¾›ç›¸å…³åŸæ–‡ï¼Œæˆ‘ä»¬ä»ä¸Šä¸‹æ–‡ä¸­é€‰æ‹©ä¸€ä¸ª
    if not relevant_excerpt and context:
        relevant_excerpt = context[0][:200] + "..."  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡çš„å‰200ä¸ªå­—ç¬¦
    
    # æ‰¾å‡ºåŒ…å«ç›¸å…³åŸæ–‡çš„æ–‡ä»¶
    relevant_sources = []
    if relevant_excerpt:
        for file_name, chunk in context_with_sources:
            if relevant_excerpt in chunk:
                relevant_sources.append((file_name, chunk))
                break  # åªæ·»åŠ ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
    if not relevant_sources and context_with_sources:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡æº
        relevant_sources.append(context_with_sources[0])

    return main_answer, relevant_sources, relevant_excerpt

# ä¿å­˜ç´¢å¼•å’Œchunks
def save_index(file_name, chunks, index):
    if not os.path.exists('indices'):
        os.makedirs('indices')
    with open(f'indices/{file_name}.pkl', 'wb') as f:
        pickle.dump((chunks, index), f)
    # ä¿å­˜æ–‡ä»¶ååˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
    else:
        file_list = []
    if file_name not in file_list:
        file_list.append(file_name)
        with open(file_list_path, 'w') as f:
            f.write('\n'.join(file_list))

# åŠ è½½æ‰€æœ‰ä¿å­˜çš„ç´¢å¼•
def load_all_indices():
    file_indices = {}
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
        for file_name in file_list:
            file_path = f'indices/{file_name}.pkl'
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    chunks, index = pickle.load(f)
                file_indices[file_name] = (chunks, index)
    return file_indices

def delete_index(file_name):
    if os.path.exists(f'indices/{file_name}.pkl'):
        os.remove(f'indices/{file_name}.pkl')
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
        if file_name in file_list:
            file_list.remove(file_name)
            with open(file_list_path, 'w') as f:
                f.write('\n'.join(file_list))

def main():
    st.markdown("""
    <style>
    .reportview-container .main .block-container{
        padding-top: 1rem;
        padding-right: 1rem;
        padding-left: 1rem;
        padding-bottom: 1rem;
    }
    .stColumn {
        padding: 5px;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")

    # åˆå§‹åŒ– session state
    if "rag_messages" not in st.session_state:
        st.session_state.rag_messages = []
    if "file_indices" not in st.session_state:
        st.session_state.file_indices = load_all_indices()

    # åˆ›å»ºç­¾é¡µ
    tab1, tab2, tab3 = st.tabs(["RAGçŸ¥è¯†é—®ç­”", "ç½‘ç»œæœç´¢é—®ç­”", "AIæ•°æ®åˆ†æ"])

    with tab1:
        st.header("RAG é—®ç­”")

        # æ–‡æ¡£ä¸Šä¼ éƒ¨åˆ†
        st.subheader("æ–‡æ¡£ä¸Šä¼ ")
        
        max_tokens = 4096

        uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "docx", "txt"], accept_multiple_files=True, key="rag_file_uploader_1")

        if uploaded_files:
            for uploaded_file in uploaded_files:
                with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£: {uploaded_file.name}..."):
                    chunks, index = vectorize_document(uploaded_file, max_tokens)
                    st.session_state.file_indices[uploaded_file.name] = (chunks, index)
                    save_index(uploaded_file.name, chunks, index)
                st.success(f"æ–‡æ¡£ {uploaded_file.name} ï¿½ï¿½å‘é‡åŒ–å¹¶æ·»åŠ åˆ°ç´¢ï¿½ï¿½ä¸­ï¼")

        # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡ä»¶å¹¶æ·»åŠ åˆ é™¤æŒ‰é’®
        st.subheader("å·²å¤„ç†æ–‡æ¡£:")
        for file_name in list(st.session_state.file_indices.keys()):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"â€¢ {file_name}")
            with col2:
                if st.button("åˆ é™¤", key=f"delete_{file_name}"):
                    del st.session_state.file_indices[file_name]
                    delete_index(file_name)
                    st.success(f"æ–‡æ¡£ {file_name} å·²åˆ é™¤ï¼")
                    st.rerun()

        # æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
        st.subheader("å…³é”®è¯æœç´¢")
        search_keywords = st.text_input("è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰", key="rag_search_keywords_1")
        if search_keywords:
            keywords = search_keywords.split()
            relevant_docs = search_documents(keywords, st.session_state.file_indices)
            if relevant_docs:
                st.write("ç›¸å…³æ–‡æ¡£ï¼š")
                for doc in relevant_docs:
                    st.write(f"â€¢ {doc}")
                st.session_state.relevant_docs = relevant_docs
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                st.session_state.relevant_docs = None

        # å¯¹è¯éƒ¨åˆ†
        st.subheader("å¯¹è¯")
        chat_container = st.container()

        with chat_container:
            for i, message in enumerate(st.session_state.rag_messages):
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
                    if "sources" in message and message["sources"]:
                        st.markdown("**å‚è€ƒæ¥æºï¼š**")
                        file_name, _ = message["sources"][0]
                        st.markdown(f"**æ–‡ä»¶ï¼š** {file_name}")
                        if os.path.exists(f'indices/{file_name}.pkl'):
                            with open(f'indices/{file_name}.pkl', 'rb') as f:
                                file_content = pickle.load(f)[0]  # è·å–æ–‡ä»¶å†…å®¹
                            st.download_button(
                                label="ä¸‹è½½æºæ–‡ä»¶",
                                data='\n'.join(file_content),
                                file_name=file_name,
                                mime='text/plain',
                                key=f"download_{i}"
                            )
                    if "relevant_excerpt" in message:
                        st.markdown(f"**ç›¸å…³åŸæ–‡ï¼š** <mark>{message['relevant_excerpt']}</mark>", unsafe_allow_html=True)

        # ç”¨æˆ·è¾“å…¥
        prompt = st.chat_input("è¯·åŸºäºä¸Šä¼ çš„æ–‡æ¡£æå‡ºé—®é¢˜:", key="rag_chat_input_1")

        if prompt:
            st.session_state.rag_messages.append({"role": "user", "content": prompt})
            
            if st.session_state.file_indices:
                with chat_container:
                    with st.chat_message("user"):
                        st.markdown(prompt)
                    with st.chat_message("assistant"):
                        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                            try:
                                relevant_docs = st.session_state.get('relevant_docs')
                                response, sources, relevant_excerpt = rag_qa(prompt, st.session_state.file_indices, relevant_docs)
                                st.markdown(response)
                                if sources:
                                    st.markdown("**å‚è€ƒæ¥æºï¼š**")
                                    file_name, _ = sources[0]
                                    st.markdown(f"**æ–‡ä»¶ï¼š** {file_name}")
                                    if os.path.exists(f'indices/{file_name}.pkl'):
                                        with open(f'indices/{file_name}.pkl', 'rb') as f:
                                            file_content = pickle.load(f)[0]  # è·å–æ–‡ä»¶å†…å®¹
                                        st.download_button(
                                            label="ä¸‹è½½æºæ–‡ä»¶",
                                            data='\n'.join(file_content),
                                            file_name=file_name,
                                            mime='text/plain',
                                            key=f"download_new_{len(st.session_state.rag_messages)}"
                                        )
                                if relevant_excerpt:
                                    st.markdown(f"**ç›¸å…³åŸæ–‡ï¼š** <mark>{relevant_excerpt}</mark>", unsafe_allow_html=True)
                                else:
                                    st.warning("æœªèƒ½æå–åˆ°ç²¾ç¡®çš„ç›¸å…³åŸæ–‡ï¼Œä½†æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
                            except Exception as e:
                                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.session_state.rag_messages.append({
                    "role": "assistant", 
                    "content": response, 
                    "sources": sources,
                    "relevant_excerpt": relevant_excerpt
                })
            else:
                with chat_container:
                    with st.chat_message("assistant"):
                        st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚")

    with tab2:
        st.header("ç½‘ç»œæœç´¢é—®ç­”")

        # åˆå§‹åŒ– session state
        if "web_messages" not in st.session_state:
            st.session_state.web_messages = []
        if "web_prompt" not in st.session_state:
            st.session_state.web_prompt = ""
        if "execute_web_query" not in st.session_state:
            st.session_state.execute_web_query = False

        # åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥æ”¾ç½®å¯¹è¯å†å²
        web_chat_container = st.container()

        with web_chat_container:
            for message in st.session_state.web_messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        # ç”¨æˆ·è¾“å…¥
        web_prompt = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå¦‚éœ€æœç´¢ï¼Œè¯·ä»¥'æœç´¢'å¼€å¤´ï¼‰:", key="web_chat_input")

        if web_prompt:
            st.session_state.web_messages.append({"role": "user", "content": web_prompt})
            
            with web_chat_container:
                with st.chat_message("user"):
                    st.markdown(web_prompt)
                with st.chat_message("assistant"):
                    with st.spinner("æ­£åœ¨æœç´¢å¹¶ç”Ÿæˆå›ç­”..."):
                        try:
                            if web_prompt.lower().startswith("æœç´¢"):
                                response = serpapi_search_qa(web_prompt[2:].strip())  # å»æ‰"æœç´¢"å‰ç¼€
                            else:
                                response = direct_qa(web_prompt)
                            st.markdown(response)
                            st.session_state.web_messages.append({"role": "assistant", "content": response})
                        except Exception as e:
                            st.error(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


    with tab3:
        st.header("AIæ•°æ®åˆ†æ")
        
        def load_data():
            data_source = st.radio("é€‰æ‹©æ•°æ®æº", ["Excelæ–‡ä»¶", "RDBMSæ•°æ®åº“"])
            
            if data_source == "Excelæ–‡ä»¶":
                uploaded_file = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx", "xls"])
                if uploaded_file is not None:
                    df = pd.read_excel(uploaded_file)
                    return df, None, data_source
            else:
                conn = connect_to_database()
                if conn is not None:
                    try:
                        tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table'", conn)
                        if not tables.empty:
                            selected_table = st.selectbox("é€‰æ‹©æ•°æ®è¡¨", tables['name'])
                            if selected_table:
                                df = pd.read_sql_query(f"SELECT * FROM {selected_table}", conn)
                                return df, conn, data_source
                        else:
                            st.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨ã€‚è¯·ç¡®ä¿å·²ç»åˆå§‹åŒ–æ•°æ®åº“ã€‚")
                    except sqlite3.Error as e:
                        st.error(f"æŸ¥è¯¢æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                else:
                    st.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚è¯·æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
            
            return None, None, data_source

        def generate_table_info(df, conn):
            if conn:
                inspector = inspect(create_engine('sqlite:///chinook.db'))
                table_info = "Available tables:\n"
                for table_name in inspector.get_table_names():
                    table_info += f"- {table_name}\n"
                    columns = inspector.get_columns(table_name)
                    table_info += f"  Columns: {', '.join([col['name'] for col in columns])}\n"
            else:
                table_info = f"Table name: data\nColumns: {', '.join(df.columns)}\n"
                table_info += "\n".join([f"{col}: {df[col].dtype}" for col in df.columns])
            return table_info

        def nl_to_sql(nl_query, table_info):
            prompt = f"""
            ç»™å®šä»¥ä¸‹è¡¨æ ¼ä¿¡æ¯ï¼š
            {table_info}
            
            å°†ä»¥ä¸‹è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLï¼š
            {nl_query}
            
            åªè¿”å›SQLæŸ¥è¯¢ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€‚
            """
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ï¼Œèƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¯­å¥ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150,
                n=1,
                stop=None,
                temperature=0.5,
            )
            
            sql_query = response.choices[0].message.content.strip()
            
            # ç§»é™¤å¯èƒ½çš„å‰ç¼€å’Œåç¼€
            if sql_query.startswith("```sql"):
                sql_query = sql_query[6:]
            if sql_query.endswith("```"):
                sql_query = sql_query[:-3]
            
            return sql_query

        def execute_sql_query(df, sql_query):
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ SQLite æ•°æ®åº“åœ¨å†…å­˜ä¸­
            engine = create_engine('sqlite:///:memory:')
            
            # å°† DataFrame å†™å…¥ SQLite æ•°æ®åº“
            df.to_sql('data', engine, index=False)
            
            # æ‰§è¡Œ SQL æŸ¥è¯¢
            result = pd.read_sql_query(sql_query, engine)
            
            return result

        def connect_to_database():
            try:
                conn = sqlite3.connect('chinook.db')
                return conn
            except sqlite3.Error as e:
                st.error(f"è¿æ¥æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                return None

        def get_table_relationships(conn):
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰è¡¨å
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            relationships = []
            
            for table in tables:
                table_name = table[0]
                # è·å–è¡¨çš„å¤–é”®ä¿¡æ¯
                cursor.execute(f"PRAGMA foreign_key_list({table_name})")
                foreign_keys = cursor.fetchall()
                
                for fk in foreign_keys:
                    from_table = table_name
                    to_table = fk[2]  # å¼•ç”¨çš„è¡¨å
                    from_column = fk[3]  # å¤–é”®åˆ—å
                    to_column = fk[4]  # å¼•ç”¨çš„åˆ—å
                    
                    relationships.append({
                        'from_table': from_table,
                        'to_table': to_table,
                        'from_column': from_column,
                        'to_column': to_column
                    })
            
            return relationships

        def generate_relationship_graph(relationships):
            net = Network(notebook=True, height="500px", width="100%", bgcolor="#ffffff", font_color="black")
            
            # æ·»åŠ èŠ‚ç‚¹ï¼ˆè¡¨ï¼‰
            tables = set()
            for rel in relationships:
                tables.add(rel['from_table'])
                tables.add(rel['to_table'])
            
            for table in tables:
                net.add_node(table, label=table, title=table, shape="box")
            
            # æ·»åŠ è¾¹ï¼ˆå…³ç³»ï¼‰
            for rel in relationships:
                edge_label = f"{rel['from_column']} -> {rel['to_column']}"
                net.add_edge(rel['from_table'], rel['to_table'], 
                             title=edge_label, label=edge_label, arrows='to')
            
            # é…ç½®ç½‘ç»œå›¾çš„ç‰©ç†å¸ƒå±€
            net.set_options("""
            var options = {
                "physics": {
                    "forceAtlas2Based": {
                        "gravitationalConstant": -50,
                        "centralGravity": 0.01,
                        "springLength": 200,
                        "springConstant": 0.08
                    },
                    "maxVelocity": 50,
                    "solver": "forceAtlas2Based",
                    "timestep": 0.35,
                    "stabilization": {"iterations": 150}
                },
                "edges": {
                    "font": {
                        "size": 12,
                        "align": "middle"
                    },
                    "smooth": {
                        "type": "continuous",
                        "forceDirection": "none"
                    }
                },
                "nodes": {
                    "font": {
                        "size": 16,
                        "face": "Tahoma"
                    },
                    "shape": "box"
                }
            }
            """)
            
            # ç”ŸæˆHTMLæ–‡ä»¶
            net.save_graph("temp_graph.html")
            
            # è¯»å–ç”Ÿæˆçš„HTMLæ–‡ä»¶å†…å®¹
            with open("temp_graph.html", "r", encoding="utf-8") as f:
                html_string = f.read()
            
            return html_string

        # åŠ è½½æ•°æ®
        df, conn, data_source = load_data()

        if df is not None:
            st.success("æ•°æ®å·²æˆåŠŸåŠ è½½ï¼")
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.subheader("æ•°æ®é¢„è§ˆ")
            st.dataframe(df.head())
            
            table_info = generate_table_info(df, conn)
            
            # è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            nl_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼š")
            
            if nl_query:
                # ä½¿ç”¨NL2SQLè½¬æ¢æŸ¥è¯¢
                sql_query = nl_to_sql(nl_query, table_info)
                
                st.write(f"ç”Ÿæˆçš„SQLæŸ¥è¯¢ï¼š{sql_query}")
                
                try:
                    if conn:
                        result_df = pd.read_sql_query(sql_query, conn)
                    else:
                        result_df = execute_sql_query(df, sql_query)
                    
                    st.write("æŸ¥è¯¢ç»“æœï¼š")
                    st.dataframe(result_df)
                    
                    # åˆ†æé€‰é¡¹
                    if not result_df.empty:
                        st.subheader("æ•°æ®å¯è§†åŒ–")
                        
                        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
                        col1, col2 = st.columns([1, 3])
                        
                        with col1:
                            analysis_type = st.selectbox("é€‰æ‹©åˆ†æå›¾è¡¨ç±»å‹", [
                                "æŸ±çŠ¶å›¾", "æŠ˜çº¿å›¾", "æ•£ç‚¹å›¾", "é¥¼å›¾", "ç®±çº¿å›¾", "çƒ­åŠ›å›¾", "é¢ç§¯å›¾", "ç›´æ–¹å›¾"
                            ])
                            
                            x_column = st.selectbox("é€‰æ‹©Xè½´", result_df.columns)
                            y_column = st.selectbox("é€‰æ‹©Yè½´", result_df.columns)
                            
                            if analysis_type in ["æ•£ç‚¹å›¾", "çƒ­åŠ›å›¾"]:
                                color_column = st.selectbox("é€‰æ‹©é¢œè‰²æ˜ å°„åˆ—", result_df.columns)
                            
                            # å›¾è¡¨å¤§å°è°ƒæ•´
                            chart_width = st.slider("å›¾è¡¨å®½åº¦", 400, 1200, 800)
                            chart_height = st.slider("å›¾è¡¨é«˜åº¦", 300, 900, 500)
                        
                        with col2:
                            if analysis_type == "æŸ±çŠ¶å›¾":
                                fig = px.bar(result_df, x=x_column, y=y_column, title="æŸ±çŠ¶å›¾")
                            elif analysis_type == "æŠ˜çº¿å›¾":
                                fig = px.line(result_df, x=x_column, y=y_column, title="æŠ˜çº¿å›¾")
                            elif analysis_type == "æ•£ç‚¹å›¾":
                                fig = px.scatter(result_df, x=x_column, y=y_column, color=color_column, title="æ•£ç‚¹å›¾")
                            elif analysis_type == "é¥¼å›¾":
                                fig = px.pie(result_df, values=y_column, names=x_column, title="é¥¼å›¾")
                            elif analysis_type == "ç®±çº¿å›¾":
                                fig = px.box(result_df, x=x_column, y=y_column, title="ç®±çº¿å›¾")
                            elif analysis_type == "çƒ­åŠ›å›¾":
                                fig = px.density_heatmap(result_df, x=x_column, y=y_column, z=color_column, title="çƒ­åŠ›å›¾")
                            elif analysis_type == "é¢ç§¯å›¾":
                                fig = px.area(result_df, x=x_column, y=y_column, title="é¢ç§¯å›¾")
                            elif analysis_type == "ç›´æ–¹å›¾":
                                fig = px.histogram(result_df, x=x_column, title="ç›´æ–¹å›¾")
                            
                            # è°ƒæ•´å›¾è¡¨å¤§å°
                            fig.update_layout(width=chart_width, height=chart_height)
                            
                            # æ˜¾ç¤ºå›¾è¡¨
                            st.plotly_chart(fig)
                    else:
                        st.write("æŸ¥è¯¢ç»“æœä¸ºç©º")
                
                except Exception as e:
                    st.error(f"æŸ¥è¯¢æ‰§è¡Œé”™è¯¯: {e}")

        else:
            st.info("é€‰æ‹©æ•°æ®æºå¹¶åŠ è½½æ•°æ®")

        # å…³é—­æ•°æ®åº“è¿æ¥
        if conn:
            conn.close()

        # æ•°æ®åº“ç»“æ„å¯è§†åŒ–éƒ¨åˆ†
        if data_source == "RDBMSæ•°æ®åº“":
            st.subheader("æ•°æ®åº“è¡¨å…³ç³»")

            if st.button("æ˜¾ç¤ºæ•°æ®åº“è¡¨å…³ç³»"):
                conn = connect_to_database()
                if conn:
                    relationships = get_table_relationships(conn)
                    if relationships:
                        html_string = generate_relationship_graph(relationships)
                        components.html(html_string, height=600, scrolling=True)
                    else:
                        st.info("æœªæ‰¾åˆ°è¡¨ä¹‹é—´çš„å…³ç³»ã€‚")
                    conn.close()

def direct_qa(query):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
            {"role": "user", "content": query}
        ]
    )
    return response.choices[0].message.content.strip()

def serpapi_search_qa(query, num_results=3):
    params = {
        "engine": "google",
        "q": query,
        "api_key": "04fec5e75c6f477225ce29bc358f4cc7088945d0775e7f75721cd85b36387125",
        "num": num_results
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    organic_results = results.get("organic_results", [])
    
    if not organic_results:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
    
    snippets = [result.get("snippet", "") for result in organic_results]
    links = [result.get("link", "") for result in organic_results]
    
    search_results = "\n".join([f"{i+1}. {snippet} ({link})" for i, (snippet, link) in enumerate(zip(snippets, links))])
    prompt = f"""é—®é¢˜: {query}
æœç´¢ç»“æœ:
{search_results}

è¯·æ ¹æ®ä¸Šè¿°æœç´¢ç»“æœå›ç­”é—®é¢˜ã€‚å¦‚æœæœç´¢ç»“æœä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´"æ ¹æ®æœç´¢ç»“æœæ— æ³•å›ç­”é—®é¢˜"ã€‚"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®æœç´¢ç»“æœå›ç­”é—®é¢˜ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip()

def download_and_create_database():
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite"
    try:
        response = requests.get(url)
        response.raise_for_status()
        
        with open('chinook.db', 'wb') as f:
            f.write(response.content)
        
        print("æ•°æ®åº“æ–‡ä»¶å·²ä¸‹è½½å¹¶ä¿å­˜")
        
        conn = sqlite3.connect('chinook.db')
        c = conn.cursor()
        c.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = c.fetchall()
        if tables:
            print(f"æˆåŠŸåˆ›å»ºæ•°æ®åº“ï¼ŒåŒ…å«ä»¥ä¸‹è¡¨ï¼š{[table[0] for table in tables]}")
        else:
            print("æ•°æ®åº“æ–‡ä»¶å·²åˆ›å»ºï¼Œä½†æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨")
        conn.close()
    except Exception as e:
        print(f"ä¸‹è½½æˆ–åˆ›å»ºæ•°æ®åº“æ—¶å‡ºé”™ï¼š{e}")

def get_table_info():
    try:
        conn = sqlite3.connect('chinook.db')
        c = conn.cursor()
        c.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = c.fetchall()
        
        table_info = {}
        for table in tables:
            table_name = table[0]
            c.execute(f"PRAGMA table_info({table_name})")
            columns = c.fetchall()
            table_info[table_name] = [column[1] for column in columns]
        
        conn.close()
        return table_info
    except Exception as e:
        print(f"è·å–è¡¨ä¿¡æ¯æ—¶å‡ºé”™ï¼š{e}")
        return {}

def clean_sql_query(sql_query):
    # ç§»é™¤å¯èƒ½çš„ Markdown ä»£ç å—æ ‡è®°å’Œå¤šä½™çš„ç©ºç™½å­—ç¬¦
    sql_query = sql_query.replace('```sql', '').replace('```', '').strip()
    return ' '.join(sql_query.split())  # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦

def nl_to_sql(nl_query):
    table_info = get_table_info()
    table_descriptions = "\n".join([f"è¡¨å: {table}\nå­—æ®µ: {', '.join(columns)}" for table, columns in table_info.items()])
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ï¼Œå¤Ÿå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¯­å¥ã€‚æ•°æ®åº“åŒ…å«ä»¥ä¸‹è¡¨å’Œå­—æ®µï¼š\n\n{table_descriptions}"},
            {"role": "user", "content": f"å°†ä»¥ä¸‹è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¯­å¥ï¼š\n{nl_query}\nåªè¿”å›SQLè¯­å¥ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šã€‚"}
        ]
    )
    return clean_sql_query(response.choices[0].message.content.strip())

def execute_sql(sql_query):
    conn = sqlite3.connect('chinook.db')
    c = conn.cursor()
    try:
        sql_query = clean_sql_query(sql_query)  # æ¸…ç† SQL æŸ¥è¯¢
        c.execute(sql_query)
        results = c.fetchall()
        column_names = [description[0] for description in c.description]
        conn.close()
        return results, column_names
    except sqlite3.Error as e:
        conn.close()
        return f"SQLæ‰§è¡Œé”™è¯¯: {str(e)}", None

def generate_explanation(nl_query, sql_query, df):
    df_str = df.to_string(index=False, max_rows=5)
    
    prompt = (
        f"è‡ªç„¶è¯­è¨€æŸ¥è¯¢: {nl_query}\n"
        f"SQLæŸ¥è¯¢: {sql_query}\n"
        f"æŸ¥è¯¢ç»“æœ (å‰5è¡Œ):\n"
        f"{df_str}\n\n"
        "è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šè¿™ä¸ªæŸ¥è¯¢çš„ç»“æœã€‚è§£é‡Šåº”è¯¥åŒ…æ‹¬ï¼š\n"
        "1. æŸ¥è¯¢çš„ä¸»è¦ç›®çš„\n"
        "2. ç»“æœçš„æ¦‚è¿°\n"
        "3. ä»»ä½•æœ‰è¶£æˆ–é‡è¦çš„å‘ç°\n\n"
        "è¯·ç¡®ä¿è§£é‡Šç®€æ´æ˜äº†ï¼Œé€‚åˆéæŠ€æœ¯äººå‘˜ç†è§£ã€‚"
        "åœ¨è§£é‡Šä¸­ï¼Œè¯·ç”¨**åŒæ˜Ÿå·**å°†ä¸ç»“æœç›´æ¥ç›¸å…³çš„é‡è¦æ•°å­—æˆ–å…³é”®è¯æ‹¬èµ·æ¥ï¼Œä»¥ä¾¿åç»­é«˜äº®æ˜¾ç¤ºã€‚"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿è§£é‡ŠSQLæŸ¥è¯¢ç»“æœã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    explanation = response.choices[0].message.content.strip()
    
    # å°†åŒæ˜Ÿå·åŒ…å›´çš„æ–‡æœ¬è½¬æ¢ä¸ºHTMLçš„é«˜äº®æ ‡è®°
    highlighted_explanation = explanation.replace("**", "<mark>", 1)
    while "**" in highlighted_explanation:
        highlighted_explanation = highlighted_explanation.replace("**", "</mark>", 1)
        highlighted_explanation = highlighted_explanation.replace("**", "<mark>", 1)
    
    return highlighted_explanation

# è¿è¡Œä¸»åº”ç”¨
if __name__ == "__main__":
    main()

